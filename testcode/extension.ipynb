{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.cpp_extension import load\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "\n",
    "def manual_seed(seed):\n",
    "    np.random.seed(seed) #1\n",
    "    random.seed(seed) #2\n",
    "    torch.manual_seed(seed) #3\n",
    "    torch.cuda.manual_seed(seed) #4.1\n",
    "    torch.cuda.manual_seed_all(seed) #4.2\n",
    "    torch.backends.cudnn.benchmark = False #5 \n",
    "    torch.backends.cudnn.deterministic = True #6\n",
    "\n",
    "manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch integer data flow 는 gpu에서 사용 불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_data = torch.randint(0,255,(1,3,24,24), dtype=torch.uint8)\n",
    "# weight = torch.randint(0,255,(1,3,3,3), dtype=torch.uint8)\n",
    "\n",
    "# # b = torch.nn.functional.conv2d(int_data.cuda(), weight=weight.cuda(),stride=1)\n",
    "# b = torch.nn.functional.conv2d(int_data, weight=weight,stride=1,bias=None, padding=1, dtype=torch.uint8)\n",
    "# print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int8 torch.int8\n",
      "tensor([[1, 2, 3, 4]], device='cuda:0', dtype=torch.int8)\n",
      "tensor([[  43,  -60,   21,    9],\n",
      "        [  73,   32,  115,  -23],\n",
      "        [-103,   74,  -43,  109],\n",
      "        [ -33,  -17,   58,   55],\n",
      "        [ -96,   48,   90,    5],\n",
      "        [  96,  -25, -118,  -48],\n",
      "        [ 120,  -66, -126,  -30],\n",
      "        [  16,  104,   15,  -15],\n",
      "        [ -18, -104,   10,   -7],\n",
      "        [  28,  123,   61,   55],\n",
      "        [  -5,  -63,  -34,   56],\n",
      "        [ -38,   74,   50, -103]], device='cuda:0', dtype=torch.int8)\n",
      "tensor([[  22,  390,  352,  327,  290, -500, -510,  209, -224,  677,   -9, -152]],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import int8mm_cuda\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "class IntLinear(Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(IntLinear,self).__init__()\n",
    "        self.weight = torch.randint(-127,127,(out_channels, in_channels), dtype=torch.int8)\n",
    "        # self.weight = torch.ones((out_channels, in_channels), dtype=torch.int8)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # weight [OUT, IN} - > [IN, OUT]\n",
    "        # input [BATCH, IN]\n",
    "        y = int8mm_cuda.int8_mm(x,self.weight.transpose(1,0).contiguous())\n",
    "        return y\n",
    "    \n",
    "    def cuda(self):\n",
    "        self.weight = self.weight.cuda()\n",
    "        \n",
    "\n",
    "mm = IntLinear(4,12)\n",
    "# x = torch.randint(-127,127,(1,4), dtype=torch.int8).cuda()\n",
    "# x = torch.ones((1,4), dtype=torch.int8).cuda()\n",
    "x = torch.tensor([[1,2,3,4]], dtype=torch.int8).cuda()\n",
    "print(x.dtype, mm.weight.dtype)\n",
    "with torch.no_grad():\n",
    "    mm.cuda()\n",
    "    y = mm(x)\n",
    "print(x)\n",
    "print(mm.weight)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  43,   73, -103,  -33,  -96,   96,  120,   16,  -18,   28,   -5,  -38],\n",
      "        [ -60,   32,   74,  -17,   48,  -25,  -66,  104, -104,  123,  -63,   74],\n",
      "        [  21,  115,  -43,   58,   90, -118, -126,   15,   10,   61,  -34,   50],\n",
      "        [   9,  -23,  109,   55,    5,  -48,  -30,  -15,   -7,   55,   56, -103]],\n",
      "       device='cuda:0', dtype=torch.int8)\n",
      "\n",
      "x data - [[1 2 3 4]]\n",
      "\n",
      "mm data - [[  43  -60   21    9]\n",
      " [  73   32  115  -23]\n",
      " [-103   74  -43  109]\n",
      " [ -33  -17   58   55]\n",
      " [ -96   48   90    5]\n",
      " [  96  -25 -118  -48]\n",
      " [ 120  -66 -126  -30]\n",
      " [  16  104   15  -15]\n",
      " [ -18 -104   10   -7]\n",
      " [  28  123   61   55]\n",
      " [  -5  -63  -34   56]\n",
      " [ -38   74   50 -103]]\n",
      "mm Trans - [[  43   73 -103  -33  -96   96  120   16  -18   28   -5  -38]\n",
      " [ -60   32   74  -17   48  -25  -66  104 -104  123  -63   74]\n",
      " [  21  115  -43   58   90 -118 -126   15   10   61  -34   50]\n",
      " [   9  -23  109   55    5  -48  -30  -15   -7   55   56 -103]]\n",
      "\n",
      "y - [[  22 -122   96   71   34   12    2  -47   32  -91   -9  104]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "print(mm.weight.transpose(1,0),end=\"\\n\\n\")\n",
    "x_data = x.detach().cpu().numpy()\n",
    "mm_data = mm.weight.detach().cpu().numpy()\n",
    "print(f\"x data - {x_data}\\n\")\n",
    "print(f\"mm data - {mm_data}\\nmm Trans - {mm_data.T}\\n\")\n",
    "y = x_data @ mm_data.T\n",
    "print(f\"y - {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n",
      "tensor([[1., 2., 3., 4.]], device='cuda:0')\n",
      "True\n",
      "Parameter containing:\n",
      "tensor([[ 0.4578, -0.1687, -0.1773, -0.4838],\n",
      "        [-0.2863,  0.1249, -0.0660, -0.3629],\n",
      "        [ 0.0117, -0.3415, -0.4242, -0.2753],\n",
      "        [-0.4376, -0.3184,  0.4998,  0.0944],\n",
      "        [ 0.1541, -0.4663, -0.3284, -0.1664],\n",
      "        [ 0.0782, -0.4400, -0.2154, -0.2993],\n",
      "        [ 0.0014, -0.1861, -0.0346, -0.3388],\n",
      "        [-0.3432, -0.2917, -0.1711, -0.3946],\n",
      "        [ 0.4192, -0.0992,  0.4302,  0.1558],\n",
      "        [-0.4234,  0.3460, -0.1376, -0.1917]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4150, -0.4971,  0.1431, -0.1092,  0.1947, -0.4103,  0.3712, -0.3670,\n",
      "        -0.0863,  0.1044], device='cuda:0', requires_grad=True)\n",
      "compare y shape\n",
      "tensor([[-2.7616, -2.1834, -2.9022,  0.6936, -2.2348, -3.0557, -1.4587, -3.3856,\n",
      "          2.0482, -0.8063]], device='cuda:0')\n",
      "tensor([[-2.7616, -2.1834, -2.9022,  0.6936, -2.2348, -3.0557, -1.4587, -3.3856,\n",
      "          2.0482, -0.8063]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import int8mm_cuda\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "class FloatLinear(Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FloatLinear,self).__init__()\n",
    "        self.weight = torch.randn((in_channels, out_channels), dtype=torch.float)\n",
    "        # self.weight = torch.ones((out_channels, in_channels), dtype=torch.int8)\n",
    "        self.bias = torch.zeros((out_channels), dtype=torch.float)\n",
    "    def forward(self,x):\n",
    "        # weight [OUT, IN} - > [IN, OUT]\n",
    "        # input [BATCH, IN]\n",
    "        y = int8mm_cuda.float_mm(x,self.weight.contiguous())\n",
    "        y = y+ self.bias\n",
    "        return y\n",
    "    \n",
    "    def cuda(self):\n",
    "        self.weight = self.weight.cuda()\n",
    "        self.bias = self.bias.cuda()\n",
    "        \n",
    "\n",
    "mm = FloatLinear(4,12)\n",
    "torch_linear = torch.nn.Linear(4,10,bias=True)\n",
    "\n",
    "mm.weight = torch_linear.weight.data.transpose(1,0).contiguous()\n",
    "mm.bias = torch_linear.bias.data.contiguous()\n",
    "\n",
    "x = torch.tensor([[1,2,3,4]],dtype=torch.float).cuda()\n",
    "print(x.dtype, mm.weight.dtype)\n",
    "with torch.no_grad():\n",
    "    mm.cuda()\n",
    "    torch_linear.cuda()\n",
    "    y = mm(x)\n",
    "    trans = torch_linear(x)\n",
    "print(x)\n",
    "print(torch.equal(mm.weight.transpose(0,1),torch_linear.weight.data))\n",
    "print(torch_linear.weight)\n",
    "print(torch_linear.bias)\n",
    "print(f\"compare y shape\")\n",
    "print(y)\n",
    "print(trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import int8pool_cuda\n",
    "\n",
    "class IntPool(Module):\n",
    "    def __init__(self,kernel_size = 2, stride = 2, padding=0, mode=0):\n",
    "        super(IntPool,self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.mode = mode\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y = int8pool_cuda.int8_pool(x,self.kernel_size, self.stride, self.padding, self.mode)\n",
    "        # y = (y > 10).int()*5\n",
    "        # y = y.type(torch.int8)\n",
    "        return y\n",
    "\n",
    "pool = IntPool()\n",
    "x = torch.randint(0, 127,(4,32,32,4), dtype=torch.int8).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = pool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x - torch.Size([4, 32, 32, 4]) \n",
      "tensor([[[[ 78,  85,  10,  48],\n",
      "          [112,   8,  18,  64],\n",
      "          [ 29,   5,  45, 100],\n",
      "          ...,\n",
      "          [123, 124, 101,  95],\n",
      "          [  1,  16,   2,  40],\n",
      "          [ 96,  67,  56,  97]],\n",
      "\n",
      "         [[ 17,  25,  98,  68],\n",
      "          [ 76, 122, 101,  95],\n",
      "          [115,  69,  13,  88],\n",
      "          ...,\n",
      "          [ 98,  47,  96,  47],\n",
      "          [106, 125,  79,  95],\n",
      "          [ 49,  41,  20,  14]],\n",
      "\n",
      "         [[  5,  31,  22,  83],\n",
      "          [ 77, 119,  17,  13],\n",
      "          [ 89,   0,  74,  74],\n",
      "          ...,\n",
      "          [  0, 115,  19, 118],\n",
      "          [119,  72, 107,  11],\n",
      "          [ 14, 126,  13,  29]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 67,  91,   6,  25],\n",
      "          [ 27,  34, 124,  21],\n",
      "          [ 41,  30,  74,  48],\n",
      "          ...,\n",
      "          [121,  69,  64, 103],\n",
      "          [ 56, 120,  34,  28],\n",
      "          [ 31,  36, 115,  53]],\n",
      "\n",
      "         [[ 14,  20,  81,  20],\n",
      "          [  9, 113,  80,  94],\n",
      "          [ 63, 113,  20,  20],\n",
      "          ...,\n",
      "          [ 25,  31,  81,  90],\n",
      "          [ 18,   1,  34,  37],\n",
      "          [ 82,  48,   0,   8]],\n",
      "\n",
      "         [[ 85,   0,  57,  18],\n",
      "          [ 25,  23,  94,  97],\n",
      "          [ 85,  85, 108,  65],\n",
      "          ...,\n",
      "          [  2,  92,   6,  71],\n",
      "          [ 35, 104, 102,  25],\n",
      "          [104,  71,  85, 107]]],\n",
      "\n",
      "\n",
      "        [[[ 11,  63, 105,  68],\n",
      "          [ 42,  80, 110, 119],\n",
      "          [  6,  18,  51,  56],\n",
      "          ...,\n",
      "          [ 94,  13, 108, 109],\n",
      "          [109, 107,  84,  34],\n",
      "          [  2,  33,  34,  74]],\n",
      "\n",
      "         [[109,  39, 113,   5],\n",
      "          [106,  83, 122,   6],\n",
      "          [  1,  50,  68,  29],\n",
      "          ...,\n",
      "          [ 22,  71,  31,  27],\n",
      "          [ 22,  12, 124, 126],\n",
      "          [100,  12,  64, 112]],\n",
      "\n",
      "         [[ 61,  98, 105,  22],\n",
      "          [ 34,  40,  85,  80],\n",
      "          [119,  71,   6,  43],\n",
      "          ...,\n",
      "          [ 38, 124,  90,   7],\n",
      "          [ 19,  41,  83, 108],\n",
      "          [  1, 112,  14,   0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 12,  17,  80,  17],\n",
      "          [ 99,  53,  24,  25],\n",
      "          [ 11,  11,  96,  16],\n",
      "          ...,\n",
      "          [115, 113,  56,  40],\n",
      "          [117, 124, 114,  28],\n",
      "          [ 96, 121,  68,  12]],\n",
      "\n",
      "         [[116, 109,  74,  66],\n",
      "          [ 47,  37,  97,  83],\n",
      "          [ 51,  15,  45,  73],\n",
      "          ...,\n",
      "          [119,  15, 121,  71],\n",
      "          [ 52,   9,  48,  26],\n",
      "          [ 49,  73, 108,  68]],\n",
      "\n",
      "         [[  7, 118,  54,  94],\n",
      "          [ 94,  36, 109,  56],\n",
      "          [116,   6,  13,  70],\n",
      "          ...,\n",
      "          [ 92, 113,  53, 116],\n",
      "          [ 79, 102,  85,  30],\n",
      "          [ 66,  53,  84,  19]]],\n",
      "\n",
      "\n",
      "        [[[  3, 102,  24, 104],\n",
      "          [ 12,  54,  57,  29],\n",
      "          [  8,  27, 103,  15],\n",
      "          ...,\n",
      "          [ 48, 111,  55,  34],\n",
      "          [109, 109,  19,  26],\n",
      "          [ 42,  30,   8,  82]],\n",
      "\n",
      "         [[ 16,   2,  40,  49],\n",
      "          [ 65,  49,  59,  72],\n",
      "          [ 55,  42,  52,  13],\n",
      "          ...,\n",
      "          [ 57, 124,  66,  18],\n",
      "          [ 61, 120,  19,  94],\n",
      "          [ 82,  25, 116, 125]],\n",
      "\n",
      "         [[ 63,  76,  25, 107],\n",
      "          [ 60,  96,  47,  54],\n",
      "          [  3,  47,  17, 105],\n",
      "          ...,\n",
      "          [ 99,  16,  43,  93],\n",
      "          [120,  63,  57,  50],\n",
      "          [ 19,  85,  81,  76]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[122,  50,  84,  30],\n",
      "          [ 22,  55, 113, 119],\n",
      "          [ 98, 118,  86,  51],\n",
      "          ...,\n",
      "          [ 95,  66,  36, 117],\n",
      "          [103,  16,  69,   8],\n",
      "          [ 33,  41,  65,  42]],\n",
      "\n",
      "         [[ 54,  73,  65,  26],\n",
      "          [ 50,  10,  95,  99],\n",
      "          [117,  74,  91,  61],\n",
      "          ...,\n",
      "          [ 38, 103,   4,  62],\n",
      "          [  4, 106, 103, 119],\n",
      "          [ 48,  21,  17, 115]],\n",
      "\n",
      "         [[ 34, 124,  96,  80],\n",
      "          [ 43, 118,   4,  17],\n",
      "          [  9,  96, 102,  81],\n",
      "          ...,\n",
      "          [ 75,  30,  22,  24],\n",
      "          [ 10,  91,  84,  26],\n",
      "          [ 70,  52,  20,  25]]],\n",
      "\n",
      "\n",
      "        [[[ 76,  16,  38,  48],\n",
      "          [ 39, 119,  84,  81],\n",
      "          [ 58,  49,  16,  39],\n",
      "          ...,\n",
      "          [ 63,  59, 111,  83],\n",
      "          [111,  46,  14, 113],\n",
      "          [ 38,  53,   1,  20]],\n",
      "\n",
      "         [[ 35,  46,  43,  36],\n",
      "          [ 88, 104,  60, 113],\n",
      "          [ 32,  85,   5,  62],\n",
      "          ...,\n",
      "          [ 88,  51,  84,  16],\n",
      "          [ 79,  61,  33,   1],\n",
      "          [119,   6,  66, 102]],\n",
      "\n",
      "         [[ 37,  45,   6,  80],\n",
      "          [125, 111,  14,  47],\n",
      "          [  3,  33, 121,  67],\n",
      "          ...,\n",
      "          [ 32,  68, 102,  90],\n",
      "          [ 37,   2, 114,  31],\n",
      "          [114, 112,  47,  74]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  7,  52,  91,  98],\n",
      "          [109,  20,  18, 111],\n",
      "          [105,  45, 107, 121],\n",
      "          ...,\n",
      "          [ 72,  61, 120, 105],\n",
      "          [ 23,  16,  54, 106],\n",
      "          [ 87,  54,  37, 101]],\n",
      "\n",
      "         [[ 66,  87,  37,  64],\n",
      "          [ 22, 104,  49,  89],\n",
      "          [ 92,  25,  76,  41],\n",
      "          ...,\n",
      "          [ 25,  36, 113,  84],\n",
      "          [ 41,  81,  69, 112],\n",
      "          [ 41,  53,  24, 111]],\n",
      "\n",
      "         [[  6,  13,  94,  39],\n",
      "          [ 17, 112,  71,  38],\n",
      "          [123,  15, 101,  63],\n",
      "          ...,\n",
      "          [  3,   9,  67,  23],\n",
      "          [ 38, 126, 109,  25],\n",
      "          [ 49, 103,  81,  54]]]], device='cuda:0', dtype=torch.int8)\n",
      "\n",
      "y - torch.Size([4, 16, 16, 4])\n",
      "tensor([[[[112, 122, 101,  95],\n",
      "          [115, 126, 125, 100],\n",
      "          [125,  77,  92, 120],\n",
      "          ...,\n",
      "          [105,  89, 111, 121],\n",
      "          [123, 124, 101, 113],\n",
      "          [106, 125,  79,  97]],\n",
      "\n",
      "         [[ 77, 122, 108, 105],\n",
      "          [ 89,  90,  94,  90],\n",
      "          [100,  72,  97, 118],\n",
      "          ...,\n",
      "          [ 77, 108, 119, 116],\n",
      "          [ 91, 115,  76, 118],\n",
      "          [119, 126, 107, 108]],\n",
      "\n",
      "         [[112, 105, 126,  77],\n",
      "          [119, 119, 124,  51],\n",
      "          [119, 121,  47,  92],\n",
      "          ...,\n",
      "          [ 96,  99,  83,  80],\n",
      "          [102,  78,  97, 114],\n",
      "          [113, 119, 112,  87]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[122, 109,  65, 101],\n",
      "          [110,  46, 118, 100],\n",
      "          [122,  71, 116, 110],\n",
      "          ...,\n",
      "          [107, 104,  88, 122],\n",
      "          [122,  88, 116,  93],\n",
      "          [ 43, 102, 100, 118]],\n",
      "\n",
      "         [[ 90, 108, 124, 122],\n",
      "          [ 51, 116,  93,  64],\n",
      "          [102, 117, 101,  82],\n",
      "          ...,\n",
      "          [114,  93,  98, 111],\n",
      "          [121,  82, 105, 109],\n",
      "          [123, 120, 115, 101]],\n",
      "\n",
      "         [[ 85, 113,  94,  97],\n",
      "          [124, 113, 108,  65],\n",
      "          [ 98,  63, 120,  51],\n",
      "          ...,\n",
      "          [109, 103, 107, 122],\n",
      "          [ 25,  94,  81,  98],\n",
      "          [104, 104, 102, 107]]],\n",
      "\n",
      "\n",
      "        [[[109,  83, 122, 119],\n",
      "          [ 83, 107, 112,  61],\n",
      "          [125, 100,  87,  54],\n",
      "          ...,\n",
      "          [ 92, 104, 116,  80],\n",
      "          [ 94,  89, 108, 109],\n",
      "          [109, 107, 124, 126]],\n",
      "\n",
      "         [[ 70,  98, 105,  80],\n",
      "          [119,  97, 111,  43],\n",
      "          [ 99,  49,  83,  72],\n",
      "          ...,\n",
      "          [122,  95, 112,  98],\n",
      "          [ 96, 124, 114, 106],\n",
      "          [110, 112, 117, 108]],\n",
      "\n",
      "         [[ 42, 106,  73,  97],\n",
      "          [ 75, 126, 106, 111],\n",
      "          [113,  93,  91, 120],\n",
      "          ...,\n",
      "          [ 97,  87, 118, 105],\n",
      "          [115,  48, 115,  99],\n",
      "          [115,  54, 103, 109]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 99,  58,  99, 124],\n",
      "          [119,  87, 113,  37],\n",
      "          [113,  94,  70, 125],\n",
      "          ...,\n",
      "          [ 87,  89, 113, 108],\n",
      "          [125, 111, 119, 112],\n",
      "          [124, 125,  92, 120]],\n",
      "\n",
      "         [[ 99,  53, 103,  63],\n",
      "          [109, 101, 118, 111],\n",
      "          [125,  76, 112,  63],\n",
      "          ...,\n",
      "          [115,  81, 100,  96],\n",
      "          [115, 113, 112,  97],\n",
      "          [117, 124, 114,  67]],\n",
      "\n",
      "         [[116, 118, 109,  94],\n",
      "          [116,  81, 104,  88],\n",
      "          [101,  98, 123,  57],\n",
      "          ...,\n",
      "          [ 95,  75, 111, 114],\n",
      "          [119, 113, 121, 117],\n",
      "          [ 79, 102, 108,  68]]],\n",
      "\n",
      "\n",
      "        [[[ 65, 102,  59, 104],\n",
      "          [ 55,  84, 103,  39],\n",
      "          [114, 124, 117,  93],\n",
      "          ...,\n",
      "          [ 63, 102, 104, 110],\n",
      "          [115, 124, 126,  47],\n",
      "          [109, 120, 116, 125]],\n",
      "\n",
      "         [[ 81,  96,  94, 107],\n",
      "          [104, 125,  88, 105],\n",
      "          [ 96,  99,  71, 101],\n",
      "          ...,\n",
      "          [102,  53, 120, 124],\n",
      "          [124, 100,  98, 105],\n",
      "          [120,  85,  97,  80]],\n",
      "\n",
      "         [[ 94, 111,  67, 113],\n",
      "          [124, 121,  78, 114],\n",
      "          [114, 106, 110,  32],\n",
      "          ...,\n",
      "          [110,  63, 102,  93],\n",
      "          [119,  90, 116, 116],\n",
      "          [ 99,  96, 118, 109]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[111, 123, 118,  91],\n",
      "          [108,  89,  84,  91],\n",
      "          [109, 106, 122, 115],\n",
      "          ...,\n",
      "          [104,  82, 118,  52],\n",
      "          [ 66,  48, 118, 101],\n",
      "          [122,  74, 123, 124]],\n",
      "\n",
      "         [[122, 124, 113, 119],\n",
      "          [105, 119, 112, 110],\n",
      "          [115,  84, 102, 120],\n",
      "          ...,\n",
      "          [110,  84, 125, 125],\n",
      "          [115,  66,  48, 117],\n",
      "          [103, 103, 119, 125]],\n",
      "\n",
      "         [[ 54, 124,  96,  99],\n",
      "          [123,  96, 102, 123],\n",
      "          [113, 118,  56, 114],\n",
      "          ...,\n",
      "          [116, 113,  27, 118],\n",
      "          [ 75, 103, 100,  69],\n",
      "          [ 70, 106, 103, 119]]],\n",
      "\n",
      "\n",
      "        [[[ 88, 119,  84, 113],\n",
      "          [ 76,  85,  73, 122],\n",
      "          [113,  89,  42, 108],\n",
      "          ...,\n",
      "          [126, 121, 121, 122],\n",
      "          [104, 121, 117,  99],\n",
      "          [119,  61,  66, 113]],\n",
      "\n",
      "         [[125, 111, 117, 113],\n",
      "          [119,  46, 121, 111],\n",
      "          [ 85, 114, 122,  60],\n",
      "          ...,\n",
      "          [ 88, 110,  69, 122],\n",
      "          [ 82,  98, 102,  90],\n",
      "          [114, 112, 114,  74]],\n",
      "\n",
      "         [[121, 123, 123, 125],\n",
      "          [ 93, 124,  93, 117],\n",
      "          [ 97,  88,  61, 114],\n",
      "          ...,\n",
      "          [101, 104, 104,  86],\n",
      "          [ 99, 121,  95, 105],\n",
      "          [ 79, 108,  23,  76]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 59,  78, 102,  99],\n",
      "          [118,  95, 101, 123],\n",
      "          [109, 106,  99, 105],\n",
      "          ...,\n",
      "          [ 69,  96, 120, 102],\n",
      "          [114, 121, 116,  86],\n",
      "          [113,  69, 104,  92]],\n",
      "\n",
      "         [[109,  52,  91, 111],\n",
      "          [105, 114, 107, 121],\n",
      "          [106, 119,  83,  79],\n",
      "          ...,\n",
      "          [118, 119, 112, 113],\n",
      "          [117, 118, 120, 105],\n",
      "          [ 87,  78, 121, 114]],\n",
      "\n",
      "         [[ 66, 112,  94,  89],\n",
      "          [123,  63, 125, 114],\n",
      "          [ 60, 113, 105, 106],\n",
      "          ...,\n",
      "          [124, 119, 101, 117],\n",
      "          [ 34,  72, 113,  84],\n",
      "          [ 49, 126, 109, 112]]]], device='cuda:0', dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x - {x.shape} \\n{x}\\n\")\n",
    "print(f\"y - {y.shape}\\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x - torch.Size([4, 32, 32, 4]) \n",
      "tensor([[[[118,  55,  55,  16],\n",
      "          [118,   4,  35,  35],\n",
      "          [ 89,  77,  19,  56],\n",
      "          ...,\n",
      "          [ 40,  59,  59,  79],\n",
      "          [105,  37,  20,  19],\n",
      "          [ 68,  91,  66,  62]],\n",
      "\n",
      "         [[ 43,  75,  77, 115],\n",
      "          [122,  33,  27,  56],\n",
      "          [ 33,  46,  47,  14],\n",
      "          ...,\n",
      "          [ 85,   9,  34, 105],\n",
      "          [105, 102,  17,  76],\n",
      "          [ 61, 125, 119,  13]],\n",
      "\n",
      "         [[ 84, 111,  36,  98],\n",
      "          [  6,  15, 126,  61],\n",
      "          [ 72, 119,  51,   9],\n",
      "          ...,\n",
      "          [ 64,  71,  34,  72],\n",
      "          [102, 118,  28,  70],\n",
      "          [ 38,  77,  31,  34]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[109,  77,  85, 124],\n",
      "          [ 31,  32, 103,  92],\n",
      "          [ 36,   0,  40, 119],\n",
      "          ...,\n",
      "          [ 48,   9, 111,  87],\n",
      "          [ 52,  86,  99,  39],\n",
      "          [ 90, 119, 114,  26]],\n",
      "\n",
      "         [[ 27,  33,   4,  99],\n",
      "          [ 82, 119,  22,  89],\n",
      "          [108,  56,  23,  90],\n",
      "          ...,\n",
      "          [ 95,  46,  32,  63],\n",
      "          [ 68,  38,  67, 110],\n",
      "          [ 21,  36,  60, 117]],\n",
      "\n",
      "         [[ 16, 104,  77,  41],\n",
      "          [ 13,  75,  15,  68],\n",
      "          [ 66,  94,  77,   6],\n",
      "          ...,\n",
      "          [100,  60,  35,  98],\n",
      "          [ 31, 115,  50, 115],\n",
      "          [ 31, 114,  19,  55]]],\n",
      "\n",
      "\n",
      "        [[[ 39,  76, 109,  41],\n",
      "          [116,  26,  97,  17],\n",
      "          [108,  71,  16,  80],\n",
      "          ...,\n",
      "          [ 14,  62,  42, 126],\n",
      "          [111,   4,  32, 121],\n",
      "          [ 21,  46,  47,  44]],\n",
      "\n",
      "         [[ 42,  13,   4,  50],\n",
      "          [114,  30, 106,  81],\n",
      "          [ 80, 120,  36, 104],\n",
      "          ...,\n",
      "          [ 81,  12,  18,  67],\n",
      "          [ 67,  94,   2,  11],\n",
      "          [125,  90,  26,  39]],\n",
      "\n",
      "         [[ 25, 111,   5,  57],\n",
      "          [ 48, 101, 100,  20],\n",
      "          [117,   7,  55,  94],\n",
      "          ...,\n",
      "          [114,  13,  83,  72],\n",
      "          [  9,  46,  23, 115],\n",
      "          [117, 126,  56,  29]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 69,  77, 102,  38],\n",
      "          [ 80,  28,  57, 105],\n",
      "          [ 52,  28,  10,  45],\n",
      "          ...,\n",
      "          [ 26,  36,  22,  43],\n",
      "          [107, 101,  88,  70],\n",
      "          [ 80,  37, 117,  94]],\n",
      "\n",
      "         [[ 85,  90,  18,  65],\n",
      "          [ 13,   1,   2,   7],\n",
      "          [ 94, 124,  38,  57],\n",
      "          ...,\n",
      "          [ 60,  59,  69,  87],\n",
      "          [ 44,  90,  42, 113],\n",
      "          [ 96, 119,  34,  11]],\n",
      "\n",
      "         [[ 26,  77,  75,  60],\n",
      "          [ 39,  89,  95, 121],\n",
      "          [ 61,  73,  38,  23],\n",
      "          ...,\n",
      "          [ 83,  59,  91,  66],\n",
      "          [ 42,  63,   8,  79],\n",
      "          [ 81,   3,  66,  70]]],\n",
      "\n",
      "\n",
      "        [[[  2,  12, 118, 114],\n",
      "          [ 89,  83,   9,  13],\n",
      "          [ 34, 110,  69,   8],\n",
      "          ...,\n",
      "          [ 53,  72,  64,  76],\n",
      "          [ 22,  64,  53,  75],\n",
      "          [117, 101,  75, 113]],\n",
      "\n",
      "         [[  8,  98,  75,  53],\n",
      "          [ 63,  72,  75,  49],\n",
      "          [ 60,   8,  61,   9],\n",
      "          ...,\n",
      "          [ 78, 120,  42, 118],\n",
      "          [ 65,  23,  35, 101],\n",
      "          [104,  34,  89,  17]],\n",
      "\n",
      "         [[ 96,  18, 123, 109],\n",
      "          [ 18,  74,  54,  96],\n",
      "          [101,  32,  55,  33],\n",
      "          ...,\n",
      "          [ 16,  32,  42,  94],\n",
      "          [ 96,   0,  27,  98],\n",
      "          [ 26,  27,  10, 106]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 38, 111,   0,  59],\n",
      "          [ 62,  59,   9,  29],\n",
      "          [ 61,  38,  69, 109],\n",
      "          ...,\n",
      "          [ 49,  80, 113, 107],\n",
      "          [109,  85, 112,  75],\n",
      "          [104,  40, 122,  96]],\n",
      "\n",
      "         [[ 15,  24,  33,  28],\n",
      "          [ 76,  18,  67,  91],\n",
      "          [ 76,  19,  42,  51],\n",
      "          ...,\n",
      "          [ 47,  28, 117,  44],\n",
      "          [ 70,  43,  82,  48],\n",
      "          [ 31,  29,  19, 119]],\n",
      "\n",
      "         [[ 17,  10,   0,  43],\n",
      "          [ 97,  19,  86,  58],\n",
      "          [118,  48, 116,  60],\n",
      "          ...,\n",
      "          [ 36,   4, 104,  93],\n",
      "          [ 24,  32,  31,  26],\n",
      "          [ 88,  37,  64,  76]]],\n",
      "\n",
      "\n",
      "        [[[103, 102,  27,  94],\n",
      "          [ 67,  74,  75,  83],\n",
      "          [ 83, 116, 120,  79],\n",
      "          ...,\n",
      "          [ 11,  88, 124,  50],\n",
      "          [111,  35, 113,  93],\n",
      "          [ 35,  55,   0,  89]],\n",
      "\n",
      "         [[ 54,  75, 110, 124],\n",
      "          [ 58,  98,  25,  26],\n",
      "          [102, 102, 120,   7],\n",
      "          ...,\n",
      "          [ 79,   6,  48,  14],\n",
      "          [109, 119,  18,  38],\n",
      "          [103,  44, 111,  73]],\n",
      "\n",
      "         [[ 17,  56,  63, 122],\n",
      "          [ 89,  34, 108,  66],\n",
      "          [ 84,  48, 116,  14],\n",
      "          ...,\n",
      "          [ 96,  46, 107,  68],\n",
      "          [ 71, 104,  77,  18],\n",
      "          [ 86,  17,  95, 118]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 87,  60,  58,  24],\n",
      "          [ 29,  34,   9,   0],\n",
      "          [ 32, 116,  96, 116],\n",
      "          ...,\n",
      "          [ 56, 125, 111,  36],\n",
      "          [ 17,  45, 116,  77],\n",
      "          [124,  18, 111,  70]],\n",
      "\n",
      "         [[ 78,  11,  12,  83],\n",
      "          [ 77,  38,  93,  61],\n",
      "          [  7,  20,   5,  65],\n",
      "          ...,\n",
      "          [ 15,  13, 123,  86],\n",
      "          [ 13,  27,  21,  42],\n",
      "          [112,  23,  15, 103]],\n",
      "\n",
      "         [[104,  62,  60,  93],\n",
      "          [ 44,   7,  19,  75],\n",
      "          [ 28, 102,  46,  50],\n",
      "          ...,\n",
      "          [ 11,  35,   6,  33],\n",
      "          [ 58, 110, 126,  48],\n",
      "          [104,  16,  58, 116]]]], device='cuda:0', dtype=torch.int8)\n",
      "\n",
      "y - torch.Size([4, 16, 16, 4])\n",
      "tensor([[[[122,  75,  77, 115],\n",
      "          [106,  85,  70, 112],\n",
      "          [115, 105, 119, 122],\n",
      "          ...,\n",
      "          [ 95, 119, 114,  97],\n",
      "          [ 97,  59, 117, 106],\n",
      "          [105, 125, 119,  76]],\n",
      "\n",
      "         [[ 84, 111, 126,  98],\n",
      "          [ 79, 119, 104, 104],\n",
      "          [ 57, 108, 111, 122],\n",
      "          ...,\n",
      "          [ 63, 106,  65, 113],\n",
      "          [ 73, 112, 121,  72],\n",
      "          [110, 118, 118,  98]],\n",
      "\n",
      "         [[125,  98, 110, 107],\n",
      "          [ 88, 114, 118,  44],\n",
      "          [ 56,  86, 111, 115],\n",
      "          ...,\n",
      "          [117,  93, 109, 105],\n",
      "          [104,  87, 101, 122],\n",
      "          [ 98, 111, 102, 122]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 89, 121, 118,  87],\n",
      "          [119, 126, 105,  97],\n",
      "          [118, 121, 118,  85],\n",
      "          ...,\n",
      "          [ 54, 102, 126, 102],\n",
      "          [124, 116,  96, 119],\n",
      "          [110,  96, 107, 104]],\n",
      "\n",
      "         [[109,  77, 112, 124],\n",
      "          [ 71, 102, 101, 119],\n",
      "          [ 67, 107, 102, 106],\n",
      "          ...,\n",
      "          [ 71,  86, 107, 116],\n",
      "          [ 73, 112, 111, 115],\n",
      "          [105, 119, 114, 125]],\n",
      "\n",
      "         [[ 82, 119,  77,  99],\n",
      "          [108, 126,  77,  94],\n",
      "          [105,  85,  98,  85],\n",
      "          ...,\n",
      "          [105, 117, 118, 120],\n",
      "          [100,  80, 105,  98],\n",
      "          [ 68, 115,  67, 117]]],\n",
      "\n",
      "\n",
      "        [[[116,  76, 109,  81],\n",
      "          [108, 120,  90, 107],\n",
      "          [125,  52,  99, 119],\n",
      "          ...,\n",
      "          [110, 121,  86, 102],\n",
      "          [105,  62,  46, 126],\n",
      "          [125,  94,  47, 121]],\n",
      "\n",
      "         [[ 48, 111, 100,  80],\n",
      "          [117, 118, 100,  94],\n",
      "          [ 78,  97, 111, 123],\n",
      "          ...,\n",
      "          [102,  80,  95, 105],\n",
      "          [116,  69, 107, 110],\n",
      "          [117, 126, 114, 115]],\n",
      "\n",
      "         [[118,  79,  86,  75],\n",
      "          [119,  91,  99,  97],\n",
      "          [107,  96, 115, 116],\n",
      "          ...,\n",
      "          [ 49,  82, 105,  58],\n",
      "          [ 45, 118,  88, 109],\n",
      "          [ 73,  71,  87, 124]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[102, 125,  92,  98],\n",
      "          [ 92, 103,  91, 120],\n",
      "          [ 76,  93, 126, 123],\n",
      "          ...,\n",
      "          [112, 109,  87, 118],\n",
      "          [ 85,  80, 122, 123],\n",
      "          [115,  76, 116, 125]],\n",
      "\n",
      "         [[123,  77, 102, 105],\n",
      "          [122, 112,  77,  96],\n",
      "          [117,  70, 108, 101],\n",
      "          ...,\n",
      "          [ 96, 111, 121, 126],\n",
      "          [120,  86,  89,  96],\n",
      "          [107, 107, 117, 122]],\n",
      "\n",
      "         [[ 85,  90,  95, 121],\n",
      "          [ 94, 124, 126,  95],\n",
      "          [ 82, 113, 125, 123],\n",
      "          ...,\n",
      "          [115, 120, 116,  81],\n",
      "          [ 83, 125,  93,  87],\n",
      "          [ 96, 119,  66, 113]]],\n",
      "\n",
      "\n",
      "        [[[ 89,  98, 118, 114],\n",
      "          [ 60, 110,  96,  40],\n",
      "          [ 86, 115, 111, 123],\n",
      "          ...,\n",
      "          [115,  74, 105, 126],\n",
      "          [117, 120,  64, 118],\n",
      "          [117, 101,  89, 113]],\n",
      "\n",
      "         [[108,  95, 123, 109],\n",
      "          [101, 107, 114,  80],\n",
      "          [117,  94, 105, 119],\n",
      "          ...,\n",
      "          [ 93, 119, 120, 124],\n",
      "          [117, 111, 107, 113],\n",
      "          [ 96,  27,  97, 123]],\n",
      "\n",
      "         [[ 82,  92, 107, 126],\n",
      "          [ 95, 100, 123,  98],\n",
      "          [121,  99, 124, 106],\n",
      "          ...,\n",
      "          [120,  79, 126, 123],\n",
      "          [ 72, 121, 111,  63],\n",
      "          [100,  71, 114,  60]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 81, 124, 112,  46],\n",
      "          [ 90,  83, 103,  75],\n",
      "          [ 81,  84,  79, 112],\n",
      "          ...,\n",
      "          [119,  73, 121,  95],\n",
      "          [115,  91,  88, 124],\n",
      "          [106,  83, 112, 126]],\n",
      "\n",
      "         [[123, 111,  86,  90],\n",
      "          [119,  96,  69, 109],\n",
      "          [ 99,  62, 102, 108],\n",
      "          ...,\n",
      "          [122,  78,  90, 102],\n",
      "          [109,  93, 113, 107],\n",
      "          [109,  85, 122,  96]],\n",
      "\n",
      "         [[ 97,  24,  86,  91],\n",
      "          [118, 124, 116, 114],\n",
      "          [ 88,  89,  77, 115],\n",
      "          ...,\n",
      "          [ 68, 121, 121, 105],\n",
      "          [105, 122, 117, 126],\n",
      "          [ 88,  43,  82, 119]]],\n",
      "\n",
      "\n",
      "        [[[103, 102, 110, 124],\n",
      "          [118, 124, 120, 115],\n",
      "          [ 89,  95, 103, 121],\n",
      "          ...,\n",
      "          [ 75, 105,  93, 119],\n",
      "          [113,  88, 124,  50],\n",
      "          [111, 119, 113,  93]],\n",
      "\n",
      "         [[ 89, 111, 108, 122],\n",
      "          [ 84,  85, 116,  71],\n",
      "          [ 78,  91, 112, 104],\n",
      "          ...,\n",
      "          [108, 120, 113,  87],\n",
      "          [108,  46, 107,  94],\n",
      "          [ 86, 104, 112, 118]],\n",
      "\n",
      "         [[ 92, 105, 114, 100],\n",
      "          [121, 116, 100,  97],\n",
      "          [124, 106, 109,  99],\n",
      "          ...,\n",
      "          [ 88, 112,  86, 124],\n",
      "          [119,  97,  94, 119],\n",
      "          [ 95,  84, 111, 110]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[101,  73,  73, 114],\n",
      "          [ 87,  86, 118, 122],\n",
      "          [109, 102, 126, 110],\n",
      "          ...,\n",
      "          [102,  82, 116, 111],\n",
      "          [104,  83, 101,  99],\n",
      "          [109,  69,  98,  69]],\n",
      "\n",
      "         [[102, 119, 104,  86],\n",
      "          [ 94, 122, 108, 116],\n",
      "          [119,  50,  39,  64],\n",
      "          ...,\n",
      "          [ 58,  94,  81,  88],\n",
      "          [ 90, 125, 111, 118],\n",
      "          [124, 125, 116,  78]],\n",
      "\n",
      "         [[104,  62,  93,  93],\n",
      "          [123, 102, 103,  65],\n",
      "          [ 48, 120,  92, 118],\n",
      "          ...,\n",
      "          [119, 123,  89,  88],\n",
      "          [ 63, 115, 123,  86],\n",
      "          [112, 110, 126, 116]]]], device='cuda:0', dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "avg_pool = IntPool(mode=1)\n",
    "x = torch.randint(0, 127,(4,32,32,4), dtype=torch.int8).cuda()\n",
    "with torch.no_grad():\n",
    "    y = pool(x)\n",
    "print(f\"x - {x.shape} \\n{x}\\n\")\n",
    "print(f\"y - {y.shape}\\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch conv layer parmeter shape 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 3])\n",
      "torch.Size([12, 4, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.vgg.vgg16(pretrained=True)\n",
    "print(model.features[0].weight.shape)\n",
    "c = torch.nn.Conv2d(4,12,3,1,1)\n",
    "print(c.weight.shape) # NCHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "import cutlassconv\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "class IntConv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride =1, padding =1):\n",
    "        super(IntConv2d,self).__init__()\n",
    "        self.weight = torch.randint(-127,127,(out_channels, kernel_size, kernel_size, in_channels), dtype=torch.int8)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self,x):\n",
    "        # trans_weight = torch.flip(self.weight,[1,2]).transpose(0,3).contiguous()\n",
    "        # trans_weight = self.weight.permute(0,2,3,1).contiguous()\n",
    "        trans_weight = self.weight\n",
    "        return cutlassconv.int8_conv(x,trans_weight)\n",
    "    \n",
    "    def cuda(self):\n",
    "        self.weight = self.weight.cuda()\n",
    "        \n",
    "## cutlass는 16의 배수만\n",
    "input_channel= 16\n",
    "conv = IntConv2d(input_channel,32,3,1,1)\n",
    "print(conv.weight.shape)\n",
    "x = torch.randint(0,127,(1,32,32,input_channel), dtype=torch.int8).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.cuda()\n",
    "    y = conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x data - torch.Size([1, 32, 32, 16]) \n",
      "[[1 2 3 4]]\n",
      "\n",
      "conv data - torch.Size([32, 3, 3, 16])\n",
      "tensor([[[[  53,   47,  -18,  ...,   87,  -56,  125],\n",
      "          [  79,   58,   13,  ...,  -32,   32,   56],\n",
      "          [-115,  -43,  -29,  ...,   45,  -96,  -55]],\n",
      "\n",
      "         [[  74,   46,   42,  ...,   87,  -40,  -29],\n",
      "          [ -96,   22,  -64,  ...,   94,   25,   51],\n",
      "          [  60,  -28,   48,  ...,   77,   66,   42]],\n",
      "\n",
      "         [[ 117,  120,  -43,  ...,    1,   72, -119],\n",
      "          [  65,  -41,  -36,  ...,   14,  -78, -100],\n",
      "          [ -54,   30,  -33,  ...,  -96, -103,  -94]]],\n",
      "\n",
      "\n",
      "        [[[ -74,  -85,   49,  ...,   37,  118,  118],\n",
      "          [ -70, -110,   82,  ...,   -7,    4, -126],\n",
      "          [  22,  -35,   -7,  ...,   85,    2,   43]],\n",
      "\n",
      "         [[  31, -104,  -15,  ...,   65,  -91,   13],\n",
      "          [  67,   14,  -42,  ..., -110,  -75,   18],\n",
      "          [-119,  115,  -66,  ...,  -37,   36,    1]],\n",
      "\n",
      "         [[ -86,  125,   92,  ...,   47,   85,  -38],\n",
      "          [  15,   63,  -13,  ...,   98,  120,  -11],\n",
      "          [ -26,  109,   86,  ...,  -69,   69,  114]]],\n",
      "\n",
      "\n",
      "        [[[ -56, -121,   31,  ...,  -17, -126, -122],\n",
      "          [-102,   -6,   52,  ...,   90,  -89,  -69],\n",
      "          [ -66,  123,   52,  ...,  101,  -19,  -78]],\n",
      "\n",
      "         [[  -1,   -8,  106,  ...,  -45,  -69,  110],\n",
      "          [-112,   13,  -72,  ..., -124,  -31,  -88],\n",
      "          [  16,   41,   15,  ...,   16,  -29, -108]],\n",
      "\n",
      "         [[   9,  -43,  -70,  ...,  -70,   23,  -41],\n",
      "          [  88,  -33,   -8,  ...,  -91,  -99,    1],\n",
      "          [ -13,  -46, -119,  ...,   89,   37,   -4]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -98,  -75,   73,  ...,  -28,   41,  -10],\n",
      "          [ -58,   80,   72,  ...,  -46,   -2,   93],\n",
      "          [  94,  -64,  -31,  ...,   -4,   57, -125]],\n",
      "\n",
      "         [[ -75,   52,   47,  ...,  -53,  -34,   42],\n",
      "          [  49,  -22,   30,  ..., -109,  115,   -7],\n",
      "          [ -33,   17,  -63,  ...,   -8,  -78,  -91]],\n",
      "\n",
      "         [[  43,  122,   55,  ...,   29,  -30,   79],\n",
      "          [ -43,  112,   93,  ...,   86, -118,  112],\n",
      "          [-127,  -98,  -79,  ..., -100,    7,   77]]],\n",
      "\n",
      "\n",
      "        [[[ -96,  -48, -113,  ...,   82,  -51, -111],\n",
      "          [ -82,   71,  -57,  ..., -125,   48,  -75],\n",
      "          [  63,  106,   98,  ..., -108,  -15,  124]],\n",
      "\n",
      "         [[  45,  103,   20,  ...,   80,  -30,  113],\n",
      "          [  62,   26,  -39,  ..., -116, -100,   24],\n",
      "          [ -73,  -61,  -56,  ...,   74,   85,  -23]],\n",
      "\n",
      "         [[  52,  -71,  -41,  ...,  -77,   54,  -10],\n",
      "          [-110,  -19,   71,  ...,   95,   45,  -92],\n",
      "          [  76,  -58,   -4,  ...,    4,  -37,  -79]]],\n",
      "\n",
      "\n",
      "        [[[  16,  124,    2,  ...,  -22,  100, -100],\n",
      "          [  69,  -36,   -9,  ..., -101,   20,  -55],\n",
      "          [  43,  -43,  109,  ...,  110,  -28, -124]],\n",
      "\n",
      "         [[  47,   18,  111,  ...,   21,  -33,    3],\n",
      "          [  97,   79,   39,  ...,   -8,  -85,   61],\n",
      "          [ 100,   89, -111,  ...,   34,  -47,  -86]],\n",
      "\n",
      "         [[ -33,   34,   96,  ...,  -98,  112,  104],\n",
      "          [  52,   -9,   33,  ...,   86,  -72,   51],\n",
      "          [ -88,  -46,   45,  ...,  -33,  121,  -12]]]], device='cuda:0',\n",
      "       dtype=torch.int8)\n",
      "\n",
      "y data - torch.Size([1, 32, 32, 32])\n",
      "tensor([[[[ -78241,   -3212,  -21387,  ...,   15288,  -22412,  -15352],\n",
      "          [ -42688,   44776,  -66224,  ...,   26731,  -13733,   43333],\n",
      "          [ -28291,   14888, -113373,  ...,   -7614,  -21399,    7162],\n",
      "          ...,\n",
      "          [ -41738,   37452,  -74007,  ...,   55980,   -1529,  -15405],\n",
      "          [ -67071,   58776,  -68157,  ...,  -30399,  -11230,  -21263],\n",
      "          [ -19465,   29022,    4708,  ...,   39901,   66081,   48493]],\n",
      "\n",
      "         [[ -39314,   11575, -103346,  ...,   11255,  -43413,   -6608],\n",
      "          [-113372,   74447,  -99543,  ...,   18576,   -7033,   31414],\n",
      "          [ -64596,   43075, -155080,  ...,  -36112,  -10774,   17227],\n",
      "          ...,\n",
      "          [ -49293,   46962, -228636,  ...,    1058,  -33388,  -21168],\n",
      "          [ -29292,  111354, -152575,  ...,   36917,  -64573,  -33094],\n",
      "          [  35043,   83154,  -55487,  ...,   99963,  -21712,   15644]],\n",
      "\n",
      "         [[ -65376,    3610,  -81384,  ...,   -7529,  -42225,  -91350],\n",
      "          [  21966,   78446, -142591,  ...,   81604,  -72068,  -55053],\n",
      "          [ -33949,   89929,  -74148,  ...,   36484,  -68921,  -55839],\n",
      "          ...,\n",
      "          [  15787,   50519, -158125,  ...,   56132,  -65131,  -42592],\n",
      "          [ -37892,   38470, -154673,  ...,   16848,  -12857,    8578],\n",
      "          [  -7725,   19530,  -92808,  ...,  102219,  -10385,   38251]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -56573,   -9273, -126523,  ...,   15322,  -94505,  -14129],\n",
      "          [ -35606,   40815, -150520,  ...,   20064,  -13766,  -20701],\n",
      "          [ -48837,   92280, -139728,  ...,   40667,  -26036,   -8676],\n",
      "          ...,\n",
      "          [  76253,   80701, -112733,  ...,   55498,  -59752,  -43237],\n",
      "          [ -39000,   92825, -132705,  ...,   26539,  -45037,   -1129],\n",
      "          [  40194,   52392,  -87704,  ...,   89056,   41823,   74476]],\n",
      "\n",
      "         [[ -43516,   -4230,  -83656,  ...,  -49749,    6462,  -59078],\n",
      "          [  -5186,   29494, -135556,  ...,   17502,  -35367,   13024],\n",
      "          [ -28306,   -8066, -120579,  ...,   24069,  -97150,    9473],\n",
      "          ...,\n",
      "          [ -37045,   69277, -116588,  ...,   -8519,   13587,  -13671],\n",
      "          [  -4073,   52861,  -91283,  ...,   66237,  -24767,  -16100],\n",
      "          [  53267,   38936,  -92755,  ...,   86816,  -38055,   38443]],\n",
      "\n",
      "         [[    811,  -21084,  -26798,  ...,   11413,  -14397,  -89300],\n",
      "          [  10861,   28603,  -73736,  ...,   29346,  -46546,  -36063],\n",
      "          [ -43564,   18071,  -97704,  ...,     804,  -27579,  -20384],\n",
      "          ...,\n",
      "          [ -12694,  -35715, -101172,  ...,    2605,  -11354,  -63605],\n",
      "          [  18020,  -29634,  -76585,  ...,    9758,   16521,  -23016],\n",
      "          [  36541,  -24270,  -78457,  ...,   22882,  -34542,  -18287]]]],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "print(f\"x data - {x.shape} \\n{x_data}\\n\")\n",
    "print(f\"conv data - {conv.weight.shape}\\n{conv.weight}\\n\")\n",
    "print(f\"y data - {y.shape}\\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import int8conv_cuda\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "class IntConv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride =1, padding =1):\n",
    "        super(IntConv2d,self).__init__()\n",
    "        # self.weight = torch.ones((out_channels, kernel_size, kernel_size, in_channels), dtype=torch.int8)\n",
    "        # self.weight[:,:,:,1] = self.weight[:,:,:,1]*2\n",
    "        # self.weight[2,:,:,:] = self.weight[2,:,:,:]*3\n",
    "\n",
    "        self.weight = torch.ones((out_channels,in_channels , kernel_size, kernel_size), dtype=torch.int8)\n",
    "        self.weight[:,1,:,:] = self.weight[:,1,:,:]*2\n",
    "        self.weight[2,:,:,:] = self.weight[2,:,:,:]*3\n",
    "        self.weight = self.weight.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self,x):\n",
    "        # trans_weight = torch.flip(self.weight,[1,2]).transpose(0,3).contiguous()\n",
    "        # trans_weight = self.weight.permute(0,2,3,1).contiguous()\n",
    "        trans_weight = self.weight\n",
    "        y = int8conv_cuda.int8_conv(x,trans_weight,self.stride, self.padding,1)\n",
    "        # y = (y > 127).int()*5\n",
    "        # y = y.type(torch.int8)\n",
    "        return y\n",
    "    \n",
    "    def cuda(self):\n",
    "        self.weight = self.weight.cuda()\n",
    "    \n",
    "# cudnn은 4의 배수만\n",
    "input_channel= 4\n",
    "conv = IntConv2d(input_channel,8,3,1,1)\n",
    "print(conv.weight.shape)\n",
    "x = torch.ones((1,3,3,input_channel), dtype=torch.int8).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 8]) cuda:0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    conv.cuda()\n",
    "    y = conv(x)\n",
    "print(y.shape, y.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x data - torch.Size([1, 3, 3, 4]) (36, 12, 4, 1),tensor([[[[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]]]], device='cuda:0', dtype=torch.int8) \n",
      "\n",
      "conv data - torch.Size([8, 3, 3, 4]), (36, 12, 4, 1), tensor([[[[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[3, 6, 3, 3],\n",
      "          [3, 6, 3, 3],\n",
      "          [3, 6, 3, 3]],\n",
      "\n",
      "         [[3, 6, 3, 3],\n",
      "          [3, 6, 3, 3],\n",
      "          [3, 6, 3, 3]],\n",
      "\n",
      "         [[3, 6, 3, 3],\n",
      "          [3, 6, 3, 3],\n",
      "          [3, 6, 3, 3]]],\n",
      "\n",
      "\n",
      "        [[[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]],\n",
      "\n",
      "         [[1, 2, 1, 1],\n",
      "          [1, 2, 1, 1],\n",
      "          [1, 2, 1, 1]]]], device='cuda:0', dtype=torch.int8)\n",
      "\n",
      "y data - torch.Size([1, 3, 3, 8]) (72, 24, 8, 1)\n",
      "tensor([[[[ 20,  20,  60,  20,  20,  20,  20,  20],\n",
      "          [ 30,  30,  90,  30,  30,  30,  30,  30],\n",
      "          [ 20,  20,  60,  20,  20,  20,  20,  20]],\n",
      "\n",
      "         [[ 30,  30,  90,  30,  30,  30,  30,  30],\n",
      "          [ 45,  45, 135,  45,  45,  45,  45,  45],\n",
      "          [ 30,  30,  90,  30,  30,  30,  30,  30]],\n",
      "\n",
      "         [[ 20,  20,  60,  20,  20,  20,  20,  20],\n",
      "          [ 30,  30,  90,  30,  30,  30,  30,  30],\n",
      "          [ 20,  20,  60,  20,  20,  20,  20,  20]]]], device='cuda:0',\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "conv_data = conv.weight.detach().cpu().numpy()\n",
    "x_data = x.detach().cpu().numpy()\n",
    "y_data = y.detach().cpu().numpy()\n",
    "print(f\"x data - {x.shape} {x.stride()},{x.data} \\n\")\n",
    "print(f\"conv data - {conv.weight.shape}, {conv.weight.stride()}, {conv.weight}\\n\")\n",
    "print(f\"y data - {y.shape} {y.stride()}\\n{y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[2., 2., 2.],\n",
      "          [2., 2., 2.],\n",
      "          [2., 2., 2.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n",
      "tensor([[[[1., 2., 1., 1.],\n",
      "          [1., 2., 1., 1.],\n",
      "          [1., 2., 1., 1.]],\n",
      "\n",
      "         [[1., 2., 1., 1.],\n",
      "          [1., 2., 1., 1.],\n",
      "          [1., 2., 1., 1.]],\n",
      "\n",
      "         [[1., 2., 1., 1.],\n",
      "          [1., 2., 1., 1.],\n",
      "          [1., 2., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "test_tensor = torch.ones((1,4,3,3))\n",
    "print(test_tensor)\n",
    "test_tensor[:,1,:,:] = test_tensor[:,1,:,:]*2\n",
    "print(test_tensor)\n",
    "trans_tensor = test_tensor.permute(0,2,3,1)\n",
    "print(trans_tensor[:,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch 와 동일한 conv가 나오는지 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "torch_conv2d = torch.nn.Conv2d(4,8,3,1,1,bias=False)\n",
    "print(torch_conv2d.weight.data.shape)\n",
    "torch_conv2d.weight.data = torch.ones((8,4,3,3),dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 3, 4]) (36, 12, 4, 1)\n",
      "torch.Size([1, 4, 3, 3]) torch.Size([1, 3, 3, 4])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import int8conv_cuda\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "class FloatConv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride =1, padding =1):\n",
    "        super(FloatConv2d,self).__init__()\n",
    "        self.weight = torch.ones((out_channels, kernel_size, kernel_size, in_channels),dtype=torch.float).to(memory_format=torch.channels_last)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self,x):\n",
    "        # trans_weight = torch.flip(self.weight,[1,2]).transpose(0,3).contiguous()\n",
    "        # trans_weight = self.weight.permute(0,2,3,1).contiguous()\n",
    "        trans_weight = self.weight\n",
    "        y = int8conv_cuda.float_conv(x,trans_weight,self.stride, self.padding,1)\n",
    "        # y = (y > 127).int()*5\n",
    "        # y = y.type(torch.int8)\n",
    "        return y\n",
    "    \n",
    "    def cuda(self):\n",
    "        self.weight = self.weight.cuda()\n",
    "    \n",
    "# cudnn은 4의 배수만\n",
    "input_channel= 4\n",
    "conv = FloatConv2d(input_channel,8,3,1,1)\n",
    "conv.weight = torch_conv2d.weight.data.permute(0,2,3,1).contiguous()\n",
    "print(conv.weight.shape, conv.weight.stride())\n",
    "torch_x = torch.ones((1,input_channel,3,3),dtype=torch.float).contiguous()\n",
    "x = torch_x.permute(0,2,3,1).contiguous()\n",
    "print(torch_x.shape, x.shape)\n",
    "print(torch.equal(torch_x.permute(0,2,3,1),x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 8]) False\n",
      "torch.Size([1, 8, 3, 3]) False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    conv.cuda()\n",
    "    torch_conv2d.cuda()\n",
    "    x= x.cuda()\n",
    "    torch_x = torch_x.cuda()\n",
    "    y = conv(x)\n",
    "    trans = torch_conv2d(torch_x)\n",
    "print(y.shape,y.is_contiguous(memory_format=torch.channels_last))\n",
    "print(trans.shape, trans.is_contiguous(memory_format=torch.channels_last))\n",
    "\n",
    "print(torch.equal(y, trans.permute(0,2,3,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y data - torch.Size([1, 3, 3, 8])\n",
      "tensor([[16., 24., 16.],\n",
      "        [24., 36., 24.],\n",
      "        [16., 24., 16.]], device='cuda:0')\n",
      "trans - torch.Size([1, 8, 3, 3])\n",
      "tensor([[[16., 24., 16.],\n",
      "         [24., 36., 24.],\n",
      "         [16., 24., 16.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "conv_data = conv.weight.detach().cpu().numpy()\n",
    "x_data = x.detach().cpu().numpy()\n",
    "y_data = y.detach().cpu().numpy()\n",
    "# print(f\"x data - {x.shape},{x.data} \\n\")\n",
    "# print(f\"conv data - {conv_data.shape}, {conv_data}\\n\")\n",
    "print(f\"y data - {y.shape}\\n{y[0,:,:,0]}\")\n",
    "print(f\"trans - {trans.shape}\\n{trans[:,0,:,:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(\n",
    "        self, features: nn.Module, num_classes: int = 100, dropout: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        # self.avgpool = IntPool(7,1,0,1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            IntLinear(25088,4096),\n",
    "            # RELU가 아닌 32bit을 8bit으로 변경하는 activation을 사용해야한다.\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            IntLinear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            IntLinear(4096,num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        print(f\"after feature {x.shape}\")\n",
    "        # x = self.avgpool(x)\n",
    "        # print(f\"after pooling {x.shape}\")\n",
    "        x = torch.flatten(x, 1)\n",
    "        print(f\"after flatten {x.shape} {x.dtype}\")\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def cuda(self):\n",
    "        for layer in model.modules():\n",
    "            if 'Int' in str(type(layer)):\n",
    "                layer.cuda()\n",
    "\n",
    "def make_layers(cfg, batch_norm: bool = False) -> nn.Sequential:\n",
    "    layers = []\n",
    "    in_channels = 4\n",
    "    for vs in cfg:\n",
    "        for v in vs:\n",
    "            v = int(v)\n",
    "            conv2d = IntConv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            layers += [conv2d, nn.ReLU()]\n",
    "            in_channels = v\n",
    "        layers += [IntPool(kernel_size=2, stride=2)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfgs = {\n",
    "    \"D\": [[64, 64], [128, 128], [256, 256, 256], [512, 512, 512],[512, 512, 512]],\n",
    "    # \"D\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n",
    "}\n",
    "\n",
    "def int_vgg(cfg: str, **kwargs) -> VGG:\n",
    "    model = VGG(make_layers(cfgs[cfg]), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = []\n",
    "before_l = []\n",
    "after_l = []\n",
    "hooks = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    modules.append(module)\n",
    "    before_l.append(input[0])\n",
    "    after_l.append(output)\n",
    "\n",
    "def add_forward_hook(net, hooks):\n",
    "    for name, layer in net._modules.items():\n",
    "        if isinstance(layer, nn.Sequential) or isinstance(layer, torchvision.models.vgg.VGG):\n",
    "            add_forward_hook(layer, hooks)\n",
    "        else:\n",
    "            hook = layer.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "            \n",
    "    return hooks\n",
    "\n",
    "def remove_forward_hook(hooks):\n",
    "    for i in hooks:\n",
    "        i.remove()\n",
    "# out = model((torch.randn(1,3,32,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class test_module(Module):\n",
    "#     def __init__(self,num_classes= 100):\n",
    "#         super(test_module,self).__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             IntLinear(512,4096),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             IntLinear(4096,4096),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             IntLinear(4096,num_classes),\n",
    "#         )\n",
    "#     def forward(self,x):\n",
    "#         for l in self.layers:\n",
    "#             x = l(x)\n",
    "#             print(x.shape, x.dtype)\n",
    "#         return x\n",
    "#     def cuda(self):\n",
    "#         for layer in model.modules():\n",
    "#             if 'Int' in str(type(layer)):\n",
    "#                 layer.cuda()\n",
    "\n",
    "# model = test_module()\n",
    "# x = torch.randint(-127,127,(1,512)).cuda()\n",
    "# model.eval()\n",
    "# model.cuda()\n",
    "# print(model.layers[0].weight)\n",
    "# print(x.dtype)\n",
    "# with torch.no_grad():\n",
    "#     y = model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after feature torch.Size([1, 7, 7, 512])\n",
      "after flatten torch.Size([1, 25088]) torch.int8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Char but found Int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/workspace/int_infer/extension.ipynb Cell 23\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m-\u001b[39m\u001b[39m127\u001b[39m,\u001b[39m127\u001b[39m,(\u001b[39m1\u001b[39m,\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m,\u001b[39m4\u001b[39m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint8)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     y \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(hooks), \u001b[39mlen\u001b[39m(modules), \u001b[39mlen\u001b[39m(before_l), \u001b[39mlen\u001b[39m(after_l))\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     remove_forward_hook(hooks)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/workspace/int_infer/extension.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mafter flatten \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/workspace/int_infer/extension.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# weight [OUT, IN} - > [IN, OUT]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# input [BATCH, IN]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     y \u001b[39m=\u001b[39m int8mm_cuda\u001b[39m.\u001b[39;49mint8_mm(x,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mcontiguous())\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Char but found Int"
     ]
    }
   ],
   "source": [
    "model = int_vgg(\"D\")\n",
    "model.eval()\n",
    "hooks = add_forward_hook(model, hooks)\n",
    "# remove hook, hook works at once\n",
    "remove_forward_hook(hooks)\n",
    "model.cuda()\n",
    "with torch.no_grad():\n",
    "    x = torch.randint(-127,127,(1,224,224,4), dtype=torch.int8).cuda()\n",
    "    y = model(x)\n",
    "    print(len(hooks), len(modules), len(before_l), len(after_l))\n",
    "    remove_forward_hook(hooks)\n",
    "    hooks=[]\n",
    "print(y.dtype, y.shape, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  77,   60,   40],\n",
      "          [  93,   43,  -67],\n",
      "          [  88,   87,  122],\n",
      "          [ 114,    7,  -48]],\n",
      "\n",
      "         [[-101,  -36,   60],\n",
      "          [  -5,  -62,  -72],\n",
      "          [ -30,   30,  -32],\n",
      "          [  34,  101,    7]],\n",
      "\n",
      "         [[  68,  120,   36],\n",
      "          [-109,  -55,    9],\n",
      "          [ -99,   61,  -12],\n",
      "          [ -57,  -66,   -7]],\n",
      "\n",
      "         [[  53,   16,   57],\n",
      "          [-116,   80,  -19],\n",
      "          [  -2,  -51,  -21],\n",
      "          [ -72,   69,   96]]]], device='cuda:0', dtype=torch.int8)\n",
      "tensor([[[[ 77,  60,  40],\n",
      "          [ 93,  43,   0],\n",
      "          [ 88,  87, 122],\n",
      "          [114,   7,   0]],\n",
      "\n",
      "         [[  0,   0,  60],\n",
      "          [  0,   0,   0],\n",
      "          [  0,  30,   0],\n",
      "          [ 34, 101,   7]],\n",
      "\n",
      "         [[ 68, 120,  36],\n",
      "          [  0,   0,   9],\n",
      "          [  0,  61,   0],\n",
      "          [  0,   0,   0]],\n",
      "\n",
      "         [[ 53,  16,  57],\n",
      "          [  0,  80,   0],\n",
      "          [  0,   0,   0],\n",
      "          [  0,  69,  96]]]], device='cuda:0', dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "i = torch.randint(-127, 127,(1,4,4,3), dtype=torch.int8).cuda()\n",
    "lay = nn.Dropout(0.5)\n",
    "\n",
    "lay.cuda()\n",
    "with torch.no_grad():\n",
    "    lay.eval()\n",
    "    y = lay(i)\n",
    "    k = nn.functional.relu(y)\n",
    "print(y)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n",
      "tensor([[ -574282,  -967096,   718499,  -854490,  -480142,  1088080,  -752827,\n",
      "          -116280,  -313422,  -976545,  -148737,  -783753,  -647241,  -154183,\n",
      "           160387,  -120619,   -25240,   725703,  -217888,  -175336, -1143886,\n",
      "          -437042,  -478281,   371989,  -784157,  -368922,    77165,  -562955,\n",
      "          -791538, -1247484,  -639698,  -404810,  -830884,  -352098,  -982993,\n",
      "         -1149701,   388485,   430506, -1301755,  -722884, -1382657,  -926842,\n",
      "          -209861,   663056,   616099, -1206340,  -615280,  -344674,   939926,\n",
      "           749104,  -966203,   174706,    50013,  -642108,  -657352,  -455320,\n",
      "          -236708,  -146362,   413926,   203769,  -677935, -1635767,  -263175,\n",
      "         -1041366,   -54477,  -756444,  -234938,  -921265,  -493369, -1095055,\n",
      "          -715730,    15213,   374301,   473852,  -453373, -1359736,   253718,\n",
      "          -337226,  -387232,  -113524,     6194,   477962, -1093968,   738111,\n",
      "          -730462,   -55275,  -436522, -1422282,   354789,  -365973,  -717804,\n",
      "           999484,  -231589,    94428,  -324601,   283188,  -638532,  -392440,\n",
      "         -1528391,   309704]], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "from models import vgg\n",
    "\n",
    "model = vgg.int_vgg16(\"D\")\n",
    "x = torch.randint(-128,127,(1,224,224,4),dtype=torch.int8).cuda()\n",
    "model.eval()\n",
    "model.cuda()\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "    print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[262145, 262145, 262145],\n",
      "        [262145, 262145, 262145],\n",
      "        [262145, 262145, 262145]])\n",
      "tensor([[294913, 294913, 294913],\n",
      "        [294913, 294913, 294913],\n",
      "        [294913, 294913, 294913]])\n",
      "tensor([[65535, 65535, 65535],\n",
      "        [65535, 65535, 65535],\n",
      "        [65535, 65535, 65535]])\n",
      "tensor([[255.9980, 255.9980, 255.9980],\n",
      "        [255.9980, 255.9980, 255.9980],\n",
      "        [255.9980, 255.9980, 255.9980]])\n",
      "tensor([[127.9980, 127.9980, 127.9980],\n",
      "        [127.9980, 127.9980, 127.9980],\n",
      "        [127.9980, 127.9980, 127.9980]])\n",
      "tensor([[127, 127, 127],\n",
      "        [127, 127, 127],\n",
      "        [127, 127, 127]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((3,3), dtype=torch.int64)*(2**18+1)\n",
    "print(x)\n",
    "x = x + 2**15\n",
    "print(x)\n",
    "x = torch.clamp(x, min=0, max=2**16-1)\n",
    "print(x)\n",
    "x = torch.sqrt(x)\n",
    "print(x)\n",
    "x = x-128\n",
    "print(x)\n",
    "x = x.type(torch.int8)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 1])\n",
      "tensor([[[[1],\n",
      "          [2],\n",
      "          [3]],\n",
      "\n",
      "         [[4],\n",
      "          [5],\n",
      "          [6]],\n",
      "\n",
      "         [[7],\n",
      "          [8],\n",
      "          [9]]]])\n",
      "torch.Size([1, 3, 3, 4])\n",
      "tensor([[[[1, 1, 1, 1],\n",
      "          [2, 2, 2, 2],\n",
      "          [3, 3, 3, 3]],\n",
      "\n",
      "         [[4, 4, 4, 4],\n",
      "          [5, 5, 5, 5],\n",
      "          [6, 6, 6, 6]],\n",
      "\n",
      "         [[7, 7, 7, 7],\n",
      "          [8, 8, 8, 8],\n",
      "          [9, 9, 9, 9]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3],[4,5,6],[7,8,9]]).reshape(1,3,3,1)\n",
    "print(a.shape, a,sep='\\n')\n",
    "a = a.repeat(1,1,1,4)\n",
    "print(a.shape, a,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv: torch.Size([1, 3, 3, 4])\n",
      "tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]]], device='cuda:0')\n",
      "x : torch.Size([1, 3, 3, 4]) \n",
      "tensor([[[[1., 1., 1., 1.],\n",
      "          [2., 2., 2., 2.],\n",
      "          [3., 3., 3., 3.]],\n",
      "\n",
      "         [[4., 4., 4., 4.],\n",
      "          [5., 5., 5., 5.],\n",
      "          [6., 6., 6., 6.]],\n",
      "\n",
      "         [[7., 7., 7., 7.],\n",
      "          [8., 8., 8., 8.],\n",
      "          [9., 9., 9., 9.]]]], device='cuda:0')\n",
      "y : torch.Size([1, 3, 3, 1])\n",
      "tensor([[[[ 48.],\n",
      "          [ 84.],\n",
      "          [ 64.]],\n",
      "\n",
      "         [[108.],\n",
      "          [180.],\n",
      "          [132.]],\n",
      "\n",
      "         [[ 96.],\n",
      "          [156.],\n",
      "          [112.]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class FloatConv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride =1, padding =1):\n",
    "        super(FloatConv2d,self).__init__()\n",
    "        self.weight = torch.ones((out_channels, kernel_size, kernel_size, in_channels),dtype=torch.float32)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self,x):\n",
    "        # trans_weight = torch.flip(self.weight,[1,2]).transpose(0,3).contiguous()\n",
    "        # trans_weight = self.weight.permute(0,2,3,1).contiguous()\n",
    "        trans_weight = self.weight\n",
    "        y = int8conv_cuda.float_conv(x,trans_weight,self.stride, self.padding,1)\n",
    "        return y\n",
    "    \n",
    "    def cuda(self):\n",
    "        self.weight = self.weight.cuda()\n",
    "\n",
    "\n",
    "input_channel= 4\n",
    "conv = FloatConv2d(input_channel,1,3,1,1)\n",
    "conv.cuda()\n",
    "print(f\"conv: {conv.weight.shape}\\n{conv.weight}\")\n",
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9]],dtype=torch.float32).reshape(1,3,3,1).repeat(1,1,1,input_channel).cuda()\n",
    "print(f\"x : {x.shape} \\n{x}\")\n",
    "with torch.no_grad():\n",
    "    y= conv(x)\n",
    "    print(f\"y : {y.shape}\\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
