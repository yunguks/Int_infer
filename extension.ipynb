{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.cpp_extension import load\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch integer data flow 는 gpu에서 사용 불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_data = torch.randint(0,255,(1,3,24,24), dtype=torch.uint8)\n",
    "# weight = torch.randint(0,255,(1,3,3,3), dtype=torch.uint8)\n",
    "\n",
    "# # b = torch.nn.functional.conv2d(int_data.cuda(), weight=weight.cuda(),stride=1)\n",
    "# b = torch.nn.functional.conv2d(int_data, weight=weight,stride=1,bias=None, padding=1, dtype=torch.uint8)\n",
    "# print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int8 torch.int8\n",
      "tensor([[1, 2, 3, 4]], device='cuda:0', dtype=torch.int8)\n",
      "tensor([[ -32,   63,  -86, -110],\n",
      "        [ -93,   29,  -37,  113],\n",
      "        [  60,  -69,   82,  -89],\n",
      "        [ 104,   85,  -20,  -49],\n",
      "        [ -65,  -92,  -74,  -59],\n",
      "        [  29,   76,  120,  126],\n",
      "        [  27,    2,  104,  -43],\n",
      "        [ -12, -105,  -91,  -48],\n",
      "        [ -80,   98,  -22,   38],\n",
      "        [  25,   90,   52,   82],\n",
      "        [-109,   95,  -89,  -36],\n",
      "        [-122,   29,  -99,  -28]], device='cuda:0', dtype=torch.int8)\n",
      "tensor([[-604,  306, -188,   18, -707, 1045,  171, -687,  202,  689, -330, -473]],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import int8mm_cuda\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "class IntLinear(Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(IntLinear,self).__init__()\n",
    "        self.weight = torch.randint(-127,127,(out_channels, in_channels), dtype=torch.int8)\n",
    "        # self.weight = torch.ones((out_channels, in_channels), dtype=torch.int8)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # weight [OUT, IN} - > [IN, OUT]\n",
    "        # input [BATCH, IN]\n",
    "        y = int8mm_cuda.int8_mm(x,self.weight.transpose(1,0).contiguous())\n",
    "        return y\n",
    "    \n",
    "    def cuda(self):\n",
    "        self.weight = self.weight.cuda()\n",
    "        \n",
    "\n",
    "mm = IntLinear(4,12)\n",
    "# x = torch.randint(-127,127,(1,4), dtype=torch.int8).cuda()\n",
    "# x = torch.ones((1,4), dtype=torch.int8).cuda()\n",
    "x = torch.tensor([[1,2,3,4]], dtype=torch.int8).cuda()\n",
    "print(x.dtype, mm.weight.dtype)\n",
    "with torch.no_grad():\n",
    "    mm.cuda()\n",
    "    y = mm(x)\n",
    "print(x)\n",
    "print(mm.weight)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -32,  -93,   60,  104,  -65,   29,   27,  -12,  -80,   25, -109, -122],\n",
      "        [  63,   29,  -69,   85,  -92,   76,    2, -105,   98,   90,   95,   29],\n",
      "        [ -86,  -37,   82,  -20,  -74,  120,  104,  -91,  -22,   52,  -89,  -99],\n",
      "        [-110,  113,  -89,  -49,  -59,  126,  -43,  -48,   38,   82,  -36,  -28]],\n",
      "       device='cuda:0', dtype=torch.int8)\n",
      "\n",
      "x data - [[1 2 3 4]]\n",
      "\n",
      "mm data - [[ -32   63  -86 -110]\n",
      " [ -93   29  -37  113]\n",
      " [  60  -69   82  -89]\n",
      " [ 104   85  -20  -49]\n",
      " [ -65  -92  -74  -59]\n",
      " [  29   76  120  126]\n",
      " [  27    2  104  -43]\n",
      " [ -12 -105  -91  -48]\n",
      " [ -80   98  -22   38]\n",
      " [  25   90   52   82]\n",
      " [-109   95  -89  -36]\n",
      " [-122   29  -99  -28]]\n",
      "mm Trans - [[ -32  -93   60  104  -65   29   27  -12  -80   25 -109 -122]\n",
      " [  63   29  -69   85  -92   76    2 -105   98   90   95   29]\n",
      " [ -86  -37   82  -20  -74  120  104  -91  -22   52  -89  -99]\n",
      " [-110  113  -89  -49  -59  126  -43  -48   38   82  -36  -28]]\n",
      "\n",
      "y - [[-92  50  68  18  61  21 -85  81 -54 -79 -74  39]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "print(mm.weight.transpose(1,0),end=\"\\n\\n\")\n",
    "x_data = x.detach().cpu().numpy()\n",
    "mm_data = mm.weight.detach().cpu().numpy()\n",
    "print(f\"x data - {x_data}\\n\")\n",
    "print(f\"mm data - {mm_data}\\nmm Trans - {mm_data.T}\\n\")\n",
    "y = x_data @ mm_data.T\n",
    "print(f\"y - {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import int8pool_cuda\n",
    "\n",
    "class IntPool(Module):\n",
    "    def __init__(self,kernel_size = 2, stride = 2, padding=0, mode=0):\n",
    "        super(IntPool,self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.mode = mode\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y = int8pool_cuda.int8_pool(x,self.kernel_size, self.stride, self.padding, self.mode)\n",
    "        # y = (y > 10).int()*5\n",
    "        # y = y.type(torch.int8)\n",
    "        return y\n",
    "\n",
    "pool = IntPool()\n",
    "x = torch.randint(0, 127,(4,32,32,4), dtype=torch.int8).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = pool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x - torch.Size([4, 32, 32, 4]) \n",
      "tensor([[[[ 28,  26,  52,   3],\n",
      "          [ 42,  22,   8,  65],\n",
      "          [ 17,  92,  38,  47],\n",
      "          ...,\n",
      "          [ 10,  30,  50,  55],\n",
      "          [ 39,  35,  37,  48],\n",
      "          [ 85, 107,  62,  77]],\n",
      "\n",
      "         [[ 59,  38,  85, 101],\n",
      "          [ 10,   4,  73,  98],\n",
      "          [ 76,  12,  54,  34],\n",
      "          ...,\n",
      "          [125,  70,  66, 102],\n",
      "          [ 55,  49,  37,  24],\n",
      "          [ 87,  16,  83,   6]],\n",
      "\n",
      "         [[ 44,  81,  40,  19],\n",
      "          [ 46,   7, 103, 108],\n",
      "          [  9,  26, 105,  75],\n",
      "          ...,\n",
      "          [ 84,  45,  76, 120],\n",
      "          [ 77, 104,  74,  34],\n",
      "          [ 72,  35, 113,  57]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 18,  15,  31, 114],\n",
      "          [ 68, 125, 113,  72],\n",
      "          [ 55,  95,   5,  42],\n",
      "          ...,\n",
      "          [ 20,  36,  77,  20],\n",
      "          [ 29,  28,  46,  86],\n",
      "          [ 35, 105,  32, 121]],\n",
      "\n",
      "         [[ 37, 110, 125, 108],\n",
      "          [ 38, 104,  38,  71],\n",
      "          [126,  51, 123,   5],\n",
      "          ...,\n",
      "          [ 23,  51,  12,  49],\n",
      "          [ 30,  88, 114,  48],\n",
      "          [ 93,  27,  38,  74]],\n",
      "\n",
      "         [[101, 111,  19,  30],\n",
      "          [123,  77,  33,  19],\n",
      "          [ 41, 124,  37,  47],\n",
      "          ...,\n",
      "          [120,  94,  78, 113],\n",
      "          [ 71,   7, 106,  40],\n",
      "          [ 43,  59, 117,  42]]],\n",
      "\n",
      "\n",
      "        [[[ 22, 122,  59,  88],\n",
      "          [ 29,  92,  32,  66],\n",
      "          [ 90,  80, 116,  14],\n",
      "          ...,\n",
      "          [ 65, 124,  20,  61],\n",
      "          [110,  41,  33,  19],\n",
      "          [124,  59,  98,  44]],\n",
      "\n",
      "         [[ 77,  95,  53,  45],\n",
      "          [ 26,  85,  42,  30],\n",
      "          [125,  35,  38,  22],\n",
      "          ...,\n",
      "          [ 68,   9,  45,  31],\n",
      "          [ 10, 126, 126, 107],\n",
      "          [ 54,  34,  16, 126]],\n",
      "\n",
      "         [[  9,  65,  37,  60],\n",
      "          [ 74,  87,  62, 118],\n",
      "          [  5,  29,  74,  24],\n",
      "          ...,\n",
      "          [121, 115,  79,  30],\n",
      "          [101,  47,   7, 116],\n",
      "          [ 91,  26,  27, 121]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 86, 121,   0,  24],\n",
      "          [100,   7, 104,  52],\n",
      "          [ 42,  23,  97,  51],\n",
      "          ...,\n",
      "          [ 84,  87, 102,  74],\n",
      "          [ 89,  73,  79, 118],\n",
      "          [ 36, 110,  11,  76]],\n",
      "\n",
      "         [[114, 123,  89,  80],\n",
      "          [ 16, 101, 102,  78],\n",
      "          [ 96,  53, 107,  73],\n",
      "          ...,\n",
      "          [116,  36,  63,  39],\n",
      "          [ 93,  92,  99,  42],\n",
      "          [102,  29,  85,  84]],\n",
      "\n",
      "         [[ 10,  11, 123,  79],\n",
      "          [ 44,  13,  85,  70],\n",
      "          [ 49, 120, 123,   6],\n",
      "          ...,\n",
      "          [ 42,  45, 117,  57],\n",
      "          [123, 123,  92,  41],\n",
      "          [102,  19,  35,  66]]],\n",
      "\n",
      "\n",
      "        [[[ 74,  10,  30,  93],\n",
      "          [ 34,  90,  24,  28],\n",
      "          [  7,  42, 122,  31],\n",
      "          ...,\n",
      "          [ 73,  83,  69, 110],\n",
      "          [ 93,   6,  50,  26],\n",
      "          [ 23,  37,  87,  90]],\n",
      "\n",
      "         [[ 19,  73,  82,  12],\n",
      "          [ 37,   2,  53,  50],\n",
      "          [ 69,  18,  60,  47],\n",
      "          ...,\n",
      "          [ 98,  67, 118,  78],\n",
      "          [ 96,  42, 115,  10],\n",
      "          [ 53, 115,  20, 126]],\n",
      "\n",
      "         [[ 97,  64, 122,  55],\n",
      "          [ 88,  53,  67,  75],\n",
      "          [ 95,  72,  60,  55],\n",
      "          ...,\n",
      "          [ 82, 112, 110,  55],\n",
      "          [110,  76,  31,  79],\n",
      "          [ 31, 116, 105,   3]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 58,  31,  91, 122],\n",
      "          [  9, 111,  16,  21],\n",
      "          [112,  33, 112,  54],\n",
      "          ...,\n",
      "          [ 35,  54,  29,  75],\n",
      "          [118,  23,  74,  61],\n",
      "          [ 72, 126,  27,  55]],\n",
      "\n",
      "         [[ 90,  17, 114,  46],\n",
      "          [ 34, 107,  60, 101],\n",
      "          [ 21,  81, 108,  30],\n",
      "          ...,\n",
      "          [122,  11,   6,  78],\n",
      "          [ 20,  83,  91,  18],\n",
      "          [ 74,  11,   3, 103]],\n",
      "\n",
      "         [[ 31,  53,  25,  94],\n",
      "          [ 63,  36,  94, 116],\n",
      "          [ 25,  64,  56,  27],\n",
      "          ...,\n",
      "          [ 72,  62,  79,  66],\n",
      "          [ 68,  55, 100, 116],\n",
      "          [ 36,  65,  25,  99]]],\n",
      "\n",
      "\n",
      "        [[[ 53,  79,  62, 120],\n",
      "          [100, 106, 102,  88],\n",
      "          [105,  56,  59,  20],\n",
      "          ...,\n",
      "          [ 86, 114,  24, 102],\n",
      "          [ 87,  19,  49, 117],\n",
      "          [ 84,  10,  74,  84]],\n",
      "\n",
      "         [[ 36,  38, 101,  85],\n",
      "          [104,  33,  24,  11],\n",
      "          [ 49,  33,  43,  31],\n",
      "          ...,\n",
      "          [ 58, 117,  30, 115],\n",
      "          [111,  71,  92, 115],\n",
      "          [ 64,  60,  33,  99]],\n",
      "\n",
      "         [[121, 101,  46,   4],\n",
      "          [103,  54, 116, 125],\n",
      "          [ 16,   9,  23,  23],\n",
      "          ...,\n",
      "          [ 84,  76,  29,  67],\n",
      "          [  5,  78,  81,  83],\n",
      "          [ 90,  28,  27,  24]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 97,  94,  41,  57],\n",
      "          [ 58,   4, 102,  70],\n",
      "          [ 87,  50,  65,  10],\n",
      "          ...,\n",
      "          [  8,  43,  35,  35],\n",
      "          [ 81,  25,  22,  19],\n",
      "          [ 59,  70,  43,  69]],\n",
      "\n",
      "         [[118,  39, 126,   7],\n",
      "          [ 46,  68,  31,  62],\n",
      "          [116,  39,   8,  22],\n",
      "          ...,\n",
      "          [ 91, 114,  43, 101],\n",
      "          [ 61,   6,  77, 107],\n",
      "          [121,  88,   2, 112]],\n",
      "\n",
      "         [[ 40,  96,  79,   4],\n",
      "          [  0,  83,  32,  98],\n",
      "          [ 41, 109,  22,  38],\n",
      "          ...,\n",
      "          [ 28, 101,  38,  10],\n",
      "          [ 63,  42, 114,  41],\n",
      "          [ 44,  70,  62,  41]]]], device='cuda:0', dtype=torch.int8)\n",
      "\n",
      "y - torch.Size([4, 16, 16, 4])\n",
      "tensor([[[[ 59,  38,  85, 101],\n",
      "          [ 90, 105,  83, 102],\n",
      "          [ 45,  66, 119, 125],\n",
      "          ...,\n",
      "          [123, 106,  97, 126],\n",
      "          [125, 114,  66, 102],\n",
      "          [ 87, 107,  83,  77]],\n",
      "\n",
      "         [[ 59,  81, 103, 108],\n",
      "          [114, 115, 105, 124],\n",
      "          [125,  98, 108, 120],\n",
      "          ...,\n",
      "          [118, 116,  73, 104],\n",
      "          [125,  72, 122, 120],\n",
      "          [ 90, 104, 120, 107]],\n",
      "\n",
      "         [[114, 115,  88,  96],\n",
      "          [ 70,  89, 101, 106],\n",
      "          [ 97,  79, 117,  41],\n",
      "          ...,\n",
      "          [126, 114, 120, 110],\n",
      "          [ 78, 124, 105, 116],\n",
      "          [110, 126, 112, 116]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 90, 104, 101,  86],\n",
      "          [117, 103,  62, 104],\n",
      "          [126, 117, 117,  58],\n",
      "          ...,\n",
      "          [ 91, 115,  63, 112],\n",
      "          [102, 118,  65, 108],\n",
      "          [109,  61, 124,  59]],\n",
      "\n",
      "         [[ 88, 125, 113, 114],\n",
      "          [ 75,  95, 123, 120],\n",
      "          [ 93, 120,  77, 121],\n",
      "          ...,\n",
      "          [116, 123,  97, 125],\n",
      "          [123,  36,  77, 101],\n",
      "          [ 37, 118, 124, 121]],\n",
      "\n",
      "         [[123, 111, 125, 108],\n",
      "          [126, 124, 123, 110],\n",
      "          [120, 122, 117,  96],\n",
      "          ...,\n",
      "          [119,  91,  65, 121],\n",
      "          [126,  94,  78, 113],\n",
      "          [ 93,  88, 117,  74]]],\n",
      "\n",
      "\n",
      "        [[[ 77, 122,  59,  88],\n",
      "          [125,  80, 116,  77],\n",
      "          [124, 112, 101, 125],\n",
      "          ...,\n",
      "          [100,  83,  92, 109],\n",
      "          [ 68, 124, 126, 120],\n",
      "          [124, 126, 126, 126]],\n",
      "\n",
      "         [[104,  87,  62, 118],\n",
      "          [ 99,  81, 104,  51],\n",
      "          [ 94, 124,  72,  45],\n",
      "          ...,\n",
      "          [125,  55,  92, 122],\n",
      "          [121, 123, 122, 121],\n",
      "          [122, 104, 116, 121]],\n",
      "\n",
      "         [[104,  96, 105, 111],\n",
      "          [107, 115, 109, 108],\n",
      "          [110, 116,  42,  89],\n",
      "          ...,\n",
      "          [ 97,  91, 112, 117],\n",
      "          [103,  72,  65,  75],\n",
      "          [ 79,  99, 105, 120]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 79,  86, 116, 126],\n",
      "          [ 78,  64, 120,  98],\n",
      "          [ 98,  98, 120, 116],\n",
      "          ...,\n",
      "          [119, 115, 114, 119],\n",
      "          [114, 111,  65,  98],\n",
      "          [123, 103,  69, 114]],\n",
      "\n",
      "         [[100, 121, 119,  90],\n",
      "          [ 87, 122,  99,  51],\n",
      "          [110, 122,  64, 123],\n",
      "          ...,\n",
      "          [114, 118, 104, 120],\n",
      "          [120,  87, 102,  76],\n",
      "          [118, 110,  82, 118]],\n",
      "\n",
      "         [[114, 123, 123,  80],\n",
      "          [ 98, 120, 123,  81],\n",
      "          [118, 112, 106, 124],\n",
      "          ...,\n",
      "          [118, 118, 117, 117],\n",
      "          [116, 106, 117,  57],\n",
      "          [123, 123,  99,  84]]],\n",
      "\n",
      "\n",
      "        [[[ 74,  90,  82,  93],\n",
      "          [ 69,  67, 122, 122],\n",
      "          [105, 113, 125, 100],\n",
      "          ...,\n",
      "          [112, 115,  81,  68],\n",
      "          [ 98,  84, 118, 110],\n",
      "          [ 96, 115, 115, 126]],\n",
      "\n",
      "         [[112,  64, 122,  75],\n",
      "          [126,  92, 114, 111],\n",
      "          [101,  81, 122, 118],\n",
      "          ...,\n",
      "          [ 92, 113,  71, 111],\n",
      "          [124, 112, 110, 108],\n",
      "          [117, 116, 105,  79]],\n",
      "\n",
      "         [[112, 104,  90, 102],\n",
      "          [118, 110, 119, 101],\n",
      "          [117,  93, 108, 107],\n",
      "          ...,\n",
      "          [ 74,  63,  95, 116],\n",
      "          [118,  75, 126,  86],\n",
      "          [101,  91,  70,  79]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[111, 118, 106, 103],\n",
      "          [119,  98,  92, 104],\n",
      "          [118, 115, 125, 101],\n",
      "          ...,\n",
      "          [123, 117, 106, 102],\n",
      "          [ 54,  75, 115, 105],\n",
      "          [ 85, 119, 108, 110]],\n",
      "\n",
      "         [[101, 111, 106, 122],\n",
      "          [112, 120, 112,  97],\n",
      "          [ 89, 123, 104, 126],\n",
      "          ...,\n",
      "          [124, 121, 124,  76],\n",
      "          [126, 106, 117,  75],\n",
      "          [118, 126, 103,  61]],\n",
      "\n",
      "         [[ 90, 107, 114, 116],\n",
      "          [ 62,  81, 108,  48],\n",
      "          [112, 113,  86, 118],\n",
      "          ...,\n",
      "          [ 96, 101,  78,  89],\n",
      "          [122,  76,  79, 115],\n",
      "          [ 74,  83, 100, 116]]],\n",
      "\n",
      "\n",
      "        [[[104, 106, 102, 120],\n",
      "          [105, 104,  81, 111],\n",
      "          [ 95,  67, 102, 112],\n",
      "          ...,\n",
      "          [103, 103, 125, 113],\n",
      "          [ 86, 117, 109, 115],\n",
      "          [111,  71,  92, 117]],\n",
      "\n",
      "         [[121, 101, 116, 125],\n",
      "          [ 75, 123,  96,  40],\n",
      "          [ 93,  71, 126, 102],\n",
      "          ...,\n",
      "          [116,  70, 111,  90],\n",
      "          [101, 104,  89, 124],\n",
      "          [119, 117,  81, 106]],\n",
      "\n",
      "         [[115,  81, 107, 124],\n",
      "          [126, 121, 116,  95],\n",
      "          [121, 117,  74, 101],\n",
      "          ...,\n",
      "          [ 88, 117, 114, 108],\n",
      "          [ 92,  75, 126, 108],\n",
      "          [110, 120, 123, 120]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[123, 112,  52, 103],\n",
      "          [122, 119, 112, 115],\n",
      "          [101, 102, 121,  86],\n",
      "          ...,\n",
      "          [124, 124, 123, 106],\n",
      "          [105, 103, 111,  83],\n",
      "          [125,  91, 114,  81]],\n",
      "\n",
      "         [[124,  94, 110, 107],\n",
      "          [125,  86,  65,  29],\n",
      "          [103,  32, 103,  84],\n",
      "          ...,\n",
      "          [ 82, 121,  57,  94],\n",
      "          [100, 122, 114, 121],\n",
      "          [ 81,  93,  54,  75]],\n",
      "\n",
      "         [[118,  96, 126,  98],\n",
      "          [116, 109, 126,  57],\n",
      "          [110, 112, 103,  87],\n",
      "          ...,\n",
      "          [101,  98,  96,  82],\n",
      "          [ 91, 114,  54, 102],\n",
      "          [121,  88, 114, 112]]]], device='cuda:0', dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x - {x.shape} \\n{x}\\n\")\n",
    "print(f\"y - {y.shape}\\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x - torch.Size([4, 32, 32, 4]) \n",
      "tensor([[[[ 92,  89,  45,  34],\n",
      "          [121, 114,  30,  65],\n",
      "          [125,  80,  28,  29],\n",
      "          ...,\n",
      "          [ 25,  22,  39,  38],\n",
      "          [ 38,  18,  65,  63],\n",
      "          [ 36,  36,  23,  21]],\n",
      "\n",
      "         [[106,  32,  89, 124],\n",
      "          [ 98,  73,  92,  66],\n",
      "          [ 63,  56,  38,  84],\n",
      "          ...,\n",
      "          [ 81,  34,  12,  63],\n",
      "          [ 88, 108,  64,  81],\n",
      "          [ 10,  44,  43, 101]],\n",
      "\n",
      "         [[ 64, 108,  98,  61],\n",
      "          [ 69,  53, 116,  16],\n",
      "          [ 68,  66,  84,  55],\n",
      "          ...,\n",
      "          [ 80, 126,  57,  19],\n",
      "          [ 28,  82,  57, 119],\n",
      "          [ 38,  56, 125,  80]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 94,  50, 104, 103],\n",
      "          [ 73,  39,  99,  65],\n",
      "          [ 85,  65,  51,  89],\n",
      "          ...,\n",
      "          [118,   4,  79,  22],\n",
      "          [  5,  98,  13,  55],\n",
      "          [ 41,  52,  41,  75]],\n",
      "\n",
      "         [[102,  81,  92,  57],\n",
      "          [ 96, 118, 115,  24],\n",
      "          [ 54,  48,  78,   5],\n",
      "          ...,\n",
      "          [113,  15,  36, 108],\n",
      "          [ 57,  30,  61,  21],\n",
      "          [  5,  16, 112,  94]],\n",
      "\n",
      "         [[ 74,  68,  28,   1],\n",
      "          [ 53,  24,  28,  95],\n",
      "          [ 63,  46, 111, 114],\n",
      "          ...,\n",
      "          [ 22,  25,  15, 109],\n",
      "          [ 58, 102,   0,  16],\n",
      "          [ 18,  47,  61,  81]]],\n",
      "\n",
      "\n",
      "        [[[ 92,  60, 112,  86],\n",
      "          [ 89,  68,  55,  12],\n",
      "          [ 83,  18,  70,  33],\n",
      "          ...,\n",
      "          [ 76,  47,  16,  52],\n",
      "          [ 75,   4,  97, 102],\n",
      "          [ 53, 118, 108,  57]],\n",
      "\n",
      "         [[ 47,   4,  81,  51],\n",
      "          [ 35, 116,  25,  34],\n",
      "          [125, 106,   1,  67],\n",
      "          ...,\n",
      "          [ 50, 123,  98,  59],\n",
      "          [118,  56,  37,  66],\n",
      "          [ 51,  45, 120,  65]],\n",
      "\n",
      "         [[ 47,  26,  36,  15],\n",
      "          [112, 116,   9,  33],\n",
      "          [ 83,  78,   6,  62],\n",
      "          ...,\n",
      "          [ 65,  63,  27, 103],\n",
      "          [ 21,  66, 111,  14],\n",
      "          [ 58,  71,  30,  31]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 94,  99,  40,  98],\n",
      "          [122,  21, 119,  26],\n",
      "          [  2,  36,  95,  49],\n",
      "          ...,\n",
      "          [110,   2, 124,  87],\n",
      "          [101, 110,   4,  85],\n",
      "          [ 32,  57,  92,  40]],\n",
      "\n",
      "         [[ 17,  55,  47,  79],\n",
      "          [ 62, 107,  74,  40],\n",
      "          [ 28,   9,  63, 106],\n",
      "          ...,\n",
      "          [ 16,  95,  57,  26],\n",
      "          [ 47,  70,   4,   4],\n",
      "          [ 16,  20,  88,  58]],\n",
      "\n",
      "         [[ 94,  93, 117,  88],\n",
      "          [ 31, 123,  31,  66],\n",
      "          [ 92,  66,   4,  26],\n",
      "          ...,\n",
      "          [ 83,  58, 125,  88],\n",
      "          [ 56,  50,  43,  44],\n",
      "          [111,  28,  57, 106]]],\n",
      "\n",
      "\n",
      "        [[[ 64,  18,  22,  65],\n",
      "          [ 60,  86,  69,  81],\n",
      "          [ 25,  82,  98,  65],\n",
      "          ...,\n",
      "          [  2, 105,  74, 115],\n",
      "          [  4,  46,  61, 114],\n",
      "          [ 12,  67, 105,  40]],\n",
      "\n",
      "         [[116,  92,  83, 123],\n",
      "          [ 59,  38,  18,  23],\n",
      "          [ 80, 109,  10,  29],\n",
      "          ...,\n",
      "          [ 60,  46,  25,  90],\n",
      "          [122,  78,  35,  80],\n",
      "          [ 31,  33,  70, 116]],\n",
      "\n",
      "         [[ 23,  62,  48, 117],\n",
      "          [ 24,  93,   3,  37],\n",
      "          [ 85,  52,  65,  77],\n",
      "          ...,\n",
      "          [ 34,   3,  82, 106],\n",
      "          [ 65,   4,  64,  27],\n",
      "          [ 43, 102, 115,  25]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 70,  67,  95, 105],\n",
      "          [ 78,  51,  69,  66],\n",
      "          [ 38,  52,  70,  88],\n",
      "          ...,\n",
      "          [ 18,  90,  87,  55],\n",
      "          [ 13,  20, 116,   3],\n",
      "          [ 50,  49,   2,  60]],\n",
      "\n",
      "         [[126,  73,  95,   9],\n",
      "          [  2, 124, 106, 103],\n",
      "          [ 23, 113,  91,  40],\n",
      "          ...,\n",
      "          [  5, 125,  57,  25],\n",
      "          [ 73,  24,  75,  69],\n",
      "          [ 57,  28,   7,   5]],\n",
      "\n",
      "         [[  5,  11,  20,  69],\n",
      "          [ 27,  96,  63,  31],\n",
      "          [  9,   7,  36,  55],\n",
      "          ...,\n",
      "          [ 97,  97,  63,  94],\n",
      "          [ 88,  78, 101,  47],\n",
      "          [ 20,  60,  95,   0]]],\n",
      "\n",
      "\n",
      "        [[[124,  21,  96,  24],\n",
      "          [ 37,  32,  99, 109],\n",
      "          [ 80,  31,  69,   5],\n",
      "          ...,\n",
      "          [126, 122,  56, 102],\n",
      "          [ 12,  49, 115, 125],\n",
      "          [ 65,  69,  13,  86]],\n",
      "\n",
      "         [[114,  61,  36,  54],\n",
      "          [ 65,   2, 116,  47],\n",
      "          [ 63, 101, 111,  79],\n",
      "          ...,\n",
      "          [ 76,  52,  25,   7],\n",
      "          [114,   4,  10, 125],\n",
      "          [  8,  17,  70,  70]],\n",
      "\n",
      "         [[ 38,  78,  93,  26],\n",
      "          [ 18,  54, 113, 113],\n",
      "          [  9,  62,  81,  24],\n",
      "          ...,\n",
      "          [ 27,  94,  83,  54],\n",
      "          [117,  79,  70, 111],\n",
      "          [ 41,  66,  32,  94]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  6,  82,  76,  61],\n",
      "          [ 60,  76,   8,   7],\n",
      "          [ 13,  71,   2,  64],\n",
      "          ...,\n",
      "          [110,  68,  76,  57],\n",
      "          [ 47,  27,  25,  81],\n",
      "          [ 22,  15, 104,  82]],\n",
      "\n",
      "         [[  1,  75,  99,  54],\n",
      "          [125,   1,  50,  89],\n",
      "          [ 33, 113,  73,  52],\n",
      "          ...,\n",
      "          [ 16,  10,  72,  79],\n",
      "          [ 88,  58,  76,  77],\n",
      "          [101,  58,  65,  46]],\n",
      "\n",
      "         [[ 36,  21,  34,  53],\n",
      "          [115,   5, 106, 111],\n",
      "          [ 81,  18, 125,  87],\n",
      "          ...,\n",
      "          [ 74, 104,  31,  54],\n",
      "          [ 62,  16, 121, 122],\n",
      "          [ 36, 105,  62,  86]]]], device='cuda:0', dtype=torch.int8)\n",
      "\n",
      "y - torch.Size([4, 16, 16, 4])\n",
      "tensor([[[[121, 114,  92, 124],\n",
      "          [125,  80,  96,  84],\n",
      "          [101,  82,  99,  99],\n",
      "          ...,\n",
      "          [121, 117,  79, 118],\n",
      "          [ 88,  54,  66,  63],\n",
      "          [ 88, 108,  65, 101]],\n",
      "\n",
      "         [[ 93, 122, 116,  69],\n",
      "          [ 68, 121,  86,  56],\n",
      "          [ 78,  95,  88, 122],\n",
      "          ...,\n",
      "          [102,  64,  69, 111],\n",
      "          [ 98, 126,  90, 103],\n",
      "          [ 70,  82, 125, 119]],\n",
      "\n",
      "         [[ 78,  51,  34,  64],\n",
      "          [125, 102, 119,  66],\n",
      "          [ 79, 112, 105, 121],\n",
      "          ...,\n",
      "          [122, 123, 119, 124],\n",
      "          [101,  95,  86, 115],\n",
      "          [113,  61, 126,  83]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 88, 122, 123,  94],\n",
      "          [ 99, 107, 119,  68],\n",
      "          [113,  85, 115,  88],\n",
      "          ...,\n",
      "          [ 76, 115, 103, 123],\n",
      "          [126, 114, 118, 124],\n",
      "          [122,  82, 117,  88]],\n",
      "\n",
      "         [[ 94,  79, 112, 103],\n",
      "          [ 85, 122,  51,  89],\n",
      "          [118,  84,  53, 124],\n",
      "          ...,\n",
      "          [ 75,  97, 118, 105],\n",
      "          [118, 101,  92,  88],\n",
      "          [104,  98, 100, 121]],\n",
      "\n",
      "         [[102, 118, 115,  95],\n",
      "          [119,  58, 111, 114],\n",
      "          [116, 109,  57, 112],\n",
      "          ...,\n",
      "          [118,  70, 125, 126],\n",
      "          [113,  37,  87, 109],\n",
      "          [ 58, 102, 112,  94]]],\n",
      "\n",
      "\n",
      "        [[[ 92, 116, 112,  86],\n",
      "          [125, 106,  70, 100],\n",
      "          [119,  76, 117,  55],\n",
      "          ...,\n",
      "          [ 64, 103,  70,  77],\n",
      "          [ 76, 123,  98, 106],\n",
      "          [118, 118, 120, 102]],\n",
      "\n",
      "         [[112, 120, 112,  89],\n",
      "          [ 93,  85,  81,  92],\n",
      "          [ 25,  74, 113, 118],\n",
      "          ...,\n",
      "          [111, 120, 115, 100],\n",
      "          [111,  97,  35, 108],\n",
      "          [125, 120, 115,  68]],\n",
      "\n",
      "         [[ 59, 104,  95, 121],\n",
      "          [ 74, 101, 107,  94],\n",
      "          [101, 123, 101,  55],\n",
      "          ...,\n",
      "          [ 89, 121, 107,  65],\n",
      "          [104, 102, 124, 105],\n",
      "          [119, 112, 102,  75]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 87,  94,  80, 113],\n",
      "          [ 86,  95,  58, 113],\n",
      "          [118, 125,  77, 122],\n",
      "          ...,\n",
      "          [ 61, 106, 124, 108],\n",
      "          [ 71, 117, 120,  78],\n",
      "          [ 78,  96, 101, 122]],\n",
      "\n",
      "         [[122, 103, 119,  98],\n",
      "          [113, 124, 119, 122],\n",
      "          [115, 107, 105,  99],\n",
      "          ...,\n",
      "          [114, 121,  80, 122],\n",
      "          [123, 125, 124, 104],\n",
      "          [124, 110,  92, 126]],\n",
      "\n",
      "         [[ 94, 123, 117,  88],\n",
      "          [ 92, 111,  63, 106],\n",
      "          [108,  71, 115,  70],\n",
      "          ...,\n",
      "          [121, 119,  65, 101],\n",
      "          [ 83,  95, 125, 123],\n",
      "          [111,  70,  88, 106]]],\n",
      "\n",
      "\n",
      "        [[[116,  92,  83, 123],\n",
      "          [122, 109,  98, 110],\n",
      "          [118, 112, 113, 110],\n",
      "          ...,\n",
      "          [111, 105,  81,  76],\n",
      "          [ 77, 105,  85, 115],\n",
      "          [122,  78, 105, 116]],\n",
      "\n",
      "         [[109,  93,  99, 117],\n",
      "          [ 97, 122, 121, 123],\n",
      "          [120, 109,  83,  91],\n",
      "          ...,\n",
      "          [ 55,  92, 114, 125],\n",
      "          [ 58, 112,  82, 106],\n",
      "          [ 79, 102, 115,  27]],\n",
      "\n",
      "         [[113, 120, 103, 105],\n",
      "          [111, 121, 111,  86],\n",
      "          [ 96,  88, 103, 111],\n",
      "          ...,\n",
      "          [110, 104,  94, 109],\n",
      "          [117,  92, 120, 124],\n",
      "          [ 70,  54,  95,  46]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[124, 112, 120,  90],\n",
      "          [106,  65, 124,  83],\n",
      "          [ 89, 114,  72,  77],\n",
      "          ...,\n",
      "          [119, 113, 117, 104],\n",
      "          [106, 108,  81, 126],\n",
      "          [122, 117, 120,  66]],\n",
      "\n",
      "         [[ 78,  67,  95, 112],\n",
      "          [ 48,  57,  95, 122],\n",
      "          [114, 112,  95, 126],\n",
      "          ...,\n",
      "          [ 77, 110,  66,  88],\n",
      "          [112,  90, 118, 124],\n",
      "          [ 71,  85, 116,  60]],\n",
      "\n",
      "         [[126, 124, 106, 103],\n",
      "          [ 76, 113, 100,  74],\n",
      "          [117, 107, 108, 101],\n",
      "          ...,\n",
      "          [119, 109, 124,  92],\n",
      "          [ 97, 125,  92, 101],\n",
      "          [ 88,  78, 101,  69]]],\n",
      "\n",
      "\n",
      "        [[[124,  61, 116, 109],\n",
      "          [108, 121, 111,  95],\n",
      "          [119, 116,  93, 122],\n",
      "          ...,\n",
      "          [ 95, 108,  96,  63],\n",
      "          [126, 122, 111, 103],\n",
      "          [114,  69, 115, 125]],\n",
      "\n",
      "         [[ 58,  78, 123, 113],\n",
      "          [126, 117, 111,  60],\n",
      "          [105, 126, 101, 100],\n",
      "          ...,\n",
      "          [125,  80,  91,  91],\n",
      "          [115,  94, 124, 111],\n",
      "          [117,  94, 124, 111]],\n",
      "\n",
      "         [[105, 122,  95, 117],\n",
      "          [ 93, 125, 104, 112],\n",
      "          [105, 107, 120,  83],\n",
      "          ...,\n",
      "          [121, 113, 126, 107],\n",
      "          [121, 109, 124,  46],\n",
      "          [126,  72, 124, 116]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[126, 112, 111, 104],\n",
      "          [104,  91,  69,  91],\n",
      "          [ 82,  53,  73, 100],\n",
      "          ...,\n",
      "          [ 98, 107, 120, 126],\n",
      "          [109, 111, 101,  91],\n",
      "          [ 74,  73, 104, 108]],\n",
      "\n",
      "         [[ 60,  87,  76,  69],\n",
      "          [ 58,  96,  74, 121],\n",
      "          [108, 115, 123,  71],\n",
      "          ...,\n",
      "          [109, 103,  99,  79],\n",
      "          [110, 111,  86, 119],\n",
      "          [118, 114, 104,  82]],\n",
      "\n",
      "         [[125,  75, 106, 111],\n",
      "          [ 97, 113, 125, 105],\n",
      "          [ 72,  72,  85,  58],\n",
      "          ...,\n",
      "          [125, 119,  65, 123],\n",
      "          [116, 122, 103,  90],\n",
      "          [101, 105, 121, 122]]]], device='cuda:0', dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "avg_pool = IntPool(mode=1)\n",
    "x = torch.randint(0, 127,(4,32,32,4), dtype=torch.int8).cuda()\n",
    "with torch.no_grad():\n",
    "    y = pool(x)\n",
    "print(f\"x - {x.shape} \\n{x}\\n\")\n",
    "print(f\"y - {y.shape}\\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch conv layer parmeter shape 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 3])\n",
      "torch.Size([12, 4, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.vgg.vgg16(pretrained=True)\n",
    "print(model.features[0].weight.shape)\n",
    "c = torch.nn.Conv2d(4,12,3,1,1)\n",
    "print(c.weight.shape) # NCHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "import cutlassconv\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "class IntConv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride =1, padding =1):\n",
    "        super(IntConv2d,self).__init__()\n",
    "        self.weight = torch.randint(-127,127,(out_channels, kernel_size, kernel_size, in_channels), dtype=torch.int8)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self,x):\n",
    "        # trans_weight = torch.flip(self.weight,[1,2]).transpose(0,3).contiguous()\n",
    "        # trans_weight = self.weight.permute(0,2,3,1).contiguous()\n",
    "        trans_weight = self.weight\n",
    "        return cutlassconv.int8_conv(x,trans_weight)\n",
    "    \n",
    "    def cuda(self):\n",
    "        self.weight = self.weight.cuda()\n",
    "        \n",
    "## cutlass는 16의 배수만\n",
    "input_channel= 16\n",
    "conv = IntConv2d(input_channel,32,3,1,1)\n",
    "print(conv.weight.shape)\n",
    "x = torch.randint(0,127,(1,32,32,input_channel), dtype=torch.int8).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.cuda()\n",
    "    y = conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x data - torch.Size([1, 32, 32, 16]) \n",
      "[[1 2 3 4]]\n",
      "\n",
      "conv data - torch.Size([32, 3, 3, 16])\n",
      "tensor([[[[  51,  101, -104,  ...,  124,  -45,  -31],\n",
      "          [  62,   96,  -80,  ...,   45,  -15,  -37],\n",
      "          [ -33,   25, -100,  ...,   65,  -25,   30]],\n",
      "\n",
      "         [[  -2,  -97,   86,  ...,  -65,   76,  -56],\n",
      "          [ -87,  -75, -112,  ...,  -44,   58,   52],\n",
      "          [ -28,   84,   15,  ...,   51,   -5,  114]],\n",
      "\n",
      "         [[-100,   73,   56,  ..., -100,  -26,  -71],\n",
      "          [-108,   22,   86,  ...,  -89,  -24,   49],\n",
      "          [-118,  -57,   12,  ...,  120,  -61,  -20]]],\n",
      "\n",
      "\n",
      "        [[[ -15,   24,  -81,  ...,  -69,  113,  -57],\n",
      "          [ -69, -108,   20,  ...,   84,   38,   -9],\n",
      "          [ -31,  -43,  -83,  ...,  -48,  104,  -97]],\n",
      "\n",
      "         [[  54,    5,   77,  ..., -123,   59,   94],\n",
      "          [ -26,  117,  -41,  ...,    2,  -82,  -68],\n",
      "          [ -70,   46,   34,  ...,  -54,   70,   27]],\n",
      "\n",
      "         [[ 107,  -91,  117,  ...,   38,   72,   56],\n",
      "          [  54,   55,  118,  ...,  -91,   65,   89],\n",
      "          [  13,   76,   12,  ...,  -19,  -61,   91]]],\n",
      "\n",
      "\n",
      "        [[[   8, -100,  -98,  ...,   75,   94,   18],\n",
      "          [ 105,  -21,  -47,  ...,   62,   72,  -50],\n",
      "          [  87,  -50,  -21,  ...,   73,  -18,  -33]],\n",
      "\n",
      "         [[  38,  -90, -110,  ...,  -16,  -79, -123],\n",
      "          [ -98,  -29,  105,  ...,  -98, -112,    9],\n",
      "          [ -66,  -77,  -90,  ...,  -34,  -66,  -98]],\n",
      "\n",
      "         [[  19,   62,   43,  ...,   50,   93,   -4],\n",
      "          [  99,  -60,   16,  ...,  -40,   35, -116],\n",
      "          [ -58, -119, -101,  ..., -106,  -63,  -34]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -37,  -72,  -79,  ...,   -1,  -37,   90],\n",
      "          [ -16,   64, -105,  ...,  116,  120,  -47],\n",
      "          [ -92, -115,  117,  ...,  -56,   36,  -95]],\n",
      "\n",
      "         [[ 110,   63,   34,  ...,   75,  103,  -67],\n",
      "          [ -78, -107,  -26,  ...,  -34,   91,   11],\n",
      "          [ -71,  122,   28,  ..., -103,  -30, -104]],\n",
      "\n",
      "         [[ 105,  -25,   41,  ...,  -53,  111,  -19],\n",
      "          [  85,   37,  -46,  ...,  -26,  -63,   76],\n",
      "          [ 105, -124,   71,  ..., -124,  115,  116]]],\n",
      "\n",
      "\n",
      "        [[[  26,  -86,  -56,  ...,  100,  -47, -111],\n",
      "          [  76, -110, -107,  ...,   51,  -31,   15],\n",
      "          [  95,  104,  -82,  ...,   40,  -61,   68]],\n",
      "\n",
      "         [[  53,   19,   96,  ...,   16, -110,  -30],\n",
      "          [ -23,   27,  -33,  ...,  -50,  -15,  107],\n",
      "          [  -6,  -44,  -35,  ..., -123,   22,   37]],\n",
      "\n",
      "         [[  31,   78,    8,  ...,   11,  -98,   50],\n",
      "          [  74,  -16,   82,  ...,  -99,   68,    3],\n",
      "          [ -39,  -85,   89,  ...,  -57, -123,   63]]],\n",
      "\n",
      "\n",
      "        [[[  96, -107,   72,  ..., -106,  107,  -85],\n",
      "          [ 112,   39,  112,  ...,  -76,  -73,   77],\n",
      "          [ 120,   17,   92,  ...,  -34,  -12,   84]],\n",
      "\n",
      "         [[  17,  -32,  -71,  ...,  -76,  -96,  116],\n",
      "          [  46,  -41, -106,  ...,  -87,   16,  -34],\n",
      "          [  54,   97,   -1,  ...,  -49,   53,  -38]],\n",
      "\n",
      "         [[  93,  -57,   98,  ...,  -55,  -42,   28],\n",
      "          [  75,  -82,    5,  ...,  -23,  -93, -127],\n",
      "          [ -68,  111,    0,  ...,   97,  -82,  -44]]]], device='cuda:0',\n",
      "       dtype=torch.int8)\n",
      "\n",
      "y data - torch.Size([1, 32, 32, 32])\n",
      "tensor([[[[ -80160,  -28340, -102875,  ...,   37086,    4821,   19240],\n",
      "          [ -52539,  -17432, -132565,  ...,   58605,    8979,    4973],\n",
      "          [ -47900,  -45330, -113851,  ...,   62585,   26883,  -55776],\n",
      "          ...,\n",
      "          [-104051,  -54897, -118230,  ...,   86915,   -5720,  -74460],\n",
      "          [ -86611,   -4755, -133285,  ...,   47848,   28100,    9818],\n",
      "          [-120352,  -17143,  -52911,  ...,   41732,   17308,  -40603]],\n",
      "\n",
      "         [[ -38111,  -62786, -175469,  ...,   76365,   24187,   -6846],\n",
      "          [ -82748, -102263,  -95090,  ...,   92530,   -6349,   61932],\n",
      "          [-100651,  -89740, -131376,  ...,   92389,  -68438,  106616],\n",
      "          ...,\n",
      "          [ -18583,  -69479, -121828,  ...,  105703,   13201,   -2423],\n",
      "          [ -47386, -117359, -151184,  ...,   87347,  -21334,   47817],\n",
      "          [-140169,  -83360, -126966,  ...,   86585,  -65071,  -61288]],\n",
      "\n",
      "         [[ -40573,  -81454, -147208,  ...,   33843,    7657,    5088],\n",
      "          [-122135,  -77897, -122074,  ...,  123458,  -40753,   -1340],\n",
      "          [ -47697, -105401, -165669,  ...,   50359,   29527,   -2483],\n",
      "          ...,\n",
      "          [ -69289,  -89185, -150063,  ...,   21640,   13070,   54489],\n",
      "          [ -79382, -100082, -109875,  ...,   87392,   -3502,   36681],\n",
      "          [ -53854,  -36220, -116742,  ...,  121106,  -31628,  -88878]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -68249,  -59691, -100705,  ...,   68766,   24926,   40975],\n",
      "          [ -52808,  -87982, -114546,  ...,  108827,  -50237,   14682],\n",
      "          [  14333,  -83002, -119173,  ...,   23111,   -4249,   11700],\n",
      "          ...,\n",
      "          [ -18972, -112165, -143062,  ...,   51668,  -59350,     544],\n",
      "          [ -39571, -105027, -158251,  ...,  149073,   21369,  -19192],\n",
      "          [-114635,  -13479,  -86571,  ...,   81699,  -22378,  -20213]],\n",
      "\n",
      "         [[ -49022,  -60998, -126679,  ...,   68921,   37043,   -5465],\n",
      "          [ -88468,  -81452,  -94736,  ...,   44046,   -4689,   19322],\n",
      "          [ -72140, -127248, -108736,  ...,  139900,   35081,  -45681],\n",
      "          ...,\n",
      "          [   3001, -146685, -158279,  ...,   24131,  -15737,   62203],\n",
      "          [ -71010,  -95435, -164192,  ...,   44454,   19396,   50952],\n",
      "          [-100849, -100734,  -53662,  ...,   58507,  -18198,  -76065]],\n",
      "\n",
      "         [[ -13296,  -50425,  -74189,  ...,     335,   16412,   59615],\n",
      "          [ -45600, -100810,  -56699,  ...,   53274,  -21845,   82646],\n",
      "          [   1720,  -53120, -117397,  ...,   19322,  -29512,   45839],\n",
      "          ...,\n",
      "          [  14661,  -81962, -101472,  ...,   34772,  -23150,   72495],\n",
      "          [ -42089,  -85504, -114461,  ...,  -11251,   -7333,   74144],\n",
      "          [ -72665,  -74075,  -91804,  ...,   68176,   -9303,  -35055]]]],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "print(f\"x data - {x.shape} \\n{x_data}\\n\")\n",
    "print(f\"conv data - {conv.weight.shape}\\n{conv.weight}\\n\")\n",
    "print(f\"y data - {y.shape}\\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import int8conv_cuda\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "class IntConv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride =1, padding =1):\n",
    "        super(IntConv2d,self).__init__()\n",
    "        self.weight = torch.randint(0,127,(out_channels, kernel_size, kernel_size, in_channels), dtype=torch.int8)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self,x):\n",
    "        # trans_weight = torch.flip(self.weight,[1,2]).transpose(0,3).contiguous()\n",
    "        # trans_weight = self.weight.permute(0,2,3,1).contiguous()\n",
    "        trans_weight = self.weight\n",
    "        y = int8conv_cuda.int8_conv(x,trans_weight,self.stride, self.padding,1)\n",
    "        y = (y > 127).int()*5\n",
    "        y = y.type(torch.int8)\n",
    "        return y\n",
    "    \n",
    "    def cuda(self):\n",
    "        self.weight = self.weight.cuda()\n",
    "    \n",
    "# cudnn은 4의 배수만\n",
    "input_channel= 4\n",
    "conv = IntConv2d(input_channel,32,3,1,1)\n",
    "print(conv.weight.shape)\n",
    "x = torch.randint(0,127,(1,32,32,input_channel), dtype=torch.int8).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32, 32]) cuda:0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    conv.cuda()\n",
    "    y = conv(x)\n",
    "print(y.shape, y.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x data - torch.Size([1, 32, 32, 4]) \n",
      "[[[[ 63  29  83  92]\n",
      "   [116  36  26 123]\n",
      "   [ 39 123  91  62]\n",
      "   ...\n",
      "   [122  24 108  71]\n",
      "   [100   1 108  82]\n",
      "   [ 18 122  58  86]]\n",
      "\n",
      "  [[ 38 113  34 118]\n",
      "   [ 55 119  83  56]\n",
      "   [115  93  99 124]\n",
      "   ...\n",
      "   [ 28  40  48  57]\n",
      "   [ 14 108  44   1]\n",
      "   [ 74  27  39  52]]\n",
      "\n",
      "  [[ 20  11  73  80]\n",
      "   [ 64 112  86 110]\n",
      "   [ 66  16   9  15]\n",
      "   ...\n",
      "   [115  18 116  19]\n",
      "   [  0  16  44  70]\n",
      "   [104  47  43   7]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[102  85  32 104]\n",
      "   [108  46  45  13]\n",
      "   [123  88  42 109]\n",
      "   ...\n",
      "   [ 54 100  85   3]\n",
      "   [117  53  33  72]\n",
      "   [113  62  18  87]]\n",
      "\n",
      "  [[117  94  58 101]\n",
      "   [ 67 125  56   4]\n",
      "   [117   4 106  12]\n",
      "   ...\n",
      "   [ 24  31  34 113]\n",
      "   [  7  17 103  86]\n",
      "   [ 80  18  44  88]]\n",
      "\n",
      "  [[117 108  74   0]\n",
      "   [  3  68  54  30]\n",
      "   [ 35 120  42  73]\n",
      "   ...\n",
      "   [  5  72  91  39]\n",
      "   [ 74  31  93  92]\n",
      "   [100  41  88  11]]]]\n",
      "\n",
      "conv data - (32, 3, 3, 4)\n",
      "[[[[ 42  73  41 123]\n",
      "   [ 37 125  20  92]\n",
      "   [125  34  62  38]]\n",
      "\n",
      "  [[ 25  96 113  61]\n",
      "   [  0  23  77  37]\n",
      "   [ 16  24   4 106]]\n",
      "\n",
      "  [[114  11 110  92]\n",
      "   [ 13  36  96  39]\n",
      "   [100   3  32  32]]]\n",
      "\n",
      "\n",
      " [[[112  14  87  44]\n",
      "   [ 83  27  50  68]\n",
      "   [ 68  28  48  41]]\n",
      "\n",
      "  [[ 23  75  55  40]\n",
      "   [ 27  23  27 117]\n",
      "   [ 42  81  29  95]]\n",
      "\n",
      "  [[ 72 121   2 114]\n",
      "   [ 35  18  85 101]\n",
      "   [ 99  81  55  26]]]\n",
      "\n",
      "\n",
      " [[[109   0 102  28]\n",
      "   [  8  27  12 123]\n",
      "   [ 55 126  55   7]]\n",
      "\n",
      "  [[ 64  30  67  31]\n",
      "   [ 11  25  15 106]\n",
      "   [ 70  52  16  79]]\n",
      "\n",
      "  [[108  73 118  95]\n",
      "   [  9  88  46 122]\n",
      "   [115  49  86  26]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[  7  93  34  98]\n",
      "   [ 15  57  13  68]\n",
      "   [ 17  75 104  89]]\n",
      "\n",
      "  [[ 58  77  27 122]\n",
      "   [ 66  86 113 102]\n",
      "   [100  17  35  16]]\n",
      "\n",
      "  [[ 69  88 124  44]\n",
      "   [ 96 117   3  87]\n",
      "   [ 53 118  59  98]]]\n",
      "\n",
      "\n",
      " [[[  6  61  33  23]\n",
      "   [ 62  39  83  55]\n",
      "   [ 99   1   3 108]]\n",
      "\n",
      "  [[ 46  33  25  40]\n",
      "   [ 54  16  52  62]\n",
      "   [ 15   9  60  88]]\n",
      "\n",
      "  [[109 111  64  76]\n",
      "   [ 24  30  21  57]\n",
      "   [123  18  23  15]]]\n",
      "\n",
      "\n",
      " [[[ 18  34   5  52]\n",
      "   [ 77  46  82  88]\n",
      "   [111  96  86 100]]\n",
      "\n",
      "  [[ 49  50  92  42]\n",
      "   [ 70  44 105  62]\n",
      "   [116  92  91 126]]\n",
      "\n",
      "  [[115  94  41  76]\n",
      "   [  3  11 116  60]\n",
      "   [ 96  46  52 120]]]]\n",
      "\n",
      "y data - torch.Size([1, 32, 32, 32])\n",
      "tensor([[[[5, 5, 0,  ..., 5, 0, 5],\n",
      "          [5, 0, 5,  ..., 5, 0, 5],\n",
      "          [5, 0, 5,  ..., 0, 0, 5],\n",
      "          ...,\n",
      "          [0, 0, 5,  ..., 0, 5, 5],\n",
      "          [5, 5, 5,  ..., 5, 5, 5],\n",
      "          [5, 0, 5,  ..., 5, 0, 0]],\n",
      "\n",
      "         [[5, 5, 0,  ..., 5, 0, 0],\n",
      "          [5, 0, 5,  ..., 5, 5, 5],\n",
      "          [0, 0, 5,  ..., 5, 0, 5],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 5, 0, 0],\n",
      "          [0, 5, 5,  ..., 0, 0, 0],\n",
      "          [0, 5, 5,  ..., 0, 5, 0]],\n",
      "\n",
      "         [[0, 0, 5,  ..., 0, 0, 0],\n",
      "          [5, 5, 5,  ..., 5, 0, 0],\n",
      "          [0, 0, 5,  ..., 5, 5, 5],\n",
      "          ...,\n",
      "          [5, 5, 5,  ..., 5, 5, 5],\n",
      "          [0, 0, 0,  ..., 0, 5, 0],\n",
      "          [0, 0, 0,  ..., 0, 5, 0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0, 5, 0,  ..., 0, 0, 0],\n",
      "          [0, 5, 5,  ..., 0, 5, 0],\n",
      "          [5, 0, 0,  ..., 0, 5, 5],\n",
      "          ...,\n",
      "          [5, 5, 0,  ..., 5, 0, 0],\n",
      "          [5, 0, 5,  ..., 0, 0, 0],\n",
      "          [5, 5, 0,  ..., 5, 0, 5]],\n",
      "\n",
      "         [[5, 0, 0,  ..., 5, 5, 0],\n",
      "          [5, 0, 0,  ..., 0, 0, 5],\n",
      "          [5, 5, 5,  ..., 5, 0, 0],\n",
      "          ...,\n",
      "          [5, 0, 5,  ..., 5, 5, 5],\n",
      "          [0, 0, 0,  ..., 5, 0, 0],\n",
      "          [0, 5, 0,  ..., 0, 5, 0]],\n",
      "\n",
      "         [[5, 5, 5,  ..., 5, 0, 0],\n",
      "          [5, 5, 5,  ..., 5, 5, 0],\n",
      "          [5, 0, 0,  ..., 5, 5, 5],\n",
      "          ...,\n",
      "          [0, 5, 0,  ..., 0, 0, 0],\n",
      "          [5, 5, 0,  ..., 5, 0, 0],\n",
      "          [5, 5, 0,  ..., 5, 5, 5]]]], device='cuda:0', dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "conv_data = conv.weight.detach().cpu().numpy()\n",
    "x_data = x.detach().cpu().numpy()\n",
    "y_data = y.detach().cpu().numpy()\n",
    "print(f\"x data - {x.shape} \\n{x_data}\\n\")\n",
    "print(f\"conv data - {conv_data.shape}\\n{conv_data}\\n\")\n",
    "print(f\"y data - {y.shape}\\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(\n",
    "        self, features: nn.Module, num_classes: int = 100, dropout: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        # self.avgpool = IntPool(7,1,0,1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            IntLinear(25088,4096),\n",
    "            # RELU가 아닌 32bit을 8bit으로 변경하는 activation을 사용해야한다.\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            IntLinear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            IntLinear(4096,num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        print(f\"after feature {x.shape}\")\n",
    "        # x = self.avgpool(x)\n",
    "        # print(f\"after pooling {x.shape}\")\n",
    "        x = torch.flatten(x, 1)\n",
    "        print(f\"after flatten {x.shape} {x.dtype}\")\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def cuda(self):\n",
    "        for layer in model.modules():\n",
    "            if 'Int' in str(type(layer)):\n",
    "                layer.cuda()\n",
    "\n",
    "def make_layers(cfg, batch_norm: bool = False) -> nn.Sequential:\n",
    "    layers = []\n",
    "    in_channels = 4\n",
    "    for vs in cfg:\n",
    "        for v in vs:\n",
    "            v = int(v)\n",
    "            conv2d = IntConv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            layers += [conv2d, nn.ReLU()]\n",
    "            in_channels = v\n",
    "        layers += [IntPool(kernel_size=2, stride=2)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfgs = {\n",
    "    \"D\": [[64, 64], [128, 128], [256, 256, 256], [512, 512, 512],[512, 512, 512]],\n",
    "    # \"D\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n",
    "}\n",
    "\n",
    "def int_vgg(cfg: str, **kwargs) -> VGG:\n",
    "    model = VGG(make_layers(cfgs[cfg]), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = []\n",
    "before_l = []\n",
    "after_l = []\n",
    "hooks = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    modules.append(module)\n",
    "    before_l.append(input[0])\n",
    "    after_l.append(output)\n",
    "\n",
    "def add_forward_hook(net, hooks):\n",
    "    for name, layer in net._modules.items():\n",
    "        if isinstance(layer, nn.Sequential) or isinstance(layer, torchvision.models.vgg.VGG):\n",
    "            add_forward_hook(layer, hooks)\n",
    "        else:\n",
    "            hook = layer.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "            \n",
    "    return hooks\n",
    "\n",
    "def remove_forward_hook(hooks):\n",
    "    for i in hooks:\n",
    "        i.remove()\n",
    "# out = model((torch.randn(1,3,32,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class test_module(Module):\n",
    "#     def __init__(self,num_classes= 100):\n",
    "#         super(test_module,self).__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             IntLinear(512,4096),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             IntLinear(4096,4096),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             IntLinear(4096,num_classes),\n",
    "#         )\n",
    "#     def forward(self,x):\n",
    "#         for l in self.layers:\n",
    "#             x = l(x)\n",
    "#             print(x.shape, x.dtype)\n",
    "#         return x\n",
    "#     def cuda(self):\n",
    "#         for layer in model.modules():\n",
    "#             if 'Int' in str(type(layer)):\n",
    "#                 layer.cuda()\n",
    "\n",
    "# model = test_module()\n",
    "# x = torch.randint(-127,127,(1,512)).cuda()\n",
    "# model.eval()\n",
    "# model.cuda()\n",
    "# print(model.layers[0].weight)\n",
    "# print(x.dtype)\n",
    "# with torch.no_grad():\n",
    "#     y = model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after feature torch.Size([1, 7, 7, 512])\n",
      "after flatten torch.Size([1, 25088]) torch.int8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Char but found Int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/workspace/int_infer/extension.ipynb Cell 23\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m-\u001b[39m\u001b[39m127\u001b[39m,\u001b[39m127\u001b[39m,(\u001b[39m1\u001b[39m,\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m,\u001b[39m4\u001b[39m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint8)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     y \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(hooks), \u001b[39mlen\u001b[39m(modules), \u001b[39mlen\u001b[39m(before_l), \u001b[39mlen\u001b[39m(after_l))\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     remove_forward_hook(hooks)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/workspace/int_infer/extension.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mafter flatten \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/workspace/int_infer/extension.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# weight [OUT, IN} - > [IN, OUT]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# input [BATCH, IN]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     y \u001b[39m=\u001b[39m int8mm_cuda\u001b[39m.\u001b[39;49mint8_mm(x,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mcontiguous())\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/extension.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Char but found Int"
     ]
    }
   ],
   "source": [
    "model = int_vgg(\"D\")\n",
    "model.eval()\n",
    "hooks = add_forward_hook(model, hooks)\n",
    "# remove hook, hook works at once\n",
    "remove_forward_hook(hooks)\n",
    "model.cuda()\n",
    "with torch.no_grad():\n",
    "    x = torch.randint(-127,127,(1,224,224,4), dtype=torch.int8).cuda()\n",
    "    y = model(x)\n",
    "    print(len(hooks), len(modules), len(before_l), len(after_l))\n",
    "    remove_forward_hook(hooks)\n",
    "    hooks=[]\n",
    "print(y.dtype, y.shape, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  77,   60,   40],\n",
      "          [  93,   43,  -67],\n",
      "          [  88,   87,  122],\n",
      "          [ 114,    7,  -48]],\n",
      "\n",
      "         [[-101,  -36,   60],\n",
      "          [  -5,  -62,  -72],\n",
      "          [ -30,   30,  -32],\n",
      "          [  34,  101,    7]],\n",
      "\n",
      "         [[  68,  120,   36],\n",
      "          [-109,  -55,    9],\n",
      "          [ -99,   61,  -12],\n",
      "          [ -57,  -66,   -7]],\n",
      "\n",
      "         [[  53,   16,   57],\n",
      "          [-116,   80,  -19],\n",
      "          [  -2,  -51,  -21],\n",
      "          [ -72,   69,   96]]]], device='cuda:0', dtype=torch.int8)\n",
      "tensor([[[[ 77,  60,  40],\n",
      "          [ 93,  43,   0],\n",
      "          [ 88,  87, 122],\n",
      "          [114,   7,   0]],\n",
      "\n",
      "         [[  0,   0,  60],\n",
      "          [  0,   0,   0],\n",
      "          [  0,  30,   0],\n",
      "          [ 34, 101,   7]],\n",
      "\n",
      "         [[ 68, 120,  36],\n",
      "          [  0,   0,   9],\n",
      "          [  0,  61,   0],\n",
      "          [  0,   0,   0]],\n",
      "\n",
      "         [[ 53,  16,  57],\n",
      "          [  0,  80,   0],\n",
      "          [  0,   0,   0],\n",
      "          [  0,  69,  96]]]], device='cuda:0', dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "i = torch.randint(-127, 127,(1,4,4,3), dtype=torch.int8).cuda()\n",
    "lay = nn.Dropout(0.5)\n",
    "\n",
    "lay.cuda()\n",
    "with torch.no_grad():\n",
    "    lay.eval()\n",
    "    y = lay(i)\n",
    "    k = nn.functional.relu(y)\n",
    "print(y)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n",
      "tensor([[ -574282,  -967096,   718499,  -854490,  -480142,  1088080,  -752827,\n",
      "          -116280,  -313422,  -976545,  -148737,  -783753,  -647241,  -154183,\n",
      "           160387,  -120619,   -25240,   725703,  -217888,  -175336, -1143886,\n",
      "          -437042,  -478281,   371989,  -784157,  -368922,    77165,  -562955,\n",
      "          -791538, -1247484,  -639698,  -404810,  -830884,  -352098,  -982993,\n",
      "         -1149701,   388485,   430506, -1301755,  -722884, -1382657,  -926842,\n",
      "          -209861,   663056,   616099, -1206340,  -615280,  -344674,   939926,\n",
      "           749104,  -966203,   174706,    50013,  -642108,  -657352,  -455320,\n",
      "          -236708,  -146362,   413926,   203769,  -677935, -1635767,  -263175,\n",
      "         -1041366,   -54477,  -756444,  -234938,  -921265,  -493369, -1095055,\n",
      "          -715730,    15213,   374301,   473852,  -453373, -1359736,   253718,\n",
      "          -337226,  -387232,  -113524,     6194,   477962, -1093968,   738111,\n",
      "          -730462,   -55275,  -436522, -1422282,   354789,  -365973,  -717804,\n",
      "           999484,  -231589,    94428,  -324601,   283188,  -638532,  -392440,\n",
      "         -1528391,   309704]], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "from models import vgg\n",
    "\n",
    "model = vgg.int_vgg16(\"D\")\n",
    "x = torch.randint(-128,127,(1,224,224,4),dtype=torch.int8).cuda()\n",
    "model.eval()\n",
    "model.cuda()\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "    print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[262145, 262145, 262145],\n",
      "        [262145, 262145, 262145],\n",
      "        [262145, 262145, 262145]])\n",
      "tensor([[294913, 294913, 294913],\n",
      "        [294913, 294913, 294913],\n",
      "        [294913, 294913, 294913]])\n",
      "tensor([[65535, 65535, 65535],\n",
      "        [65535, 65535, 65535],\n",
      "        [65535, 65535, 65535]])\n",
      "tensor([[255.9980, 255.9980, 255.9980],\n",
      "        [255.9980, 255.9980, 255.9980],\n",
      "        [255.9980, 255.9980, 255.9980]])\n",
      "tensor([[127.9980, 127.9980, 127.9980],\n",
      "        [127.9980, 127.9980, 127.9980],\n",
      "        [127.9980, 127.9980, 127.9980]])\n",
      "tensor([[127, 127, 127],\n",
      "        [127, 127, 127],\n",
      "        [127, 127, 127]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((3,3), dtype=torch.int64)*(2**18+1)\n",
    "print(x)\n",
    "x = x + 2**15\n",
    "print(x)\n",
    "x = torch.clamp(x, min=0, max=2**16-1)\n",
    "print(x)\n",
    "x = torch.sqrt(x)\n",
    "print(x)\n",
    "x = x-128\n",
    "print(x)\n",
    "x = x.type(torch.int8)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 1])\n",
      "tensor([[[[1],\n",
      "          [2],\n",
      "          [3]],\n",
      "\n",
      "         [[4],\n",
      "          [5],\n",
      "          [6]],\n",
      "\n",
      "         [[7],\n",
      "          [8],\n",
      "          [9]]]])\n",
      "torch.Size([1, 3, 3, 4])\n",
      "tensor([[[[1, 1, 1, 1],\n",
      "          [2, 2, 2, 2],\n",
      "          [3, 3, 3, 3]],\n",
      "\n",
      "         [[4, 4, 4, 4],\n",
      "          [5, 5, 5, 5],\n",
      "          [6, 6, 6, 6]],\n",
      "\n",
      "         [[7, 7, 7, 7],\n",
      "          [8, 8, 8, 8],\n",
      "          [9, 9, 9, 9]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3],[4,5,6],[7,8,9]]).reshape(1,3,3,1)\n",
    "print(a.shape, a,sep='\\n')\n",
    "a = a.repeat(1,1,1,4)\n",
    "print(a.shape, a,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv: torch.Size([1, 3, 3, 4])\n",
      "tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]]], device='cuda:0')\n",
      "x : torch.Size([1, 3, 3, 4]) \n",
      "tensor([[[[1., 1., 1., 1.],\n",
      "          [2., 2., 2., 2.],\n",
      "          [3., 3., 3., 3.]],\n",
      "\n",
      "         [[4., 4., 4., 4.],\n",
      "          [5., 5., 5., 5.],\n",
      "          [6., 6., 6., 6.]],\n",
      "\n",
      "         [[7., 7., 7., 7.],\n",
      "          [8., 8., 8., 8.],\n",
      "          [9., 9., 9., 9.]]]], device='cuda:0')\n",
      "y : torch.Size([1, 3, 3, 1])\n",
      "tensor([[[[ 48.],\n",
      "          [ 84.],\n",
      "          [ 64.]],\n",
      "\n",
      "         [[108.],\n",
      "          [180.],\n",
      "          [132.]],\n",
      "\n",
      "         [[ 96.],\n",
      "          [156.],\n",
      "          [112.]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class FloatConv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride =1, padding =1):\n",
    "        super(FloatConv2d,self).__init__()\n",
    "        self.weight = torch.ones((out_channels, kernel_size, kernel_size, in_channels),dtype=torch.float32)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self,x):\n",
    "        # trans_weight = torch.flip(self.weight,[1,2]).transpose(0,3).contiguous()\n",
    "        # trans_weight = self.weight.permute(0,2,3,1).contiguous()\n",
    "        trans_weight = self.weight\n",
    "        y = int8conv_cuda.float_conv(x,trans_weight,self.stride, self.padding,1)\n",
    "        return y\n",
    "    \n",
    "    def cuda(self):\n",
    "        self.weight = self.weight.cuda()\n",
    "\n",
    "\n",
    "input_channel= 4\n",
    "conv = FloatConv2d(input_channel,1,3,1,1)\n",
    "conv.cuda()\n",
    "print(f\"conv: {conv.weight.shape}\\n{conv.weight}\")\n",
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9]],dtype=torch.float32).reshape(1,3,3,1).repeat(1,1,1,input_channel).cuda()\n",
    "print(f\"x : {x.shape} \\n{x}\")\n",
    "with torch.no_grad():\n",
    "    y= conv(x)\n",
    "    print(f\"y : {y.shape}\\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
