{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import _log_api_usage_once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def manual_seed(seed):\n",
    "    np.random.seed(seed) #1\n",
    "    random.seed(seed) #2\n",
    "    torch.manual_seed(seed) #3\n",
    "    torch.cuda.manual_seed(seed) #4.1\n",
    "    torch.cuda.manual_seed_all(seed) #4.2\n",
    "    torch.backends.cudnn.benchmark = False #5 \n",
    "    torch.backends.cudnn.deterministic = True #6\n",
    "\n",
    "manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantized(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        self.model = model\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    def fuse_model(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# filepath = './checkpoint/vgg16.pth'\n",
    "# checkpoint = torch.load(filepath)\n",
    "# model = checkpoint['model']\n",
    "# print(model)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = torchvision.models.vgg.vgg16(pretrained=True)\n",
    "model = Quantized(model)\n",
    "pretrain_transforms = torchvision.models.VGG16_Weights.DEFAULT.transforms()\n",
    "print(pretrain_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    pretrain_transforms,\n",
    "    ])\n",
    "\n",
    "val_data = torchvision.datasets.ImageNet(root=\"./dataset/ImageNet\", split=\"val\", transform=test_transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=32,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def memory_usage(message = \"debug\"):\n",
    "    p = psutil.Process()\n",
    "    rss = p.memory_info().rss / 2**20\n",
    "    print(f\"{message} memory usage : {rss:10.5f} MB\")\n",
    "\n",
    "def test(model, test_loader, num_calib=None):\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    test_acc = 0\n",
    "    with torch.inference_mode():\n",
    "        for i,data in enumerate(tqdm(test_loader,leave=True)):\n",
    "            imgs, target = data[0], data[1]\n",
    "            # imgs, target = data[0].to(device), data[1].to(device)\n",
    "            output = model(imgs)\n",
    "            _, preds = torch.max(output.data, 1)\n",
    "            test_acc += (preds==target).detach().sum().item()\n",
    "            if num_calib and (i > num_calib):\n",
    "                break\n",
    "            # if i % 10 == 0:\n",
    "            #     memory_usage()\n",
    "\n",
    "    test_acc = 100. * test_acc/len(test_loader.dataset)\n",
    "    \n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = []\n",
    "before_l = []\n",
    "after_l = []\n",
    "hooks = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    modules.append(module)\n",
    "    before_l.append(input[0])\n",
    "    after_l.append(output)\n",
    "\n",
    "def add_forward_hook(net, hooks):\n",
    "    for name, layer in net._modules.items():\n",
    "        if isinstance(layer, nn.Sequential) or isinstance(layer, torchvision.models.vgg.VGG):\n",
    "            add_forward_hook(layer, hooks)\n",
    "        else:\n",
    "            hook = layer.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "            \n",
    "    return hooks\n",
    "\n",
    "def remove_forward_hook(hooks):\n",
    "    for i in hooks:\n",
    "        i.remove()\n",
    "# out = model((torch.randn(1,3,32,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defualt qconfig ; QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "x86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1563 [00:23<54:29,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41 41 41\n",
      "Quantized(\n",
      "  (model): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.17084623873233795, zero_point=58, padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.35520821809768677, zero_point=73, padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.5221890211105347, zero_point=82, padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.7225201725959778, zero_point=71, padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.9027827978134155, zero_point=74, padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.046532154083252, zero_point=71, padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.045250415802002, zero_point=72, padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.2762900590896606, zero_point=75, padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0824809074401855, zero_point=80, padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.887078046798706, zero_point=83, padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.9002187848091125, zero_point=65, padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.7038030624389648, zero_point=81, padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.6258416175842285, zero_point=75, padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): QuantizedLinear(in_features=25088, out_features=4096, scale=0.42471253871917725, zero_point=77, qscheme=torch.per_channel_affine)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): QuantizedDropout(p=0.5, inplace=False)\n",
      "      (3): QuantizedLinear(in_features=4096, out_features=4096, scale=0.24681514501571655, zero_point=79, qscheme=torch.per_channel_affine)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): QuantizedDropout(p=0.5, inplace=False)\n",
      "      (6): QuantizedLinear(in_features=4096, out_features=1000, scale=0.42873188853263855, zero_point=32, qscheme=torch.per_channel_affine)\n",
      "    )\n",
      "  )\n",
      "  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()\n",
    "model.fuse_model()\n",
    "backend = \"fbgemm\"\n",
    "qconfig = torch.ao.quantization.get_default_qconfig(backend)\n",
    "model.qconfig = qconfig\n",
    "print(f\"defualt qconfig ; {model.qconfig}\")\n",
    "torch.backends.quantized.engine = backend\n",
    "print(torch.backends.quantized.engine)\n",
    "model_static_quantized = torch.ao.quantization.prepare(model, inplace = False)\n",
    "# calibration\n",
    "test(model_static_quantized,val_loader,10)\n",
    "\n",
    "# make quantized model\n",
    "torch.ao.quantization.convert(model_static_quantized, inplace = True) \n",
    "\n",
    "# make hook\n",
    "hooks = add_forward_hook(model_static_quantized, hooks)\n",
    "sample = torch.randn(1,3,256,256)\n",
    "model_static_quantized(sample)\n",
    "print(len(hooks), len(modules), len(before_l), len(after_l))\n",
    "# remove hook, hook works at once\n",
    "remove_forward_hook(hooks)\n",
    "# for _ in range(5):\n",
    "#     sample = (torch.randn(1,3,224,224))\n",
    "#     model_static_quantized(sample)\n",
    "# print(len(modules), len(before_l), len(after_l))\n",
    "print(model_static_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1060/1563 [08:21<03:57,  2.12it/s]"
     ]
    }
   ],
   "source": [
    "test_acc = test(model_static_quantized, val_loader)\n",
    "print(f\"test result : {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.ao.nn.quantized.modules.Quantize'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.dropout.Dropout'>\n",
      "<class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.dropout.Dropout'>\n",
      "<class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "<class 'torch.ao.nn.quantized.modules.DeQuantize'>\n",
      "41 41 41\n",
      "<class 'torch.ao.nn.quantized.modules.Quantize'>\n",
      "before : <class 'torch.Tensor'>, torch.float32\n",
      "after : <class 'torch.Tensor'>, torch.quint8\n",
      "tensor([[[[ 0.2494,  0.5611,  3.0550,  ...,  0.1247,  0.0000, -1.6210],\n",
      "          [ 0.1247,  0.4988,  0.0000,  ..., -1.4340,  0.7482,  0.5611],\n",
      "          [ 0.0000,  2.7433, -0.4364,  ..., -0.6858,  0.1870,  0.0000],\n",
      "          ...,\n",
      "          [ 0.4988,  0.5611, -0.1870,  ...,  0.0000,  0.5611, -0.7482],\n",
      "          [-0.4364, -0.3741, -1.0599,  ..., -0.5611, -1.6210, -0.6858],\n",
      "          [-0.1247, -2.3069, -0.7482,  ...,  1.0599,  0.0000, -0.0623]],\n",
      "\n",
      "         [[-1.2470,  0.2494, -1.2470,  ..., -2.0575, -0.1247, -1.2470],\n",
      "          [ 1.6210, -0.5611, -0.6858,  ...,  0.0000,  1.1223, -2.3069],\n",
      "          [ 1.6210, -0.7482,  1.3717,  ..., -0.4364,  1.1223,  0.3741],\n",
      "          ...,\n",
      "          [ 1.6210,  0.2494, -1.0599,  ...,  1.1846,  0.4988,  0.8729],\n",
      "          [-0.1870, -0.8105,  1.5587,  ..., -1.4340,  1.3093, -0.3117],\n",
      "          [ 0.0623,  0.1870,  0.4988,  ..., -0.4988,  0.5611, -1.8081]],\n",
      "\n",
      "         [[-0.1870, -0.0623, -1.1223,  ..., -0.5611, -1.6210,  0.0000],\n",
      "          [-0.2494, -0.6858,  0.8729,  ..., -1.5587,  0.7482,  0.6235],\n",
      "          [-0.4364, -0.2494,  1.7457,  ..., -0.8105, -0.3741, -0.2494],\n",
      "          ...,\n",
      "          [ 0.6235, -0.1870, -0.2494,  ..., -0.1870, -0.3741, -1.0599],\n",
      "          [-0.3741, -0.0623, -1.4340,  ..., -2.1198,  2.8057,  0.1247],\n",
      "          [ 0.6858, -1.2470, -0.4364,  ...,  1.9951, -0.1247, -0.6235]]]],\n",
      "       size=(1, 3, 256, 256), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.06234787032008171,\n",
      "       zero_point=63)\n",
      "tensor([[[[1.1618, 3.3564, 0.2582,  ..., 0.2582, 0.0000, 0.0000],\n",
      "          [2.8400, 1.2909, 0.0000,  ..., 1.8073, 0.0000, 0.0000],\n",
      "          [2.7110, 1.0327, 0.0000,  ..., 2.0655, 3.0982, 0.0000],\n",
      "          ...,\n",
      "          [1.2909, 1.9364, 0.0000,  ..., 0.0000, 0.2582, 4.2601],\n",
      "          [0.0000, 0.5164, 1.2909,  ..., 0.0000, 0.0000, 2.5819],\n",
      "          [0.0000, 0.0000, 2.8400,  ..., 0.0000, 0.0000, 2.4528]],\n",
      "\n",
      "         [[1.1618, 0.0000, 0.0000,  ..., 1.5491, 1.9364, 0.9037],\n",
      "          [0.0000, 1.0327, 3.2273,  ..., 3.0982, 1.4200, 0.0000],\n",
      "          [1.1618, 0.0000, 0.0000,  ..., 3.6146, 2.0655, 0.2582],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 2.5819,  ..., 0.0000, 1.1618, 0.5164],\n",
      "          [0.0000, 0.1291, 3.6146,  ..., 2.8400, 1.0327, 0.0000],\n",
      "          [0.0000, 1.1618, 2.7110,  ..., 0.7746, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.4200, 0.3873, 0.3873,  ..., 5.0346, 0.0000, 3.3564],\n",
      "          [0.0000, 0.5164, 0.7746,  ..., 2.4528, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 2.0655, 1.2909, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 4.2601,  ..., 1.4200, 0.0000, 0.0000],\n",
      "          [1.6782, 4.0019, 1.2909,  ..., 2.8400, 0.0000, 1.8073],\n",
      "          [0.0000, 1.9364, 0.6455,  ..., 0.0000, 0.0000, 2.9691]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000, 0.3873, 0.7746,  ..., 0.2582, 0.3873, 0.0000],\n",
      "          [0.1291, 1.0327, 0.9037,  ..., 0.3873, 0.9037, 0.3873],\n",
      "          [0.0000, 0.0000, 1.2909,  ..., 0.0000, 0.2582, 0.0000],\n",
      "          ...,\n",
      "          [0.2582, 0.7746, 0.3873,  ..., 0.0000, 0.0000, 0.1291],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.2582, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.2582, 0.3873, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5164, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2582],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1291]],\n",
      "\n",
      "         [[0.1291, 0.2582, 0.7746,  ..., 0.3873, 0.0000, 0.3873],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3873],\n",
      "          [0.0000, 1.4200, 0.1291,  ..., 0.0000, 0.0000, 0.9037],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 1.5491, 0.0000, 0.0000],\n",
      "          [1.1618, 1.0327, 0.0000,  ..., 0.0000, 0.0000, 1.2909],\n",
      "          [1.0327, 0.0000, 0.0000,  ..., 0.0000, 1.9364, 2.0655]]]],\n",
      "       size=(1, 64, 256, 256), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.12909291684627533,\n",
      "       zero_point=59)\n"
     ]
    }
   ],
   "source": [
    "for i in modules:\n",
    "    print(type(i))\n",
    "print(len(modules), len(before_l), len(after_l))\n",
    "for i in range(len(modules)):\n",
    "    if not isinstance(type(modules[i]), torch.quantization.QuantStub) and not isinstance(type(modules[i]), torch.quantization.DeQuantStub):\n",
    "        print(type(modules[i]))\n",
    "        print(f\"before : {type(before_l[i])}, {before_l[i].dtype}\")\n",
    "        print(f\"after : {type(after_l[i])}, {after_l[i].dtype}\")\n",
    "        break\n",
    "\n",
    "print(before_l[1])\n",
    "print(after_l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "torch.quint8 tensor([[[[68, 85, 61,  ..., 61, 59, 59],\n",
      "          [81, 69, 59,  ..., 73, 59, 59],\n",
      "          [80, 67, 59,  ..., 75, 83, 59],\n",
      "          ...,\n",
      "          [69, 74, 59,  ..., 59, 61, 92],\n",
      "          [59, 63, 69,  ..., 59, 59, 79],\n",
      "          [59, 59, 81,  ..., 59, 59, 78]],\n",
      "\n",
      "         [[68, 59, 59,  ..., 71, 74, 66],\n",
      "          [59, 67, 84,  ..., 83, 70, 59],\n",
      "          [68, 59, 59,  ..., 87, 75, 61],\n",
      "          ...,\n",
      "          [59, 59, 79,  ..., 59, 68, 63],\n",
      "          [59, 60, 87,  ..., 81, 67, 59],\n",
      "          [59, 68, 80,  ..., 65, 59, 59]],\n",
      "\n",
      "         [[70, 62, 62,  ..., 98, 59, 85],\n",
      "          [59, 63, 65,  ..., 78, 59, 59],\n",
      "          [59, 59, 59,  ..., 75, 69, 59],\n",
      "          ...,\n",
      "          [59, 59, 92,  ..., 70, 59, 59],\n",
      "          [72, 90, 69,  ..., 81, 59, 73],\n",
      "          [59, 74, 64,  ..., 59, 59, 82]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[59, 62, 65,  ..., 61, 62, 59],\n",
      "          [60, 67, 66,  ..., 62, 66, 62],\n",
      "          [59, 59, 69,  ..., 59, 61, 59],\n",
      "          ...,\n",
      "          [61, 65, 62,  ..., 59, 59, 60],\n",
      "          [59, 59, 59,  ..., 61, 59, 59],\n",
      "          [59, 59, 59,  ..., 61, 62, 59]],\n",
      "\n",
      "         [[59, 59, 59,  ..., 59, 59, 59],\n",
      "          [59, 59, 59,  ..., 59, 59, 59],\n",
      "          [59, 59, 59,  ..., 63, 59, 59],\n",
      "          ...,\n",
      "          [59, 59, 59,  ..., 59, 59, 59],\n",
      "          [59, 59, 59,  ..., 59, 59, 61],\n",
      "          [59, 59, 59,  ..., 59, 59, 60]],\n",
      "\n",
      "         [[60, 61, 65,  ..., 62, 59, 62],\n",
      "          [59, 59, 59,  ..., 59, 59, 62],\n",
      "          [59, 70, 60,  ..., 59, 59, 66],\n",
      "          ...,\n",
      "          [59, 59, 59,  ..., 71, 59, 59],\n",
      "          [68, 67, 59,  ..., 59, 59, 69],\n",
      "          [67, 59, 59,  ..., 59, 74, 75]]]], dtype=torch.uint8)\n",
      "torch.quint8 tensor([[[[1.1618, 3.3564, 0.2582,  ..., 0.2582, 0.0000, 0.0000],\n",
      "          [2.8400, 1.2909, 0.0000,  ..., 1.8073, 0.0000, 0.0000],\n",
      "          [2.7110, 1.0327, 0.0000,  ..., 2.0655, 3.0982, 0.0000],\n",
      "          ...,\n",
      "          [1.2909, 1.9364, 0.0000,  ..., 0.0000, 0.2582, 4.2601],\n",
      "          [0.0000, 0.5164, 1.2909,  ..., 0.0000, 0.0000, 2.5819],\n",
      "          [0.0000, 0.0000, 2.8400,  ..., 0.0000, 0.0000, 2.4528]],\n",
      "\n",
      "         [[1.1618, 0.0000, 0.0000,  ..., 1.5491, 1.9364, 0.9037],\n",
      "          [0.0000, 1.0327, 3.2273,  ..., 3.0982, 1.4200, 0.0000],\n",
      "          [1.1618, 0.0000, 0.0000,  ..., 3.6146, 2.0655, 0.2582],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 2.5819,  ..., 0.0000, 1.1618, 0.5164],\n",
      "          [0.0000, 0.1291, 3.6146,  ..., 2.8400, 1.0327, 0.0000],\n",
      "          [0.0000, 1.1618, 2.7110,  ..., 0.7746, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.4200, 0.3873, 0.3873,  ..., 5.0346, 0.0000, 3.3564],\n",
      "          [0.0000, 0.5164, 0.7746,  ..., 2.4528, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 2.0655, 1.2909, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 4.2601,  ..., 1.4200, 0.0000, 0.0000],\n",
      "          [1.6782, 4.0019, 1.2909,  ..., 2.8400, 0.0000, 1.8073],\n",
      "          [0.0000, 1.9364, 0.6455,  ..., 0.0000, 0.0000, 2.9691]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000, 0.3873, 0.7746,  ..., 0.2582, 0.3873, 0.0000],\n",
      "          [0.1291, 1.0327, 0.9037,  ..., 0.3873, 0.9037, 0.3873],\n",
      "          [0.0000, 0.0000, 1.2909,  ..., 0.0000, 0.2582, 0.0000],\n",
      "          ...,\n",
      "          [0.2582, 0.7746, 0.3873,  ..., 0.0000, 0.0000, 0.1291],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.2582, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.2582, 0.3873, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5164, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2582],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1291]],\n",
      "\n",
      "         [[0.1291, 0.2582, 0.7746,  ..., 0.3873, 0.0000, 0.3873],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3873],\n",
      "          [0.0000, 1.4200, 0.1291,  ..., 0.0000, 0.0000, 0.9037],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 1.5491, 0.0000, 0.0000],\n",
      "          [1.1618, 1.0327, 0.0000,  ..., 0.0000, 0.0000, 1.2909],\n",
      "          [1.0327, 0.0000, 0.0000,  ..., 0.0000, 1.9364, 2.0655]]]],\n",
      "       size=(1, 64, 256, 256), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.12909291684627533,\n",
      "       zero_point=59)\n"
     ]
    }
   ],
   "source": [
    "print(type(modules[2]))\n",
    "print(before_l[2].dtype,before_l[2].int_repr())\n",
    "print(after_l[2].dtype,after_l[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.ao.nn.quantized.modules.DeQuantize'>\n",
      "torch.quint8 tensor([[ 0.3106,  2.7956, -1.2425, -1.0872,  0.0777,  2.2520,  1.3978,  0.6212,\n",
      "          0.4659,  0.3106,  0.9319,  2.1743,  1.2425,  0.6989,  0.3883,  1.8637,\n",
      "         -1.6308, -1.4754,  0.8542, -0.2330, -0.5436, -0.6989,  0.6989,  0.3883,\n",
      "         -1.0095, -1.1648,  0.7766, -0.0777, -0.9319,  0.5436, -1.6308, -1.4754,\n",
      "         -1.9414, -0.1553,  0.0777, -0.6989, -0.3106, -1.5531, -0.8542, -1.2425,\n",
      "         -0.4659, -1.5531, -0.6989, -0.8542, -1.7861,  0.1553, -0.2330, -1.4754,\n",
      "         -0.8542, -0.8542,  0.0000, -2.1743, -0.6212, -1.1648, -1.5531, -1.5531,\n",
      "         -2.1743, -2.5626,  0.0000, -1.9414, -1.0095, -2.5626, -2.1743, -0.3106,\n",
      "         -1.0095,  0.8542, -1.3978, -1.7084, -2.6403, -0.1553,  0.9319,  0.3883,\n",
      "          0.3106,  1.3978,  1.4754,  1.5531,  0.6212,  2.0190,  2.7956,  1.8637,\n",
      "          0.3106, -0.4659, -0.0777,  0.2330, -0.2330,  0.2330,  1.2425, -1.2425,\n",
      "          1.3201,  0.0000,  0.2330,  0.4659,  2.2520,  0.1553, -0.0777, -1.2425,\n",
      "          1.8637, -0.7766, -1.0095,  0.9319, -0.1553, -0.2330, -1.1648,  0.4659,\n",
      "         -0.2330, -1.6308, -0.3883,  3.9604,  2.7956,  2.4850,  2.7179,  3.2615,\n",
      "          0.8542,  0.5436,  0.6989,  2.1743,  1.8637,  0.9319,  1.8637,  0.3106,\n",
      "         -0.0777,  0.4659, -0.6212, -0.1553,  0.2330, -0.8542,  2.2520,  0.6989,\n",
      "          0.1553,  0.9319,  1.0095, -0.6989,  1.0872,  0.6989,  1.4754, -0.0777,\n",
      "         -1.0872, -0.6212,  0.7766, -1.0872, -1.0872, -0.7766, -0.6989, -0.2330,\n",
      "         -0.5436,  1.2425, -1.4754, -1.0872, -0.6989, -0.7766,  0.4659,  0.3106,\n",
      "         -1.6308, -0.6212, -0.9319, -0.7766, -0.6212, -0.1553, -0.8542,  0.0000,\n",
      "         -0.8542, -0.3106,  0.1553,  0.6212, -1.3978, -0.6212, -0.8542, -0.6212,\n",
      "         -0.1553, -0.6212, -0.7766, -1.0095, -1.3978, -0.1553, -1.7861, -0.3883,\n",
      "         -1.3201, -1.0872, -0.8542, -1.3201, -1.6308, -0.4659, -0.2330, -0.7766,\n",
      "         -0.1553, -0.3883, -0.1553, -0.3106, -0.9319, -0.1553, -1.5531,  1.0872,\n",
      "         -0.9319,  0.4659, -0.3883, -0.3883, -1.1648, -0.7766, -1.8637, -0.3106,\n",
      "         -0.7766, -0.4659, -0.3106,  1.0095,  0.0000, -1.4754, -1.0095,  0.5436,\n",
      "         -0.6989, -1.0095, -2.7956,  0.6212, -0.7766,  0.9319,  0.2330, -0.1553,\n",
      "         -0.6989, -0.8542, -0.3883, -0.1553, -0.3883,  0.5436, -0.4659, -0.5436,\n",
      "         -0.3883, -0.6989, -1.0095, -0.5436, -0.5436,  0.6989,  0.5436,  0.7766,\n",
      "         -0.7766, -1.0095,  0.5436, -0.2330, -0.0777, -0.2330, -0.2330, -0.2330,\n",
      "          0.3106, -0.3106, -0.3883, -0.6989, -0.5436, -0.7766, -1.9414, -0.3883,\n",
      "         -0.6989, -0.8542,  0.0777, -1.1648, -0.7766,  0.1553, -0.0777, -0.6989,\n",
      "         -0.6212, -0.3106,  0.3106,  1.3201,  1.4754, -0.6212, -1.2425,  0.6212,\n",
      "         -0.8542,  0.3106, -1.2425, -0.3883, -2.6403,  0.0000,  0.6212,  2.0190,\n",
      "          0.6989,  1.4754,  1.8637,  0.2330,  0.2330,  2.1743,  0.7766,  1.3978,\n",
      "          1.3978, -0.6989,  0.4659,  0.3883,  0.0777, -0.6989,  0.4659, -1.2425,\n",
      "         -0.3106, -1.4754, -1.3978,  0.3106,  0.6212,  0.5436,  1.0095,  0.7766,\n",
      "          2.0190,  0.0000,  0.9319,  0.3106, -1.3978,  1.2425,  0.6212,  0.3106,\n",
      "          0.2330, -1.0095,  0.2330,  0.6212,  0.6212,  1.4754,  3.9604, -1.3201,\n",
      "         -0.6989,  1.0872,  2.3297, -0.6989, -1.0872, -1.0872,  0.9319,  0.0000,\n",
      "         -1.0872, -2.0190, -1.3978, -0.0777, -1.8637, -0.3883, -1.9414,  1.5531,\n",
      "          2.4850,  2.7179, -0.2330,  0.9319,  0.2330,  0.0777,  1.3978,  0.6989,\n",
      "         -0.4659,  0.8542,  0.3106, -0.4659,  0.1553,  0.0777, -0.2330, -1.5531,\n",
      "          0.2330,  0.5436, -0.4659,  0.9319, -1.1648, -1.6308, -0.4659,  0.9319,\n",
      "          1.1648,  1.5531,  0.1553, -0.4659,  0.5436, -0.2330, -0.0777, -0.3883,\n",
      "         -0.3106,  0.1553,  0.6989, -0.9319, -2.2520,  1.3201, -1.4754, -0.7766,\n",
      "          0.4659, -1.0095,  0.7766,  0.9319,  0.4659, -0.4659,  1.5531, -0.2330,\n",
      "          0.9319,  1.2425,  0.3883,  0.6989,  1.1648,  1.0095,  1.1648, -0.3106,\n",
      "         -0.1553,  0.1553, -0.5436,  0.7766, -0.6989, -0.2330,  0.6212,  0.8542,\n",
      "          1.7084,  0.6212, -0.9319,  0.6212,  0.3883,  0.0000, -0.3106,  1.1648,\n",
      "         -0.9319, -0.5436, -0.2330, -2.0190, -1.0872,  0.8542,  0.2330, -1.5531,\n",
      "         -3.0285,  1.7861,  0.6989,  0.8542,  0.8542, -0.7766, -0.3106, -0.4659,\n",
      "         -0.1553,  2.2520,  0.1553, -0.2330,  0.1553,  0.3883, -1.0095, -1.1648,\n",
      "          0.0000,  1.3978,  0.0777,  1.3201,  0.5436,  1.0095,  0.9319, -1.9414,\n",
      "         -0.0777,  1.1648,  0.9319,  1.3201, -2.0190,  1.3978, -0.1553, -0.1553,\n",
      "          0.0000,  2.0967, -0.5436,  0.5436,  0.0777,  0.3883,  2.4073, -0.2330,\n",
      "         -0.5436, -0.7766, -2.7179, -0.0777,  0.5436, -2.2520, -1.3978,  0.0777,\n",
      "         -0.7766, -0.3883,  1.5531,  0.0000, -0.8542,  0.5436,  1.7084,  0.8542,\n",
      "         -0.7766, -1.1648, -2.4073, -0.5436,  0.1553,  1.5531,  2.5626, -0.5436,\n",
      "         -0.3883, -2.0967, -0.5436,  1.2425, -0.6989, -2.0967, -0.6212, -1.7861,\n",
      "          0.9319, -0.8542, -1.6308, -1.0872, -1.0095, -0.3106,  0.6212,  0.2330,\n",
      "          1.7861,  3.1839,  2.0190, -0.6212, -0.3883,  0.1553,  1.2425, -1.7861,\n",
      "         -0.3106,  0.8542, -0.0777, -0.3106, -0.8542,  0.9319, -1.7084, -0.6212,\n",
      "          0.6989, -0.3883,  0.9319, -0.6212, -0.8542, -2.4073, -2.1743, -1.1648,\n",
      "         -1.0095, -0.1553, -1.1648, -1.0095, -1.7084,  0.1553, -0.5436, -0.8542,\n",
      "         -1.9414,  1.2425, -0.1553, -0.2330, -0.1553, -1.2425, -1.9414, -0.3883,\n",
      "         -0.8542, -1.7861,  1.0095, -0.5436, -0.6989,  0.8542,  0.3883, -3.1839,\n",
      "         -1.2425, -0.7766,  1.5531,  3.1839, -0.1553,  0.6989,  0.6212, -1.3201,\n",
      "          1.0095,  0.1553, -0.7766, -2.2520, -1.7084,  2.6403, -2.1743,  0.3883,\n",
      "          1.5531, -0.6212, -0.9319, -1.3978,  5.7465,  1.1648,  0.0000,  0.9319,\n",
      "         -1.2425, -1.6308,  3.4168,  0.6989, -1.2425,  0.5436, -0.7766,  1.5531,\n",
      "          0.5436, -1.4754,  0.0000, -0.6212,  0.9319, -2.4073, -0.1553, -0.6212,\n",
      "         -1.5531,  2.3297,  1.2425, -0.2330,  1.3978, -0.3883, -1.7084, -0.9319,\n",
      "         -0.5436,  0.1553, -2.1743, -0.9319,  0.1553,  0.2330, -1.0095,  1.3201,\n",
      "         -1.1648, -0.9319, -0.5436, -0.6989, -1.1648, -1.7861, -0.5436,  2.7179,\n",
      "          1.0095,  0.9319, -0.4659, -1.7861,  1.2425,  0.1553, -0.5436,  0.2330,\n",
      "         -0.4659, -0.9319,  1.1648,  1.7084, -1.1648, -0.6212, -0.0777, -1.0872,\n",
      "          0.1553, -1.0095,  0.3106,  3.6498,  1.9414, -0.5436,  0.2330, -0.4659,\n",
      "         -1.9414, -1.0872,  0.5436, -2.0190, -0.6989,  0.8542, -1.8637, -0.9319,\n",
      "          1.6308, -0.0777, -1.3978, -0.9319,  0.6212,  0.1553,  0.0000,  0.6989,\n",
      "          0.6989,  0.5436, -0.0777, -0.6989,  2.4073, -0.9319,  1.9414, -0.3883,\n",
      "         -0.4659, -0.0777,  1.0095,  1.1648, -2.5626, -0.1553, -1.4754,  1.4754,\n",
      "         -1.0872,  1.8637, -0.0777,  0.7766, -1.3978, -2.0967,  0.4659, -0.9319,\n",
      "          1.7084, -0.6212, -0.8542, -0.6212,  0.3883,  1.3201, -0.3883, -0.5436,\n",
      "          0.1553, -0.3106, -0.4659, -1.3978, -2.5626,  1.3201, -1.1648,  0.1553,\n",
      "         -1.4754,  0.1553,  1.7861,  1.1648, -1.0872, -1.1648, -0.6212, -1.7861,\n",
      "          2.4850, -0.1553, -1.6308, -2.2520,  0.5436,  0.3883, -1.0872,  1.4754,\n",
      "          1.6308, -1.4754, -0.8542, -0.2330,  1.7084,  0.9319, -0.6989,  0.4659,\n",
      "          0.8542, -2.4850, -0.5436, -0.2330, -0.3106,  0.0777, -0.6212,  0.6989,\n",
      "          2.4850, -0.9319, -0.0777, -0.8542,  1.3201, -2.7956, -0.3883, -1.0872,\n",
      "         -1.0095,  1.9414,  0.3883, -1.4754, -1.7861,  0.9319, -2.6403,  1.7084,\n",
      "          0.7766, -0.9319, -1.2425,  0.5436, -1.0872,  1.9414, -2.6403,  2.7179,\n",
      "         -0.7766, -0.1553,  1.7084, -1.6308, -1.1648,  3.0285, -0.2330, -0.1553,\n",
      "          1.2425, -0.5436, -1.0095,  0.1553,  0.6212,  2.0967, -0.0777, -2.0967,\n",
      "          0.3883,  0.0000, -0.0777,  0.0000,  0.6989, -0.7766, -0.3106, -1.1648,\n",
      "          0.7766, -1.0872, -0.9319, -1.3978,  1.3978, -0.5436,  0.1553, -0.1553,\n",
      "         -0.5436,  0.2330, -2.7956,  0.0777,  0.0777,  0.3883, -1.8637,  2.4073,\n",
      "          0.3106, -0.3883,  0.1553,  0.1553, -0.6989, -0.2330,  2.5626,  0.6212,\n",
      "         -0.3883, -1.0095, -1.2425,  0.3106, -1.8637,  0.1553, -1.0095,  0.8542,\n",
      "          0.8542, -0.6212,  4.3487, -0.4659, -0.4659, -0.5436, -1.4754,  0.7766,\n",
      "         -2.2520, -0.1553, -0.7766, -0.2330,  0.3883, -0.5436, -0.9319,  0.9319,\n",
      "         -0.9319,  1.0095, -0.9319,  2.1743,  0.7766,  0.4659, -1.4754,  3.7274,\n",
      "          0.3883, -2.0967,  2.0190,  1.3978, -0.6989, -0.3883,  0.2330, -0.9319,\n",
      "          3.8051, -0.8542, -0.3883,  2.3297,  2.3297, -1.0095, -2.0967, -0.6989,\n",
      "         -0.6212, -1.4754,  0.2330, -0.3883, -0.1553,  0.4659, -0.6212, -0.2330,\n",
      "          0.6212, -0.1553,  0.7766,  0.6989,  1.3201,  0.3106,  2.0967, -1.0872,\n",
      "         -0.7766, -0.3106, -0.3106,  1.7084,  2.2520, -0.3883,  2.0967,  0.9319,\n",
      "         -2.9509, -0.2330,  0.5436,  1.3201, -1.0872,  1.0872,  1.0872, -2.1743,\n",
      "         -2.2520, -2.6403,  0.0777, -0.2330,  3.0285, -1.1648,  0.4659, -1.2425,\n",
      "          1.3201, -1.0872, -2.2520, -0.7766,  1.3201, -0.3883, -2.0190,  1.7084,\n",
      "          0.4659, -0.3883,  0.3883,  1.8637,  1.0095,  3.1062, -0.3883, -0.2330,\n",
      "          0.0777,  0.2330, -1.2425, -1.3978,  1.7084,  0.9319,  0.5436, -0.6212,\n",
      "          1.0872,  0.3883,  0.0000, -0.0777,  1.7861,  0.3106, -0.8542,  1.3978,\n",
      "          4.8146,  1.7084, -0.3883,  0.6989,  0.8542,  1.8637,  0.8542,  3.2615,\n",
      "          0.8542, -0.2330, -1.2425, -0.4659,  1.3201,  0.8542, -0.1553,  0.3106,\n",
      "          1.7084,  4.0381,  2.0967, -0.4659, -2.0190,  1.6308, -0.3106, -1.5531,\n",
      "         -0.8542,  0.6989,  0.2330, -0.1553, -0.8542, -0.5436, -0.6212, -0.4659,\n",
      "         -0.9319,  0.3106, -1.3978, -0.5436,  1.0872,  0.8542,  0.3883,  0.0777,\n",
      "         -1.8637, -0.6212, -0.8542,  1.9414,  1.4754,  0.2330,  2.7179,  2.4073,\n",
      "          1.6308,  0.4659,  0.6212,  0.8542, -2.1743,  0.8542,  1.1648, -1.8637,\n",
      "          0.5436, -0.0777,  0.2330,  0.3106, -0.3106, -1.3201,  1.3978,  1.5531,\n",
      "          0.6212,  0.0777,  0.0000,  2.7179,  0.0777,  3.5721,  1.9414,  2.0190,\n",
      "         -0.3883,  1.7861,  1.3978,  0.4659,  4.8146, -0.5436,  0.5436,  1.4754,\n",
      "          0.6989,  0.2330, -0.5436,  1.7861,  0.6989,  1.3978,  1.4754,  2.0967,\n",
      "          1.0872, -0.3883, -0.0777,  0.7766,  0.6989,  1.3978,  1.7084,  0.2330]],\n",
      "       size=(1, 1000), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.07765501737594604,\n",
      "       zero_point=47)\n",
      "torch.float32 tensor([[ 0.3106,  2.7956, -1.2425, -1.0872,  0.0777,  2.2520,  1.3978,  0.6212,\n",
      "          0.4659,  0.3106,  0.9319,  2.1743,  1.2425,  0.6989,  0.3883,  1.8637,\n",
      "         -1.6308, -1.4754,  0.8542, -0.2330, -0.5436, -0.6989,  0.6989,  0.3883,\n",
      "         -1.0095, -1.1648,  0.7766, -0.0777, -0.9319,  0.5436, -1.6308, -1.4754,\n",
      "         -1.9414, -0.1553,  0.0777, -0.6989, -0.3106, -1.5531, -0.8542, -1.2425,\n",
      "         -0.4659, -1.5531, -0.6989, -0.8542, -1.7861,  0.1553, -0.2330, -1.4754,\n",
      "         -0.8542, -0.8542,  0.0000, -2.1743, -0.6212, -1.1648, -1.5531, -1.5531,\n",
      "         -2.1743, -2.5626,  0.0000, -1.9414, -1.0095, -2.5626, -2.1743, -0.3106,\n",
      "         -1.0095,  0.8542, -1.3978, -1.7084, -2.6403, -0.1553,  0.9319,  0.3883,\n",
      "          0.3106,  1.3978,  1.4754,  1.5531,  0.6212,  2.0190,  2.7956,  1.8637,\n",
      "          0.3106, -0.4659, -0.0777,  0.2330, -0.2330,  0.2330,  1.2425, -1.2425,\n",
      "          1.3201,  0.0000,  0.2330,  0.4659,  2.2520,  0.1553, -0.0777, -1.2425,\n",
      "          1.8637, -0.7766, -1.0095,  0.9319, -0.1553, -0.2330, -1.1648,  0.4659,\n",
      "         -0.2330, -1.6308, -0.3883,  3.9604,  2.7956,  2.4850,  2.7179,  3.2615,\n",
      "          0.8542,  0.5436,  0.6989,  2.1743,  1.8637,  0.9319,  1.8637,  0.3106,\n",
      "         -0.0777,  0.4659, -0.6212, -0.1553,  0.2330, -0.8542,  2.2520,  0.6989,\n",
      "          0.1553,  0.9319,  1.0095, -0.6989,  1.0872,  0.6989,  1.4754, -0.0777,\n",
      "         -1.0872, -0.6212,  0.7766, -1.0872, -1.0872, -0.7766, -0.6989, -0.2330,\n",
      "         -0.5436,  1.2425, -1.4754, -1.0872, -0.6989, -0.7766,  0.4659,  0.3106,\n",
      "         -1.6308, -0.6212, -0.9319, -0.7766, -0.6212, -0.1553, -0.8542,  0.0000,\n",
      "         -0.8542, -0.3106,  0.1553,  0.6212, -1.3978, -0.6212, -0.8542, -0.6212,\n",
      "         -0.1553, -0.6212, -0.7766, -1.0095, -1.3978, -0.1553, -1.7861, -0.3883,\n",
      "         -1.3201, -1.0872, -0.8542, -1.3201, -1.6308, -0.4659, -0.2330, -0.7766,\n",
      "         -0.1553, -0.3883, -0.1553, -0.3106, -0.9319, -0.1553, -1.5531,  1.0872,\n",
      "         -0.9319,  0.4659, -0.3883, -0.3883, -1.1648, -0.7766, -1.8637, -0.3106,\n",
      "         -0.7766, -0.4659, -0.3106,  1.0095,  0.0000, -1.4754, -1.0095,  0.5436,\n",
      "         -0.6989, -1.0095, -2.7956,  0.6212, -0.7766,  0.9319,  0.2330, -0.1553,\n",
      "         -0.6989, -0.8542, -0.3883, -0.1553, -0.3883,  0.5436, -0.4659, -0.5436,\n",
      "         -0.3883, -0.6989, -1.0095, -0.5436, -0.5436,  0.6989,  0.5436,  0.7766,\n",
      "         -0.7766, -1.0095,  0.5436, -0.2330, -0.0777, -0.2330, -0.2330, -0.2330,\n",
      "          0.3106, -0.3106, -0.3883, -0.6989, -0.5436, -0.7766, -1.9414, -0.3883,\n",
      "         -0.6989, -0.8542,  0.0777, -1.1648, -0.7766,  0.1553, -0.0777, -0.6989,\n",
      "         -0.6212, -0.3106,  0.3106,  1.3201,  1.4754, -0.6212, -1.2425,  0.6212,\n",
      "         -0.8542,  0.3106, -1.2425, -0.3883, -2.6403,  0.0000,  0.6212,  2.0190,\n",
      "          0.6989,  1.4754,  1.8637,  0.2330,  0.2330,  2.1743,  0.7766,  1.3978,\n",
      "          1.3978, -0.6989,  0.4659,  0.3883,  0.0777, -0.6989,  0.4659, -1.2425,\n",
      "         -0.3106, -1.4754, -1.3978,  0.3106,  0.6212,  0.5436,  1.0095,  0.7766,\n",
      "          2.0190,  0.0000,  0.9319,  0.3106, -1.3978,  1.2425,  0.6212,  0.3106,\n",
      "          0.2330, -1.0095,  0.2330,  0.6212,  0.6212,  1.4754,  3.9604, -1.3201,\n",
      "         -0.6989,  1.0872,  2.3297, -0.6989, -1.0872, -1.0872,  0.9319,  0.0000,\n",
      "         -1.0872, -2.0190, -1.3978, -0.0777, -1.8637, -0.3883, -1.9414,  1.5531,\n",
      "          2.4850,  2.7179, -0.2330,  0.9319,  0.2330,  0.0777,  1.3978,  0.6989,\n",
      "         -0.4659,  0.8542,  0.3106, -0.4659,  0.1553,  0.0777, -0.2330, -1.5531,\n",
      "          0.2330,  0.5436, -0.4659,  0.9319, -1.1648, -1.6308, -0.4659,  0.9319,\n",
      "          1.1648,  1.5531,  0.1553, -0.4659,  0.5436, -0.2330, -0.0777, -0.3883,\n",
      "         -0.3106,  0.1553,  0.6989, -0.9319, -2.2520,  1.3201, -1.4754, -0.7766,\n",
      "          0.4659, -1.0095,  0.7766,  0.9319,  0.4659, -0.4659,  1.5531, -0.2330,\n",
      "          0.9319,  1.2425,  0.3883,  0.6989,  1.1648,  1.0095,  1.1648, -0.3106,\n",
      "         -0.1553,  0.1553, -0.5436,  0.7766, -0.6989, -0.2330,  0.6212,  0.8542,\n",
      "          1.7084,  0.6212, -0.9319,  0.6212,  0.3883,  0.0000, -0.3106,  1.1648,\n",
      "         -0.9319, -0.5436, -0.2330, -2.0190, -1.0872,  0.8542,  0.2330, -1.5531,\n",
      "         -3.0285,  1.7861,  0.6989,  0.8542,  0.8542, -0.7766, -0.3106, -0.4659,\n",
      "         -0.1553,  2.2520,  0.1553, -0.2330,  0.1553,  0.3883, -1.0095, -1.1648,\n",
      "          0.0000,  1.3978,  0.0777,  1.3201,  0.5436,  1.0095,  0.9319, -1.9414,\n",
      "         -0.0777,  1.1648,  0.9319,  1.3201, -2.0190,  1.3978, -0.1553, -0.1553,\n",
      "          0.0000,  2.0967, -0.5436,  0.5436,  0.0777,  0.3883,  2.4073, -0.2330,\n",
      "         -0.5436, -0.7766, -2.7179, -0.0777,  0.5436, -2.2520, -1.3978,  0.0777,\n",
      "         -0.7766, -0.3883,  1.5531,  0.0000, -0.8542,  0.5436,  1.7084,  0.8542,\n",
      "         -0.7766, -1.1648, -2.4073, -0.5436,  0.1553,  1.5531,  2.5626, -0.5436,\n",
      "         -0.3883, -2.0967, -0.5436,  1.2425, -0.6989, -2.0967, -0.6212, -1.7861,\n",
      "          0.9319, -0.8542, -1.6308, -1.0872, -1.0095, -0.3106,  0.6212,  0.2330,\n",
      "          1.7861,  3.1839,  2.0190, -0.6212, -0.3883,  0.1553,  1.2425, -1.7861,\n",
      "         -0.3106,  0.8542, -0.0777, -0.3106, -0.8542,  0.9319, -1.7084, -0.6212,\n",
      "          0.6989, -0.3883,  0.9319, -0.6212, -0.8542, -2.4073, -2.1743, -1.1648,\n",
      "         -1.0095, -0.1553, -1.1648, -1.0095, -1.7084,  0.1553, -0.5436, -0.8542,\n",
      "         -1.9414,  1.2425, -0.1553, -0.2330, -0.1553, -1.2425, -1.9414, -0.3883,\n",
      "         -0.8542, -1.7861,  1.0095, -0.5436, -0.6989,  0.8542,  0.3883, -3.1839,\n",
      "         -1.2425, -0.7766,  1.5531,  3.1839, -0.1553,  0.6989,  0.6212, -1.3201,\n",
      "          1.0095,  0.1553, -0.7766, -2.2520, -1.7084,  2.6403, -2.1743,  0.3883,\n",
      "          1.5531, -0.6212, -0.9319, -1.3978,  5.7465,  1.1648,  0.0000,  0.9319,\n",
      "         -1.2425, -1.6308,  3.4168,  0.6989, -1.2425,  0.5436, -0.7766,  1.5531,\n",
      "          0.5436, -1.4754,  0.0000, -0.6212,  0.9319, -2.4073, -0.1553, -0.6212,\n",
      "         -1.5531,  2.3297,  1.2425, -0.2330,  1.3978, -0.3883, -1.7084, -0.9319,\n",
      "         -0.5436,  0.1553, -2.1743, -0.9319,  0.1553,  0.2330, -1.0095,  1.3201,\n",
      "         -1.1648, -0.9319, -0.5436, -0.6989, -1.1648, -1.7861, -0.5436,  2.7179,\n",
      "          1.0095,  0.9319, -0.4659, -1.7861,  1.2425,  0.1553, -0.5436,  0.2330,\n",
      "         -0.4659, -0.9319,  1.1648,  1.7084, -1.1648, -0.6212, -0.0777, -1.0872,\n",
      "          0.1553, -1.0095,  0.3106,  3.6498,  1.9414, -0.5436,  0.2330, -0.4659,\n",
      "         -1.9414, -1.0872,  0.5436, -2.0190, -0.6989,  0.8542, -1.8637, -0.9319,\n",
      "          1.6308, -0.0777, -1.3978, -0.9319,  0.6212,  0.1553,  0.0000,  0.6989,\n",
      "          0.6989,  0.5436, -0.0777, -0.6989,  2.4073, -0.9319,  1.9414, -0.3883,\n",
      "         -0.4659, -0.0777,  1.0095,  1.1648, -2.5626, -0.1553, -1.4754,  1.4754,\n",
      "         -1.0872,  1.8637, -0.0777,  0.7766, -1.3978, -2.0967,  0.4659, -0.9319,\n",
      "          1.7084, -0.6212, -0.8542, -0.6212,  0.3883,  1.3201, -0.3883, -0.5436,\n",
      "          0.1553, -0.3106, -0.4659, -1.3978, -2.5626,  1.3201, -1.1648,  0.1553,\n",
      "         -1.4754,  0.1553,  1.7861,  1.1648, -1.0872, -1.1648, -0.6212, -1.7861,\n",
      "          2.4850, -0.1553, -1.6308, -2.2520,  0.5436,  0.3883, -1.0872,  1.4754,\n",
      "          1.6308, -1.4754, -0.8542, -0.2330,  1.7084,  0.9319, -0.6989,  0.4659,\n",
      "          0.8542, -2.4850, -0.5436, -0.2330, -0.3106,  0.0777, -0.6212,  0.6989,\n",
      "          2.4850, -0.9319, -0.0777, -0.8542,  1.3201, -2.7956, -0.3883, -1.0872,\n",
      "         -1.0095,  1.9414,  0.3883, -1.4754, -1.7861,  0.9319, -2.6403,  1.7084,\n",
      "          0.7766, -0.9319, -1.2425,  0.5436, -1.0872,  1.9414, -2.6403,  2.7179,\n",
      "         -0.7766, -0.1553,  1.7084, -1.6308, -1.1648,  3.0285, -0.2330, -0.1553,\n",
      "          1.2425, -0.5436, -1.0095,  0.1553,  0.6212,  2.0967, -0.0777, -2.0967,\n",
      "          0.3883,  0.0000, -0.0777,  0.0000,  0.6989, -0.7766, -0.3106, -1.1648,\n",
      "          0.7766, -1.0872, -0.9319, -1.3978,  1.3978, -0.5436,  0.1553, -0.1553,\n",
      "         -0.5436,  0.2330, -2.7956,  0.0777,  0.0777,  0.3883, -1.8637,  2.4073,\n",
      "          0.3106, -0.3883,  0.1553,  0.1553, -0.6989, -0.2330,  2.5626,  0.6212,\n",
      "         -0.3883, -1.0095, -1.2425,  0.3106, -1.8637,  0.1553, -1.0095,  0.8542,\n",
      "          0.8542, -0.6212,  4.3487, -0.4659, -0.4659, -0.5436, -1.4754,  0.7766,\n",
      "         -2.2520, -0.1553, -0.7766, -0.2330,  0.3883, -0.5436, -0.9319,  0.9319,\n",
      "         -0.9319,  1.0095, -0.9319,  2.1743,  0.7766,  0.4659, -1.4754,  3.7274,\n",
      "          0.3883, -2.0967,  2.0190,  1.3978, -0.6989, -0.3883,  0.2330, -0.9319,\n",
      "          3.8051, -0.8542, -0.3883,  2.3297,  2.3297, -1.0095, -2.0967, -0.6989,\n",
      "         -0.6212, -1.4754,  0.2330, -0.3883, -0.1553,  0.4659, -0.6212, -0.2330,\n",
      "          0.6212, -0.1553,  0.7766,  0.6989,  1.3201,  0.3106,  2.0967, -1.0872,\n",
      "         -0.7766, -0.3106, -0.3106,  1.7084,  2.2520, -0.3883,  2.0967,  0.9319,\n",
      "         -2.9509, -0.2330,  0.5436,  1.3201, -1.0872,  1.0872,  1.0872, -2.1743,\n",
      "         -2.2520, -2.6403,  0.0777, -0.2330,  3.0285, -1.1648,  0.4659, -1.2425,\n",
      "          1.3201, -1.0872, -2.2520, -0.7766,  1.3201, -0.3883, -2.0190,  1.7084,\n",
      "          0.4659, -0.3883,  0.3883,  1.8637,  1.0095,  3.1062, -0.3883, -0.2330,\n",
      "          0.0777,  0.2330, -1.2425, -1.3978,  1.7084,  0.9319,  0.5436, -0.6212,\n",
      "          1.0872,  0.3883,  0.0000, -0.0777,  1.7861,  0.3106, -0.8542,  1.3978,\n",
      "          4.8146,  1.7084, -0.3883,  0.6989,  0.8542,  1.8637,  0.8542,  3.2615,\n",
      "          0.8542, -0.2330, -1.2425, -0.4659,  1.3201,  0.8542, -0.1553,  0.3106,\n",
      "          1.7084,  4.0381,  2.0967, -0.4659, -2.0190,  1.6308, -0.3106, -1.5531,\n",
      "         -0.8542,  0.6989,  0.2330, -0.1553, -0.8542, -0.5436, -0.6212, -0.4659,\n",
      "         -0.9319,  0.3106, -1.3978, -0.5436,  1.0872,  0.8542,  0.3883,  0.0777,\n",
      "         -1.8637, -0.6212, -0.8542,  1.9414,  1.4754,  0.2330,  2.7179,  2.4073,\n",
      "          1.6308,  0.4659,  0.6212,  0.8542, -2.1743,  0.8542,  1.1648, -1.8637,\n",
      "          0.5436, -0.0777,  0.2330,  0.3106, -0.3106, -1.3201,  1.3978,  1.5531,\n",
      "          0.6212,  0.0777,  0.0000,  2.7179,  0.0777,  3.5721,  1.9414,  2.0190,\n",
      "         -0.3883,  1.7861,  1.3978,  0.4659,  4.8146, -0.5436,  0.5436,  1.4754,\n",
      "          0.6989,  0.2330, -0.5436,  1.7861,  0.6989,  1.3978,  1.4754,  2.0967,\n",
      "          1.0872, -0.3883, -0.0777,  0.7766,  0.6989,  1.3978,  1.7084,  0.2330]])\n"
     ]
    }
   ],
   "source": [
    "print(type(modules[-1]))\n",
    "print(before_l[-1].dtype,before_l[-1])\n",
    "print(after_l[-1].dtype,after_l[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.features.0.weight\n",
      "model.features.0.bias\n",
      "model.features.0.scale\n",
      "model.features.0.zero_point\n",
      "model.features.2.weight\n",
      "model.features.2.bias\n",
      "model.features.2.scale\n",
      "model.features.2.zero_point\n",
      "model.features.5.weight\n",
      "model.features.5.bias\n",
      "model.features.5.scale\n",
      "model.features.5.zero_point\n",
      "model.features.7.weight\n",
      "model.features.7.bias\n",
      "model.features.7.scale\n",
      "model.features.7.zero_point\n",
      "model.features.10.weight\n",
      "model.features.10.bias\n",
      "model.features.10.scale\n",
      "model.features.10.zero_point\n",
      "model.features.12.weight\n",
      "model.features.12.bias\n",
      "model.features.12.scale\n",
      "model.features.12.zero_point\n",
      "model.features.14.weight\n",
      "model.features.14.bias\n",
      "model.features.14.scale\n",
      "model.features.14.zero_point\n",
      "model.features.17.weight\n",
      "model.features.17.bias\n",
      "model.features.17.scale\n",
      "model.features.17.zero_point\n",
      "model.features.19.weight\n",
      "model.features.19.bias\n",
      "model.features.19.scale\n",
      "model.features.19.zero_point\n",
      "model.features.21.weight\n",
      "model.features.21.bias\n",
      "model.features.21.scale\n",
      "model.features.21.zero_point\n",
      "model.features.24.weight\n",
      "model.features.24.bias\n",
      "model.features.24.scale\n",
      "model.features.24.zero_point\n",
      "model.features.26.weight\n",
      "model.features.26.bias\n",
      "model.features.26.scale\n",
      "model.features.26.zero_point\n",
      "model.features.28.weight\n",
      "model.features.28.bias\n",
      "model.features.28.scale\n",
      "model.features.28.zero_point\n",
      "model.classifier.0.scale\n",
      "model.classifier.0.zero_point\n",
      "model.classifier.0._packed_params.dtype\n",
      "model.classifier.0._packed_params._packed_params\n",
      "model.classifier.3.scale\n",
      "model.classifier.3.zero_point\n",
      "model.classifier.3._packed_params.dtype\n",
      "model.classifier.3._packed_params._packed_params\n",
      "model.classifier.6.scale\n",
      "model.classifier.6.zero_point\n",
      "model.classifier.6._packed_params.dtype\n",
      "model.classifier.6._packed_params._packed_params\n",
      "quant.scale\n",
      "quant.zero_point\n",
      "tensor([[[[ -23,  -75, -101],\n",
      "          [   5,  -64, -127],\n",
      "          [  24,  -50, -100]],\n",
      "\n",
      "         [[  36,  -21,  -39],\n",
      "          [  53,    6,  -14],\n",
      "          [  53,   30,   12]],\n",
      "\n",
      "         [[  54,    4,  -36],\n",
      "          [  64,   38,   -1],\n",
      "          [   6,   17,    5]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  21,   17,   -7],\n",
      "          [  19,    3,  -26],\n",
      "          [  15,   28,  -10]],\n",
      "\n",
      "         [[  17,   32,   44],\n",
      "          [  21,   28,   26],\n",
      "          [  -7,   15,   38]],\n",
      "\n",
      "         [[  17,  -16,  -76],\n",
      "          [ -44,  -54,  -59],\n",
      "          [ -29,  -19,   -3]]],\n",
      "\n",
      "\n",
      "        [[[  -9,  -55,  -95],\n",
      "          [ -26,  -58, -100],\n",
      "          [ -30,  -76, -114]],\n",
      "\n",
      "         [[  -5,   34,   11],\n",
      "          [  13,    7,  -11],\n",
      "          [   1,  -23,  -25]],\n",
      "\n",
      "         [[  10,  -10,  -31],\n",
      "          [ -13,   10,  -58],\n",
      "          [  13,    2,    6]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  28,   25,   18],\n",
      "          [  11,    2,   -8],\n",
      "          [  17,   12,  -18]],\n",
      "\n",
      "         [[ -16,    2,  -15],\n",
      "          [   0,   -5,   12],\n",
      "          [  -3,  -16,  -15]],\n",
      "\n",
      "         [[ -20,  -19,   19],\n",
      "          [ -41,  -39,   50],\n",
      "          [ -45,  -27,   29]]],\n",
      "\n",
      "\n",
      "        [[[   5,   10,   23],\n",
      "          [ -11,   -7,   12],\n",
      "          [   5,    4,   12]],\n",
      "\n",
      "         [[ -33,  -37,  -23],\n",
      "          [ -20,  -47,   -5],\n",
      "          [  15,   10,   46]],\n",
      "\n",
      "         [[ -18,   -7,   -4],\n",
      "          [  12,   19,    7],\n",
      "          [  37,   53,   30]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[   1,    6,   -7],\n",
      "          [  -3,   -5,   -1],\n",
      "          [   3,    1,  -14]],\n",
      "\n",
      "         [[  -3,   -2,    0],\n",
      "          [   0,   -3,    4],\n",
      "          [  13,    7,   14]],\n",
      "\n",
      "         [[  25,   -3,   16],\n",
      "          [  -9,  -20,   17],\n",
      "          [  10,    3,   15]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -25,  -16,    6],\n",
      "          [ -26,  -28,   10],\n",
      "          [ -25,  -14,  -14]],\n",
      "\n",
      "         [[ -11,    9,  -36],\n",
      "          [  17,   16,  -36],\n",
      "          [  -8,    1,  -35]],\n",
      "\n",
      "         [[ -16,  -12,  -29],\n",
      "          [   6,  -29,  -31],\n",
      "          [ -22,    0,   20]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  -1,    1,   -4],\n",
      "          [   1,    5,   10],\n",
      "          [   7,    4,   19]],\n",
      "\n",
      "         [[   5,   18,   22],\n",
      "          [  -1,    1,    8],\n",
      "          [  -3,    5,   16]],\n",
      "\n",
      "         [[ -17,  -38,  -42],\n",
      "          [ -20,   -5,   34],\n",
      "          [  -9,   33,   72]]],\n",
      "\n",
      "\n",
      "        [[[  22,   21,    9],\n",
      "          [  22,   16,    9],\n",
      "          [  -3,   12,   -1]],\n",
      "\n",
      "         [[  17,   -9,    6],\n",
      "          [ -32,  -25,  -15],\n",
      "          [ -17,   10,   -7]],\n",
      "\n",
      "         [[ -54,  -54,  -72],\n",
      "          [  40,   -4,  -40],\n",
      "          [  93,  100,   18]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  14,   18,    3],\n",
      "          [  -2,   11,    9],\n",
      "          [  -3,  -16,    0]],\n",
      "\n",
      "         [[   4,   -3,    3],\n",
      "          [   0,    7,   11],\n",
      "          [   1,   -7,    2]],\n",
      "\n",
      "         [[  18,    6,   17],\n",
      "          [  -5,   33,   19],\n",
      "          [ -11,    9,   -7]]],\n",
      "\n",
      "\n",
      "        [[[ -25,    4,   29],\n",
      "          [   4,  -12,   20],\n",
      "          [ -14,  -11,   24]],\n",
      "\n",
      "         [[  20,  -71,   37],\n",
      "          [ -69,    5,   92],\n",
      "          [  -2,   11,  -33]],\n",
      "\n",
      "         [[ -55,  -48,   65],\n",
      "          [ -58,   56,   31],\n",
      "          [  35,   32,  -12]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[   5,   12,   -4],\n",
      "          [  19,   -3,  -13],\n",
      "          [   5,  -11,   10]],\n",
      "\n",
      "         [[  11,   11,    2],\n",
      "          [   1,   -7,   -8],\n",
      "          [  -3,  -14,   -7]],\n",
      "\n",
      "         [[   9,   17,   25],\n",
      "          [   4,   12,  -22],\n",
      "          [  10,  -35,  -15]]]], dtype=torch.int8)\n",
      "odict_keys(['model.features.0.weight', 'model.features.0.bias', 'model.features.2.weight', 'model.features.2.bias', 'model.features.5.weight', 'model.features.5.bias', 'model.features.7.weight', 'model.features.7.bias', 'model.features.10.weight', 'model.features.10.bias', 'model.features.12.weight', 'model.features.12.bias', 'model.features.14.weight', 'model.features.14.bias', 'model.features.17.weight', 'model.features.17.bias', 'model.features.19.weight', 'model.features.19.bias', 'model.features.21.weight', 'model.features.21.bias', 'model.features.24.weight', 'model.features.24.bias', 'model.features.26.weight', 'model.features.26.bias', 'model.features.28.weight', 'model.features.28.bias', 'model.classifier.0.weight', 'model.classifier.0.bias', 'model.classifier.3.weight', 'model.classifier.3.bias', 'model.classifier.6.weight', 'model.classifier.6.bias'])\n",
      "tensor([[[[-3.0606e-02, -9.8520e-02, -1.3260e-01],\n",
      "          [ 6.8208e-03, -8.3483e-02, -1.6697e-01],\n",
      "          [ 3.1015e-02, -6.5803e-02, -1.3171e-01]],\n",
      "\n",
      "         [[ 4.7407e-02, -2.7588e-02, -5.1127e-02],\n",
      "          [ 7.0129e-02,  8.2528e-03, -1.8340e-02],\n",
      "          [ 6.9918e-02,  3.8993e-02,  1.6228e-02]],\n",
      "\n",
      "         [[ 7.0700e-02,  5.2703e-03, -4.7362e-02],\n",
      "          [ 8.4006e-02,  4.9190e-02, -1.6474e-03],\n",
      "          [ 8.5166e-03,  2.2350e-02,  5.9118e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.7666e-02,  2.1778e-02, -9.4606e-03],\n",
      "          [ 2.5511e-02,  4.1186e-03, -3.4521e-02],\n",
      "          [ 2.0150e-02,  3.7068e-02, -1.3509e-02]],\n",
      "\n",
      "         [[ 2.1684e-02,  4.1812e-02,  5.8284e-02],\n",
      "          [ 2.7431e-02,  3.6847e-02,  3.4335e-02],\n",
      "          [-9.4839e-03,  1.9745e-02,  5.0264e-02]],\n",
      "\n",
      "         [[ 2.1769e-02, -2.1388e-02, -9.9363e-02],\n",
      "          [-5.7156e-02, -7.1328e-02, -7.7600e-02],\n",
      "          [-3.7508e-02, -2.5453e-02, -4.5096e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.3319e-02, -7.7979e-02, -1.3496e-01],\n",
      "          [-3.7411e-02, -8.1807e-02, -1.4195e-01],\n",
      "          [-4.1913e-02, -1.0756e-01, -1.6164e-01]],\n",
      "\n",
      "         [[-6.8725e-03,  4.8598e-02,  1.5008e-02],\n",
      "          [ 1.8636e-02,  9.8393e-03, -1.5973e-02],\n",
      "          [ 9.5164e-04, -3.2665e-02, -3.5824e-02]],\n",
      "\n",
      "         [[ 1.4780e-02, -1.4260e-02, -4.4468e-02],\n",
      "          [-1.8438e-02,  1.4841e-02, -8.2337e-02],\n",
      "          [ 1.8329e-02,  2.1435e-03,  9.0911e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.0342e-02,  3.6146e-02,  2.5515e-02],\n",
      "          [ 1.5779e-02,  3.1012e-03, -1.0942e-02],\n",
      "          [ 2.3790e-02,  1.6440e-02, -2.5835e-02]],\n",
      "\n",
      "         [[-2.2844e-02,  2.5371e-03, -2.1714e-02],\n",
      "          [ 3.9534e-04, -6.4903e-03,  1.6979e-02],\n",
      "          [-4.7200e-03, -2.2301e-02, -2.1298e-02]],\n",
      "\n",
      "         [[-2.8434e-02, -2.6771e-02,  2.7432e-02],\n",
      "          [-5.8088e-02, -5.5869e-02,  7.0289e-02],\n",
      "          [-6.4470e-02, -3.8172e-02,  4.1569e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2508e-02,  2.4738e-02,  5.3893e-02],\n",
      "          [-2.5838e-02, -1.6176e-02,  2.8344e-02],\n",
      "          [ 1.2948e-02,  9.0717e-03,  2.8181e-02]],\n",
      "\n",
      "         [[-7.9309e-02, -8.8828e-02, -5.5737e-02],\n",
      "          [-4.8359e-02, -1.1139e-01, -1.0901e-02],\n",
      "          [ 3.4640e-02,  2.3298e-02,  1.1002e-01]],\n",
      "\n",
      "         [[-4.3616e-02, -1.6598e-02, -1.0547e-02],\n",
      "          [ 2.8982e-02,  4.5279e-02,  1.6869e-02],\n",
      "          [ 8.8168e-02,  1.2599e-01,  7.2292e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3967e-03,  1.5333e-02, -1.7196e-02],\n",
      "          [-8.3107e-03, -1.2905e-02, -1.4394e-03],\n",
      "          [ 6.3471e-03,  1.7825e-03, -3.3770e-02]],\n",
      "\n",
      "         [[-7.9354e-03, -5.8610e-03, -7.6292e-05],\n",
      "          [ 6.8661e-04, -6.0019e-03,  1.0119e-02],\n",
      "          [ 3.0581e-02,  1.6821e-02,  3.2570e-02]],\n",
      "\n",
      "         [[ 5.9351e-02, -7.4398e-03,  3.8274e-02],\n",
      "          [-2.0792e-02, -4.8833e-02,  4.0274e-02],\n",
      "          [ 2.3138e-02,  7.4794e-03,  3.5027e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.5464e-02, -2.9275e-02,  1.0282e-02],\n",
      "          [-4.6952e-02, -4.9473e-02,  1.7632e-02],\n",
      "          [-4.3815e-02, -2.4871e-02, -2.4908e-02]],\n",
      "\n",
      "         [[-1.9359e-02,  1.5948e-02, -6.3554e-02],\n",
      "          [ 3.0096e-02,  2.8515e-02, -6.4439e-02],\n",
      "          [-1.4547e-02,  2.2238e-03, -6.3371e-02]],\n",
      "\n",
      "         [[-2.7915e-02, -2.1738e-02, -5.1691e-02],\n",
      "          [ 1.0889e-02, -5.1994e-02, -5.4649e-02],\n",
      "          [-3.9741e-02,  5.2832e-04,  3.5375e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9130e-03,  1.0995e-03, -7.4082e-03],\n",
      "          [ 1.8428e-03,  8.2602e-03,  1.8680e-02],\n",
      "          [ 1.1709e-02,  6.9263e-03,  3.3630e-02]],\n",
      "\n",
      "         [[ 8.9195e-03,  3.2807e-02,  3.9282e-02],\n",
      "          [-2.5044e-03,  1.7645e-03,  1.4056e-02],\n",
      "          [-5.4708e-03,  9.5572e-03,  2.8639e-02]],\n",
      "\n",
      "         [[-3.0241e-02, -6.7225e-02, -7.4299e-02],\n",
      "          [-3.6222e-02, -9.7721e-03,  6.1669e-02],\n",
      "          [-1.6392e-02,  5.8156e-02,  1.2894e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.0321e-02,  3.7677e-02,  1.7181e-02],\n",
      "          [ 4.0298e-02,  2.9479e-02,  1.6698e-02],\n",
      "          [-4.8739e-03,  2.1128e-02, -2.6295e-03]],\n",
      "\n",
      "         [[ 3.1079e-02, -1.6475e-02,  1.0769e-02],\n",
      "          [-5.8626e-02, -4.6721e-02, -2.7764e-02],\n",
      "          [-3.1245e-02,  1.7635e-02, -1.2211e-02]],\n",
      "\n",
      "         [[-9.8675e-02, -9.8915e-02, -1.3225e-01],\n",
      "          [ 7.3214e-02, -7.5238e-03, -7.4033e-02],\n",
      "          [ 1.7070e-01,  1.8346e-01,  3.3906e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.5318e-02,  3.2680e-02,  5.2874e-03],\n",
      "          [-4.1894e-03,  2.0011e-02,  1.6021e-02],\n",
      "          [-4.8276e-03, -2.9130e-02,  7.4975e-04]],\n",
      "\n",
      "         [[ 7.1959e-03, -5.1528e-03,  5.1333e-03],\n",
      "          [-8.6466e-04,  1.2417e-02,  2.0805e-02],\n",
      "          [ 2.4862e-03, -1.3194e-02,  3.6563e-03]],\n",
      "\n",
      "         [[ 3.2415e-02,  1.0823e-02,  3.1720e-02],\n",
      "          [-8.4911e-03,  5.9831e-02,  3.4606e-02],\n",
      "          [-2.0674e-02,  1.6525e-02, -1.3183e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.6955e-02,  1.0821e-02,  7.9260e-02],\n",
      "          [ 1.0361e-02, -3.3380e-02,  5.2775e-02],\n",
      "          [-3.8372e-02, -3.0964e-02,  6.5422e-02]],\n",
      "\n",
      "         [[ 5.4925e-02, -1.9160e-01,  9.9709e-02],\n",
      "          [-1.8754e-01,  1.3248e-02,  2.4748e-01],\n",
      "          [-5.6662e-03,  2.8737e-02, -8.9787e-02]],\n",
      "\n",
      "         [[-1.4905e-01, -1.2974e-01,  1.7649e-01],\n",
      "          [-1.5774e-01,  1.5130e-01,  8.4126e-02],\n",
      "          [ 9.4988e-02,  8.5186e-02, -3.3635e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2278e-02,  3.3348e-02, -1.0524e-02],\n",
      "          [ 5.0310e-02, -9.1280e-03, -3.3890e-02],\n",
      "          [ 1.3459e-02, -2.9601e-02,  2.6862e-02]],\n",
      "\n",
      "         [[ 3.1006e-02,  2.8481e-02,  5.2361e-03],\n",
      "          [ 3.8914e-03, -1.8899e-02, -2.1819e-02],\n",
      "          [-8.4505e-03, -3.6632e-02, -1.9476e-02]],\n",
      "\n",
      "         [[ 2.3835e-02,  4.5658e-02,  6.7297e-02],\n",
      "          [ 1.1103e-02,  3.2035e-02, -5.8449e-02],\n",
      "          [ 2.6805e-02, -9.3975e-02, -4.0504e-02]]]])\n"
     ]
    }
   ],
   "source": [
    "for child in model_static_quantized.children():\n",
    "    if isinstance(child, nn.Sequential) or isinstance(child, torchvision.models.vgg.VGG):\n",
    "        for n,c in child.named_children():\n",
    "            for name, param in c.named_parameters():\n",
    "                print(name)\n",
    "\n",
    "state = model.state_dict()\n",
    "for names in model_static_quantized.state_dict():\n",
    "    print(names)\n",
    "print(model_static_quantized.model.features[2].weight().int_repr())\n",
    "print(state.keys())\n",
    "print(state['model.features.2.weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
