{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import _log_api_usage_once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def manual_seed(seed):\n",
    "    np.random.seed(seed) #1\n",
    "    random.seed(seed) #2\n",
    "    torch.manual_seed(seed) #3\n",
    "    torch.cuda.manual_seed(seed) #4.1\n",
    "    torch.cuda.manual_seed_all(seed) #4.2\n",
    "    torch.backends.cudnn.benchmark = False #5 \n",
    "    torch.backends.cudnn.deterministic = True #6\n",
    "\n",
    "manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantized(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        self.model = model\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    def fuse_model(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# filepath = './checkpoint/vgg16.pth'\n",
    "# checkpoint = torch.load(filepath)\n",
    "# model = checkpoint['model']\n",
    "# print(model)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = torchvision.models.vgg.vgg16(pretrained=True)\n",
    "model = Quantized(model)\n",
    "pretrain_transforms = torchvision.models.VGG16_Weights.DEFAULT.transforms()\n",
    "print(pretrain_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    pretrain_transforms,\n",
    "    ])\n",
    "\n",
    "val_data = torchvision.datasets.ImageNet(root=\"./dataset/ImageNet\", split=\"val\", transform=test_transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=32,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def memory_usage(message = \"debug\"):\n",
    "    p = psutil.Process()\n",
    "    rss = p.memory_info().rss / 2**20\n",
    "    print(f\"{message} memory usage : {rss:10.5f} MB\")\n",
    "\n",
    "def test(model, test_loader, num_calib=None):\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    test_acc = 0\n",
    "    with torch.inference_mode():\n",
    "        for i,data in enumerate(tqdm(test_loader,leave=True)):\n",
    "            imgs, target = data[0], data[1]\n",
    "            # imgs, target = data[0].to(device), data[1].to(device)\n",
    "            output = model(imgs)\n",
    "            _, preds = torch.max(output.data, 1)\n",
    "            test_acc += (preds==target).detach().sum().item()\n",
    "            if num_calib and (i > num_calib):\n",
    "                break\n",
    "            # if i % 10 == 0:\n",
    "            #     memory_usage()\n",
    "\n",
    "    test_acc = 100. * test_acc/len(test_loader.dataset)\n",
    "    \n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = []\n",
    "before_l = []\n",
    "after_l = []\n",
    "hooks = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    modules.append(module)\n",
    "    before_l.append(input[0])\n",
    "    after_l.append(output)\n",
    "\n",
    "def add_forward_hook(net, hooks):\n",
    "    for name, layer in net._modules.items():\n",
    "        if isinstance(layer, nn.Sequential) or isinstance(layer, torchvision.models.vgg.VGG):\n",
    "            add_forward_hook(layer, hooks)\n",
    "        else:\n",
    "            hook = layer.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "            \n",
    "    return hooks\n",
    "\n",
    "def remove_forward_hook(hooks):\n",
    "    for i in hooks:\n",
    "        i.remove()\n",
    "# out = model((torch.randn(1,3,32,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defualt qconfig ; QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "x86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1563 [00:23<54:29,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41 41 41\n",
      "Quantized(\n",
      "  (model): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.17084623873233795, zero_point=58, padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.35520821809768677, zero_point=73, padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.5221890211105347, zero_point=82, padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.7225201725959778, zero_point=71, padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.9027827978134155, zero_point=74, padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.046532154083252, zero_point=71, padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.045250415802002, zero_point=72, padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.2762900590896606, zero_point=75, padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0824809074401855, zero_point=80, padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.887078046798706, zero_point=83, padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.9002187848091125, zero_point=65, padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.7038030624389648, zero_point=81, padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.6258416175842285, zero_point=75, padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): QuantizedLinear(in_features=25088, out_features=4096, scale=0.42471253871917725, zero_point=77, qscheme=torch.per_channel_affine)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): QuantizedDropout(p=0.5, inplace=False)\n",
      "      (3): QuantizedLinear(in_features=4096, out_features=4096, scale=0.24681514501571655, zero_point=79, qscheme=torch.per_channel_affine)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): QuantizedDropout(p=0.5, inplace=False)\n",
      "      (6): QuantizedLinear(in_features=4096, out_features=1000, scale=0.42873188853263855, zero_point=32, qscheme=torch.per_channel_affine)\n",
      "    )\n",
      "  )\n",
      "  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()\n",
    "model.fuse_model()\n",
    "backend = \"fbgemm\"\n",
    "qconfig = torch.ao.quantization.get_default_qconfig(backend)\n",
    "model.qconfig = qconfig\n",
    "print(f\"defualt qconfig ; {model.qconfig}\")\n",
    "torch.backends.quantized.engine = backend\n",
    "print(torch.backends.quantized.engine)\n",
    "model_static_quantized = torch.ao.quantization.prepare(model, inplace = False)\n",
    "# calibration\n",
    "test(model_static_quantized,val_loader,10)\n",
    "\n",
    "# make quantized model\n",
    "torch.ao.quantization.convert(model_static_quantized, inplace = True) \n",
    "\n",
    "# make hook\n",
    "hooks = add_forward_hook(model_static_quantized, hooks)\n",
    "sample = torch.randn(1,3,256,256)\n",
    "model_static_quantized(sample)\n",
    "print(len(hooks), len(modules), len(before_l), len(after_l))\n",
    "# remove hook, hook works at once\n",
    "remove_forward_hook(hooks)\n",
    "# for _ in range(5):\n",
    "#     sample = (torch.randn(1,3,224,224))\n",
    "#     model_static_quantized(sample)\n",
    "# print(len(modules), len(before_l), len(after_l))\n",
    "print(model_static_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [12:21<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test result : 71.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_acc = test(model_static_quantized, val_loader)\n",
    "print(f\"test result : {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.jit.save(torch.jit.script(model_static_quantized),\"./checkpoint/static_quant_fbegmm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.ao.nn.quantized.modules.Quantize'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.dropout.Dropout'>\n",
      "<class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.dropout.Dropout'>\n",
      "<class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "<class 'torch.ao.nn.quantized.modules.DeQuantize'>\n",
      "41 41 41\n",
      "<class 'torch.ao.nn.quantized.modules.Quantize'>\n",
      "before : <class 'torch.Tensor'>, torch.float32\n",
      "after : <class 'torch.Tensor'>, torch.quint8\n",
      "tensor([[[[ 1.2731,  0.0000, -1.2731,  ..., -1.3480, -0.7864,  2.6586],\n",
      "          [ 2.8084,  0.0749, -2.1344,  ..., -0.7115,  1.2357, -0.9361],\n",
      "          [-0.1123, -0.1498,  1.2357,  ..., -0.4868,  0.9361,  0.7115],\n",
      "          ...,\n",
      "          [-0.0374,  1.2357, -1.3855,  ...,  0.4868,  0.3370, -0.9361],\n",
      "          [ 0.1498,  0.1498, -0.9361,  ..., -1.2731, -0.2996,  0.0749],\n",
      "          [-1.7599, -0.1498,  1.9097,  ..., -0.7489,  0.1498,  0.2621]],\n",
      "\n",
      "         [[ 0.1498, -1.5353,  0.2621,  ...,  2.0969, -0.9361, -0.7489],\n",
      "          [ 0.4119, -1.1234,  0.4868,  ..., -1.1234, -0.4868, -1.1983],\n",
      "          [-0.9361, -2.1344,  1.7225,  ..., -0.5617, -0.3745,  0.6740],\n",
      "          ...,\n",
      "          [-0.7115,  0.8987,  0.2247,  ...,  0.9736,  1.9097,  0.0374],\n",
      "          [-1.3480,  2.0595,  0.5991,  ..., -0.4119, -1.3106, -0.9361],\n",
      "          [-2.1344, -1.8348,  0.1872,  ...,  0.0374, -0.4493,  0.5617]],\n",
      "\n",
      "         [[-1.3855,  0.1498,  0.5617,  ...,  0.8987, -1.0859, -1.0859],\n",
      "          [-0.4493, -0.1872, -0.8987,  ..., -0.9736, -0.0749, -2.0221],\n",
      "          [-0.8612,  1.8348,  2.9582,  ..., -0.1123,  1.7225,  0.1498],\n",
      "          ...,\n",
      "          [ 0.0000, -0.0749, -1.7974,  ...,  0.2247, -0.2247, -0.4119],\n",
      "          [ 0.6740, -0.9736, -0.9361,  ...,  0.2621, -0.7864, -0.5991],\n",
      "          [-0.0374,  0.5242, -0.9361,  ..., -1.0485,  0.5617, -0.4868]]]],\n",
      "       size=(1, 3, 256, 256), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.037445519119501114,\n",
      "       zero_point=57)\n",
      "tensor([[[[1.3668, 0.0000, 0.0000,  ..., 0.0000, 4.2712, 0.6834],\n",
      "          [2.2210, 0.0000, 0.0000,  ..., 0.5125, 5.1254, 0.3417],\n",
      "          [1.0251, 0.0000, 1.0251,  ..., 2.9044, 0.8542, 0.0000],\n",
      "          ...,\n",
      "          [2.2210, 0.0000, 0.0000,  ..., 2.0502, 0.6834, 0.0000],\n",
      "          [1.7085, 2.7335, 0.6834,  ..., 1.0251, 1.7085, 0.3417],\n",
      "          [0.1708, 3.2461, 1.8793,  ..., 0.1708, 1.8793, 0.5125]],\n",
      "\n",
      "         [[0.0000, 0.0000, 1.7085,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 2.2210, 6.4922,  ..., 1.1959, 2.2210, 2.9044],\n",
      "          [0.0000, 4.1003, 0.0000,  ..., 3.4169, 3.0752, 0.6834],\n",
      "          ...,\n",
      "          [2.0502, 0.5125, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.3417, 1.1959,  ..., 0.0000, 0.8542, 2.3918],\n",
      "          [1.1959, 3.7586, 2.2210,  ..., 0.5125, 1.5376, 1.0251]],\n",
      "\n",
      "         [[0.0000, 0.8542, 2.2210,  ..., 0.0000, 1.5376, 1.1959],\n",
      "          [0.0000, 0.5125, 2.2210,  ..., 5.2962, 0.0000, 2.5627],\n",
      "          [3.0752, 3.0752, 0.0000,  ..., 2.0502, 0.3417, 0.0000],\n",
      "          ...,\n",
      "          [4.2712, 0.0000, 1.1959,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [1.7085, 2.3918, 0.0000,  ..., 1.5376, 5.1254, 2.2210],\n",
      "          [3.2461, 5.6379, 0.3417,  ..., 1.3668, 0.0000, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.5125, 0.5125, 0.0000,  ..., 0.3417, 0.0000, 0.8542],\n",
      "          [1.3668, 1.8793, 0.0000,  ..., 0.0000, 0.1708, 1.0251],\n",
      "          [0.5125, 0.0000, 0.0000,  ..., 0.0000, 0.6834, 0.6834],\n",
      "          ...,\n",
      "          [0.0000, 0.3417, 0.0000,  ..., 0.0000, 0.1708, 0.0000],\n",
      "          [0.0000, 0.3417, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.5125,  ..., 0.0000, 0.0000, 0.6834]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1708, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3417, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.3668, 2.2210, 0.0000,  ..., 3.2461, 1.8793, 0.8542],\n",
      "          [0.5125, 0.0000, 0.0000,  ..., 0.1708, 0.0000, 0.0000],\n",
      "          [0.6834, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.8542, 2.3918,  ..., 0.3417, 2.3918, 1.3668],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 1.8793, 0.6834, 0.0000],\n",
      "          [0.1708, 0.0000, 0.0000,  ..., 1.0251, 0.0000, 0.0000]]]],\n",
      "       size=(1, 64, 256, 256), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.17084623873233795,\n",
      "       zero_point=58)\n"
     ]
    }
   ],
   "source": [
    "for i in modules:\n",
    "    print(type(i))\n",
    "print(len(modules), len(before_l), len(after_l))\n",
    "for i in range(len(modules)):\n",
    "    if not isinstance(type(modules[i]), torch.quantization.QuantStub) and not isinstance(type(modules[i]), torch.quantization.DeQuantStub):\n",
    "        print(type(modules[i]))\n",
    "        print(f\"before : {type(before_l[i])}, {before_l[i].dtype}\")\n",
    "        print(f\"after : {type(after_l[i])}, {after_l[i].dtype}\")\n",
    "        break\n",
    "\n",
    "print(before_l[1])\n",
    "print(after_l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "torch.quint8 tensor([[[[66, 58, 58,  ..., 58, 83, 62],\n",
      "          [71, 58, 58,  ..., 61, 88, 60],\n",
      "          [64, 58, 64,  ..., 75, 63, 58],\n",
      "          ...,\n",
      "          [71, 58, 58,  ..., 70, 62, 58],\n",
      "          [68, 74, 62,  ..., 64, 68, 60],\n",
      "          [59, 77, 69,  ..., 59, 69, 61]],\n",
      "\n",
      "         [[58, 58, 68,  ..., 58, 58, 58],\n",
      "          [58, 71, 96,  ..., 65, 71, 75],\n",
      "          [58, 82, 58,  ..., 78, 76, 62],\n",
      "          ...,\n",
      "          [70, 61, 58,  ..., 58, 58, 58],\n",
      "          [58, 60, 65,  ..., 58, 63, 72],\n",
      "          [65, 80, 71,  ..., 61, 67, 64]],\n",
      "\n",
      "         [[58, 63, 71,  ..., 58, 67, 65],\n",
      "          [58, 61, 71,  ..., 89, 58, 73],\n",
      "          [76, 76, 58,  ..., 70, 60, 58],\n",
      "          ...,\n",
      "          [83, 58, 65,  ..., 58, 58, 58],\n",
      "          [68, 72, 58,  ..., 67, 88, 71],\n",
      "          [77, 91, 60,  ..., 66, 58, 58]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[61, 61, 58,  ..., 60, 58, 63],\n",
      "          [66, 69, 58,  ..., 58, 59, 64],\n",
      "          [61, 58, 58,  ..., 58, 62, 62],\n",
      "          ...,\n",
      "          [58, 60, 58,  ..., 58, 59, 58],\n",
      "          [58, 60, 58,  ..., 58, 58, 58],\n",
      "          [58, 58, 61,  ..., 58, 58, 62]],\n",
      "\n",
      "         [[58, 58, 58,  ..., 58, 58, 58],\n",
      "          [58, 58, 58,  ..., 58, 58, 58],\n",
      "          [58, 58, 58,  ..., 58, 58, 58],\n",
      "          ...,\n",
      "          [58, 58, 58,  ..., 58, 58, 58],\n",
      "          [59, 58, 58,  ..., 58, 58, 58],\n",
      "          [60, 58, 58,  ..., 58, 58, 58]],\n",
      "\n",
      "         [[66, 71, 58,  ..., 77, 69, 63],\n",
      "          [61, 58, 58,  ..., 59, 58, 58],\n",
      "          [62, 58, 58,  ..., 58, 58, 58],\n",
      "          ...,\n",
      "          [58, 63, 72,  ..., 60, 72, 66],\n",
      "          [58, 58, 58,  ..., 69, 62, 58],\n",
      "          [59, 58, 58,  ..., 64, 58, 58]]]], dtype=torch.uint8)\n",
      "torch.quint8 tensor([[[[1.3668, 0.0000, 0.0000,  ..., 0.0000, 4.2712, 0.6834],\n",
      "          [2.2210, 0.0000, 0.0000,  ..., 0.5125, 5.1254, 0.3417],\n",
      "          [1.0251, 0.0000, 1.0251,  ..., 2.9044, 0.8542, 0.0000],\n",
      "          ...,\n",
      "          [2.2210, 0.0000, 0.0000,  ..., 2.0502, 0.6834, 0.0000],\n",
      "          [1.7085, 2.7335, 0.6834,  ..., 1.0251, 1.7085, 0.3417],\n",
      "          [0.1708, 3.2461, 1.8793,  ..., 0.1708, 1.8793, 0.5125]],\n",
      "\n",
      "         [[0.0000, 0.0000, 1.7085,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 2.2210, 6.4922,  ..., 1.1959, 2.2210, 2.9044],\n",
      "          [0.0000, 4.1003, 0.0000,  ..., 3.4169, 3.0752, 0.6834],\n",
      "          ...,\n",
      "          [2.0502, 0.5125, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.3417, 1.1959,  ..., 0.0000, 0.8542, 2.3918],\n",
      "          [1.1959, 3.7586, 2.2210,  ..., 0.5125, 1.5376, 1.0251]],\n",
      "\n",
      "         [[0.0000, 0.8542, 2.2210,  ..., 0.0000, 1.5376, 1.1959],\n",
      "          [0.0000, 0.5125, 2.2210,  ..., 5.2962, 0.0000, 2.5627],\n",
      "          [3.0752, 3.0752, 0.0000,  ..., 2.0502, 0.3417, 0.0000],\n",
      "          ...,\n",
      "          [4.2712, 0.0000, 1.1959,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [1.7085, 2.3918, 0.0000,  ..., 1.5376, 5.1254, 2.2210],\n",
      "          [3.2461, 5.6379, 0.3417,  ..., 1.3668, 0.0000, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.5125, 0.5125, 0.0000,  ..., 0.3417, 0.0000, 0.8542],\n",
      "          [1.3668, 1.8793, 0.0000,  ..., 0.0000, 0.1708, 1.0251],\n",
      "          [0.5125, 0.0000, 0.0000,  ..., 0.0000, 0.6834, 0.6834],\n",
      "          ...,\n",
      "          [0.0000, 0.3417, 0.0000,  ..., 0.0000, 0.1708, 0.0000],\n",
      "          [0.0000, 0.3417, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.5125,  ..., 0.0000, 0.0000, 0.6834]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1708, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3417, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.3668, 2.2210, 0.0000,  ..., 3.2461, 1.8793, 0.8542],\n",
      "          [0.5125, 0.0000, 0.0000,  ..., 0.1708, 0.0000, 0.0000],\n",
      "          [0.6834, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.8542, 2.3918,  ..., 0.3417, 2.3918, 1.3668],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 1.8793, 0.6834, 0.0000],\n",
      "          [0.1708, 0.0000, 0.0000,  ..., 1.0251, 0.0000, 0.0000]]]],\n",
      "       size=(1, 64, 256, 256), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.17084623873233795,\n",
      "       zero_point=58)\n"
     ]
    }
   ],
   "source": [
    "print(type(modules[2]))\n",
    "print(before_l[2].dtype,before_l[2].int_repr())\n",
    "print(after_l[2].dtype,after_l[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.ao.nn.quantized.modules.DeQuantize'>\n",
      "torch.quint8 tensor([[ 0.4287,  3.0011, -0.8575, -0.8575,  0.4287,  3.8586,  2.5724,  0.4287,\n",
      "          0.4287, -0.4287,  0.8575,  1.7149,  0.8575,  0.8575,  0.4287,  1.2862,\n",
      "         -1.2862, -1.7149,  0.4287, -0.8575, -0.8575, -1.2862,  0.0000, -0.4287,\n",
      "         -1.2862, -0.8575,  1.2862,  0.4287,  0.0000,  0.8575, -0.8575, -1.2862,\n",
      "         -1.2862,  0.4287,  0.4287,  0.0000,  0.4287, -0.8575,  0.0000, -1.2862,\n",
      "         -0.4287, -1.2862, -0.8575, -0.8575, -1.7149,  0.4287, -0.4287, -1.2862,\n",
      "         -1.2862, -0.4287,  0.0000, -1.7149,  0.0000, -0.4287, -0.8575, -1.2862,\n",
      "         -1.7149, -1.7149,  0.8575, -1.2862, -0.4287, -2.1437, -1.7149, -0.4287,\n",
      "         -0.8575,  1.7149, -0.8575, -1.2862, -2.1437,  0.0000,  1.7149,  0.8575,\n",
      "          0.8575,  2.1437,  2.1437,  2.1437,  0.8575,  2.1437,  3.4299,  2.5724,\n",
      "          0.0000, -0.4287, -0.4287,  0.0000, -0.4287,  0.0000,  1.2862, -1.2862,\n",
      "          0.8575, -0.4287, -0.4287, -0.4287,  1.7149, -0.4287, -0.4287, -1.7149,\n",
      "          1.2862, -1.2862, -1.2862,  0.4287, -0.4287, -0.4287, -1.7149,  0.4287,\n",
      "         -0.4287, -2.1437, -0.8575,  5.1448,  3.4299,  3.0011,  3.4299,  4.2873,\n",
      "          1.2862,  0.8575,  0.8575,  3.4299,  1.7149,  1.7149,  2.5724,  0.8575,\n",
      "          0.4287,  0.8575,  0.0000,  0.4287,  0.8575,  0.0000,  2.5724,  0.0000,\n",
      "          0.0000,  0.4287,  0.8575, -0.8575,  0.8575,  0.4287,  1.2862, -0.4287,\n",
      "         -1.2862, -0.4287,  0.4287, -0.8575, -0.8575, -0.8575, -0.4287,  0.0000,\n",
      "         -1.2862,  0.4287, -1.7149, -1.2862, -1.2862,  0.0000,  0.4287,  0.4287,\n",
      "         -1.2862, -0.4287, -0.8575, -0.4287, -0.4287, -0.4287, -0.4287,  0.0000,\n",
      "         -0.8575, -0.4287,  0.4287,  0.8575, -1.2862, -0.4287, -0.8575, -0.8575,\n",
      "          0.0000, -0.8575, -0.8575, -0.8575, -1.2862, -0.4287, -2.1437, -0.4287,\n",
      "         -1.7149, -1.2862, -0.4287, -0.8575, -1.2862, -0.8575, -0.4287, -0.8575,\n",
      "         -0.4287, -0.4287, -0.4287, -0.4287, -0.8575, -0.4287, -1.2862,  0.8575,\n",
      "         -1.2862,  0.8575, -0.4287, -0.8575, -0.8575, -1.2862, -1.7149, -0.4287,\n",
      "         -0.8575, -0.4287, -0.4287,  0.8575,  0.0000, -1.7149, -1.2862,  0.4287,\n",
      "         -0.4287, -0.8575, -2.5724,  0.8575, -0.4287,  0.8575,  0.0000,  0.0000,\n",
      "         -0.4287, -0.8575, -0.4287,  0.0000,  0.0000,  0.0000, -0.8575, -0.8575,\n",
      "         -0.8575, -1.2862, -1.2862, -0.4287, -0.8575,  0.4287,  0.4287,  0.4287,\n",
      "         -1.2862, -1.2862,  0.4287, -0.8575,  0.0000,  0.0000, -0.4287, -0.8575,\n",
      "          0.0000, -0.4287, -0.4287, -0.4287, -0.8575, -0.8575, -1.7149, -0.8575,\n",
      "         -1.2862, -0.8575, -0.4287, -1.2862, -0.8575,  0.0000,  0.0000, -1.2862,\n",
      "         -1.2862, -0.4287,  0.0000,  0.8575,  0.8575, -0.8575, -1.2862,  0.4287,\n",
      "         -0.8575,  0.0000, -1.2862, -0.8575, -2.5724, -0.4287,  0.4287,  0.8575,\n",
      "          0.4287,  0.8575,  1.2862,  0.0000, -0.4287,  1.2862,  0.0000,  0.8575,\n",
      "          0.4287, -0.8575,  0.4287,  0.0000,  0.0000, -1.2862,  0.0000, -1.7149,\n",
      "         -0.8575, -1.7149, -1.2862,  0.0000,  0.0000,  0.0000,  0.4287,  0.0000,\n",
      "          1.2862, -0.4287,  0.4287,  0.0000, -1.2862,  1.7149,  0.8575,  0.0000,\n",
      "          0.4287, -0.8575,  0.0000,  0.8575,  0.4287,  1.2862,  4.2873, -0.8575,\n",
      "         -0.4287,  1.2862,  2.5724, -0.4287, -0.8575, -0.8575,  1.2862,  0.4287,\n",
      "         -0.4287, -2.1437, -0.8575,  0.0000, -1.2862,  0.0000, -1.7149,  2.1437,\n",
      "          2.5724,  3.0011,  0.0000,  1.2862,  0.0000,  0.4287,  0.8575,  0.0000,\n",
      "         -0.8575,  0.8575, -0.4287, -0.8575, -0.4287,  0.0000, -0.4287, -1.7149,\n",
      "          0.4287,  0.4287, -0.4287,  0.4287, -1.2862, -1.2862, -0.4287,  0.8575,\n",
      "          1.2862,  1.2862,  0.0000, -0.8575,  0.0000, -0.4287, -0.4287, -0.4287,\n",
      "         -0.4287, -0.4287,  0.4287, -0.8575, -2.1437,  0.8575, -1.2862, -0.4287,\n",
      "          0.0000, -1.2862,  0.4287,  0.4287,  0.0000, -0.4287,  1.2862, -0.8575,\n",
      "          0.8575,  0.8575,  0.4287,  0.4287,  0.8575,  0.8575,  0.8575, -0.4287,\n",
      "         -0.4287, -0.4287, -0.8575, -0.4287, -1.2862,  0.0000,  0.8575,  0.8575,\n",
      "          1.2862,  0.8575, -0.4287,  0.4287,  0.8575,  0.4287,  0.0000,  1.2862,\n",
      "         -0.8575, -0.4287,  0.0000, -2.1437, -1.2862,  0.4287,  0.4287, -1.7149,\n",
      "         -3.4299,  1.7149,  0.8575,  1.2862,  0.4287, -0.8575, -0.4287, -0.4287,\n",
      "          0.4287,  2.1437,  0.0000, -0.4287,  0.0000,  0.4287, -0.4287, -1.2862,\n",
      "          0.0000,  0.4287,  0.4287,  1.2862,  0.4287,  1.2862,  0.4287, -1.2862,\n",
      "          0.0000,  1.2862,  0.8575,  2.1437, -2.1437,  0.4287,  0.0000, -0.8575,\n",
      "          0.0000,  1.7149, -0.8575,  1.2862,  0.0000,  0.8575,  2.5724, -0.4287,\n",
      "         -0.8575, -1.2862, -2.5724,  0.0000,  0.4287, -2.1437, -1.2862,  0.4287,\n",
      "         -0.4287, -0.4287,  1.7149,  0.8575, -1.7149,  0.8575,  1.2862,  0.8575,\n",
      "         -1.2862, -1.2862, -2.1437, -0.4287, -0.4287,  1.7149,  3.0011, -0.4287,\n",
      "         -0.8575, -2.1437, -0.4287,  0.8575, -0.4287, -2.1437, -0.8575, -2.1437,\n",
      "          1.2862, -0.8575, -1.7149, -1.7149, -1.7149,  0.0000,  0.4287,  0.0000,\n",
      "          1.7149,  2.5724,  1.7149, -0.8575, -0.8575,  0.4287,  1.7149, -1.2862,\n",
      "         -0.4287,  0.4287, -0.4287, -0.4287, -1.2862,  0.4287, -1.7149, -0.8575,\n",
      "          0.8575, -0.4287,  1.2862, -0.4287, -1.2862, -1.7149, -2.1437, -1.2862,\n",
      "         -1.2862,  0.0000, -1.2862, -0.8575, -1.2862,  0.0000, -0.4287, -0.8575,\n",
      "         -1.2862,  1.7149,  0.4287,  0.0000,  0.0000, -1.2862, -2.1437, -0.8575,\n",
      "         -0.4287, -1.2862,  0.8575, -0.4287, -0.4287,  0.4287,  0.0000, -3.0011,\n",
      "         -1.7149, -0.8575,  1.7149,  2.5724, -0.4287,  0.8575,  0.8575, -0.8575,\n",
      "          1.2862,  0.4287, -0.8575, -2.5724, -1.7149,  3.0011, -2.1437,  0.8575,\n",
      "          1.2862, -0.4287, -0.8575, -1.2862,  5.1448,  0.8575,  0.0000,  0.4287,\n",
      "         -1.2862, -1.7149,  3.0011,  0.0000, -1.2862,  0.4287, -0.4287,  2.1437,\n",
      "          0.0000, -1.7149,  0.4287, -1.2862,  0.8575, -2.1437,  0.4287, -0.8575,\n",
      "         -1.7149,  3.0011,  1.7149, -0.4287,  1.2862, -0.4287, -1.2862, -1.2862,\n",
      "         -0.4287,  0.0000, -2.1437, -1.2862,  0.0000,  0.4287, -0.8575,  1.2862,\n",
      "         -1.2862, -1.2862, -0.4287, -0.8575, -1.2862, -2.1437, -0.4287,  3.0011,\n",
      "          1.2862,  1.7149, -0.4287, -2.1437,  1.2862,  0.0000, -0.4287,  0.4287,\n",
      "         -0.4287, -1.2862,  1.7149,  1.7149, -1.2862, -0.4287,  0.0000, -1.2862,\n",
      "          0.0000, -0.8575,  0.4287,  3.4299,  1.7149, -0.4287,  0.4287, -0.8575,\n",
      "         -2.1437, -1.7149,  0.0000, -2.1437, -0.8575,  0.8575, -1.7149, -0.4287,\n",
      "          1.2862,  0.4287, -1.2862, -0.4287,  0.4287, -0.4287,  0.4287,  0.8575,\n",
      "          0.8575,  0.8575,  0.0000, -0.8575,  2.5724, -0.4287,  2.5724,  0.0000,\n",
      "          0.0000, -0.4287,  0.4287,  0.8575, -2.5724, -0.4287, -1.7149,  1.2862,\n",
      "         -1.2862,  1.2862, -0.4287,  1.2862, -1.7149, -2.1437,  0.4287, -1.2862,\n",
      "          1.2862, -0.8575, -0.4287, -0.8575,  0.0000,  2.1437, -0.4287, -0.8575,\n",
      "          0.0000, -0.4287, -0.8575, -1.7149, -3.0011,  1.2862, -0.8575,  0.8575,\n",
      "         -0.8575,  0.0000,  0.8575,  1.2862, -0.4287, -0.8575, -0.8575, -1.2862,\n",
      "          2.5724,  0.0000, -1.2862, -1.7149,  0.4287,  0.4287, -1.2862,  1.2862,\n",
      "          1.2862, -0.4287, -0.8575, -0.4287,  1.7149,  0.8575, -0.8575, -0.4287,\n",
      "          0.4287, -3.0011, -0.4287, -0.4287, -0.4287,  0.0000, -0.8575,  0.4287,\n",
      "          3.8586, -0.4287,  0.4287, -0.8575,  1.2862, -3.0011, -0.8575, -0.4287,\n",
      "         -0.8575,  1.7149,  0.4287, -0.8575, -2.1437,  1.2862, -2.5724,  1.7149,\n",
      "          0.8575, -0.4287, -1.2862,  0.8575, -1.2862,  1.7149, -2.5724,  3.0011,\n",
      "         -0.4287,  0.0000,  1.7149, -1.2862, -0.8575,  2.5724,  0.0000, -0.4287,\n",
      "          0.8575, -0.4287, -0.4287, -0.4287,  0.4287,  1.7149,  0.4287, -2.1437,\n",
      "          0.4287, -0.4287,  0.0000,  0.0000,  0.8575, -0.8575,  0.0000, -1.2862,\n",
      "          0.4287, -1.2862, -0.8575, -1.2862,  1.2862, -0.8575,  0.0000,  0.0000,\n",
      "         -0.4287,  0.4287, -2.5724,  0.4287,  0.0000,  0.4287, -1.7149,  2.5724,\n",
      "          0.4287, -0.8575,  0.4287, -0.4287, -1.2862, -0.4287,  2.1437,  0.8575,\n",
      "         -0.4287, -1.2862, -1.2862,  0.4287, -1.7149,  0.4287, -0.8575,  0.0000,\n",
      "          0.8575,  0.4287,  5.1448, -0.4287, -0.8575, -0.4287, -1.2862,  0.8575,\n",
      "         -2.1437,  0.0000, -0.8575, -0.4287,  0.4287, -0.4287, -1.2862,  1.2862,\n",
      "         -0.8575,  1.7149, -0.8575,  1.2862,  0.4287,  0.8575, -1.7149,  4.7161,\n",
      "          0.4287, -2.1437,  2.1437,  1.2862, -0.8575, -0.8575,  1.2862, -0.4287,\n",
      "          3.4299, -1.2862,  0.0000,  1.7149,  2.5724, -0.8575, -2.5724, -0.4287,\n",
      "         -0.8575, -1.7149,  0.0000,  0.0000, -0.4287,  0.4287, -0.4287, -0.4287,\n",
      "          0.8575,  0.0000,  0.8575,  0.4287,  1.2862,  0.4287,  1.7149, -1.2862,\n",
      "         -0.8575,  0.0000, -0.4287,  1.2862,  1.7149, -0.8575,  2.1437,  0.4287,\n",
      "         -3.0011, -0.4287,  0.0000,  0.8575, -1.2862,  1.7149,  0.8575, -2.5724,\n",
      "         -2.1437, -2.5724,  0.0000, -0.4287,  3.4299, -1.2862,  0.4287, -1.7149,\n",
      "          1.2862, -1.2862, -2.1437, -0.4287,  1.7149, -0.8575, -1.7149,  1.7149,\n",
      "          0.4287, -0.8575,  0.4287,  1.7149,  0.8575,  3.0011,  0.0000,  0.4287,\n",
      "         -0.8575,  0.4287, -0.8575, -0.8575,  2.1437,  0.4287,  0.4287, -0.8575,\n",
      "          1.2862,  0.4287,  0.0000,  0.4287,  1.2862,  0.4287, -0.8575,  1.2862,\n",
      "          3.8586,  1.2862,  0.0000,  0.4287,  0.4287,  2.5724,  1.2862,  3.0011,\n",
      "          0.8575,  0.0000, -1.7149, -0.4287,  1.7149,  0.8575,  0.0000, -0.4287,\n",
      "          1.2862,  3.4299,  2.5724,  0.4287, -1.2862,  2.1437,  0.4287, -0.8575,\n",
      "          0.0000,  0.8575,  0.0000,  0.0000, -0.8575,  0.0000, -0.4287,  0.0000,\n",
      "          0.0000,  0.8575, -0.8575, -0.4287,  1.2862,  1.2862,  0.4287,  0.8575,\n",
      "         -1.2862, -0.4287, -0.8575,  2.1437,  1.7149,  0.4287,  2.5724,  2.5724,\n",
      "          2.1437,  0.4287,  0.4287,  0.8575, -1.7149,  1.2862,  0.8575, -0.8575,\n",
      "          0.8575,  0.4287,  0.4287,  0.4287,  0.4287, -1.2862,  1.2862,  1.7149,\n",
      "          0.8575,  0.8575,  0.4287,  3.8586,  0.0000,  4.2873,  1.2862,  1.2862,\n",
      "         -0.4287,  2.1437,  1.2862,  0.4287,  4.2873, -0.4287,  0.8575,  1.7149,\n",
      "          0.8575,  0.4287,  0.0000,  2.1437,  0.8575,  1.7149,  1.2862,  2.1437,\n",
      "          1.7149,  0.4287,  0.4287,  1.2862,  1.2862,  1.2862,  2.1437,  0.4287]],\n",
      "       size=(1, 1000), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.42873188853263855,\n",
      "       zero_point=32)\n",
      "torch.float32 tensor([[ 0.4287,  3.0011, -0.8575, -0.8575,  0.4287,  3.8586,  2.5724,  0.4287,\n",
      "          0.4287, -0.4287,  0.8575,  1.7149,  0.8575,  0.8575,  0.4287,  1.2862,\n",
      "         -1.2862, -1.7149,  0.4287, -0.8575, -0.8575, -1.2862,  0.0000, -0.4287,\n",
      "         -1.2862, -0.8575,  1.2862,  0.4287,  0.0000,  0.8575, -0.8575, -1.2862,\n",
      "         -1.2862,  0.4287,  0.4287,  0.0000,  0.4287, -0.8575,  0.0000, -1.2862,\n",
      "         -0.4287, -1.2862, -0.8575, -0.8575, -1.7149,  0.4287, -0.4287, -1.2862,\n",
      "         -1.2862, -0.4287,  0.0000, -1.7149,  0.0000, -0.4287, -0.8575, -1.2862,\n",
      "         -1.7149, -1.7149,  0.8575, -1.2862, -0.4287, -2.1437, -1.7149, -0.4287,\n",
      "         -0.8575,  1.7149, -0.8575, -1.2862, -2.1437,  0.0000,  1.7149,  0.8575,\n",
      "          0.8575,  2.1437,  2.1437,  2.1437,  0.8575,  2.1437,  3.4299,  2.5724,\n",
      "          0.0000, -0.4287, -0.4287,  0.0000, -0.4287,  0.0000,  1.2862, -1.2862,\n",
      "          0.8575, -0.4287, -0.4287, -0.4287,  1.7149, -0.4287, -0.4287, -1.7149,\n",
      "          1.2862, -1.2862, -1.2862,  0.4287, -0.4287, -0.4287, -1.7149,  0.4287,\n",
      "         -0.4287, -2.1437, -0.8575,  5.1448,  3.4299,  3.0011,  3.4299,  4.2873,\n",
      "          1.2862,  0.8575,  0.8575,  3.4299,  1.7149,  1.7149,  2.5724,  0.8575,\n",
      "          0.4287,  0.8575,  0.0000,  0.4287,  0.8575,  0.0000,  2.5724,  0.0000,\n",
      "          0.0000,  0.4287,  0.8575, -0.8575,  0.8575,  0.4287,  1.2862, -0.4287,\n",
      "         -1.2862, -0.4287,  0.4287, -0.8575, -0.8575, -0.8575, -0.4287,  0.0000,\n",
      "         -1.2862,  0.4287, -1.7149, -1.2862, -1.2862,  0.0000,  0.4287,  0.4287,\n",
      "         -1.2862, -0.4287, -0.8575, -0.4287, -0.4287, -0.4287, -0.4287,  0.0000,\n",
      "         -0.8575, -0.4287,  0.4287,  0.8575, -1.2862, -0.4287, -0.8575, -0.8575,\n",
      "          0.0000, -0.8575, -0.8575, -0.8575, -1.2862, -0.4287, -2.1437, -0.4287,\n",
      "         -1.7149, -1.2862, -0.4287, -0.8575, -1.2862, -0.8575, -0.4287, -0.8575,\n",
      "         -0.4287, -0.4287, -0.4287, -0.4287, -0.8575, -0.4287, -1.2862,  0.8575,\n",
      "         -1.2862,  0.8575, -0.4287, -0.8575, -0.8575, -1.2862, -1.7149, -0.4287,\n",
      "         -0.8575, -0.4287, -0.4287,  0.8575,  0.0000, -1.7149, -1.2862,  0.4287,\n",
      "         -0.4287, -0.8575, -2.5724,  0.8575, -0.4287,  0.8575,  0.0000,  0.0000,\n",
      "         -0.4287, -0.8575, -0.4287,  0.0000,  0.0000,  0.0000, -0.8575, -0.8575,\n",
      "         -0.8575, -1.2862, -1.2862, -0.4287, -0.8575,  0.4287,  0.4287,  0.4287,\n",
      "         -1.2862, -1.2862,  0.4287, -0.8575,  0.0000,  0.0000, -0.4287, -0.8575,\n",
      "          0.0000, -0.4287, -0.4287, -0.4287, -0.8575, -0.8575, -1.7149, -0.8575,\n",
      "         -1.2862, -0.8575, -0.4287, -1.2862, -0.8575,  0.0000,  0.0000, -1.2862,\n",
      "         -1.2862, -0.4287,  0.0000,  0.8575,  0.8575, -0.8575, -1.2862,  0.4287,\n",
      "         -0.8575,  0.0000, -1.2862, -0.8575, -2.5724, -0.4287,  0.4287,  0.8575,\n",
      "          0.4287,  0.8575,  1.2862,  0.0000, -0.4287,  1.2862,  0.0000,  0.8575,\n",
      "          0.4287, -0.8575,  0.4287,  0.0000,  0.0000, -1.2862,  0.0000, -1.7149,\n",
      "         -0.8575, -1.7149, -1.2862,  0.0000,  0.0000,  0.0000,  0.4287,  0.0000,\n",
      "          1.2862, -0.4287,  0.4287,  0.0000, -1.2862,  1.7149,  0.8575,  0.0000,\n",
      "          0.4287, -0.8575,  0.0000,  0.8575,  0.4287,  1.2862,  4.2873, -0.8575,\n",
      "         -0.4287,  1.2862,  2.5724, -0.4287, -0.8575, -0.8575,  1.2862,  0.4287,\n",
      "         -0.4287, -2.1437, -0.8575,  0.0000, -1.2862,  0.0000, -1.7149,  2.1437,\n",
      "          2.5724,  3.0011,  0.0000,  1.2862,  0.0000,  0.4287,  0.8575,  0.0000,\n",
      "         -0.8575,  0.8575, -0.4287, -0.8575, -0.4287,  0.0000, -0.4287, -1.7149,\n",
      "          0.4287,  0.4287, -0.4287,  0.4287, -1.2862, -1.2862, -0.4287,  0.8575,\n",
      "          1.2862,  1.2862,  0.0000, -0.8575,  0.0000, -0.4287, -0.4287, -0.4287,\n",
      "         -0.4287, -0.4287,  0.4287, -0.8575, -2.1437,  0.8575, -1.2862, -0.4287,\n",
      "          0.0000, -1.2862,  0.4287,  0.4287,  0.0000, -0.4287,  1.2862, -0.8575,\n",
      "          0.8575,  0.8575,  0.4287,  0.4287,  0.8575,  0.8575,  0.8575, -0.4287,\n",
      "         -0.4287, -0.4287, -0.8575, -0.4287, -1.2862,  0.0000,  0.8575,  0.8575,\n",
      "          1.2862,  0.8575, -0.4287,  0.4287,  0.8575,  0.4287,  0.0000,  1.2862,\n",
      "         -0.8575, -0.4287,  0.0000, -2.1437, -1.2862,  0.4287,  0.4287, -1.7149,\n",
      "         -3.4299,  1.7149,  0.8575,  1.2862,  0.4287, -0.8575, -0.4287, -0.4287,\n",
      "          0.4287,  2.1437,  0.0000, -0.4287,  0.0000,  0.4287, -0.4287, -1.2862,\n",
      "          0.0000,  0.4287,  0.4287,  1.2862,  0.4287,  1.2862,  0.4287, -1.2862,\n",
      "          0.0000,  1.2862,  0.8575,  2.1437, -2.1437,  0.4287,  0.0000, -0.8575,\n",
      "          0.0000,  1.7149, -0.8575,  1.2862,  0.0000,  0.8575,  2.5724, -0.4287,\n",
      "         -0.8575, -1.2862, -2.5724,  0.0000,  0.4287, -2.1437, -1.2862,  0.4287,\n",
      "         -0.4287, -0.4287,  1.7149,  0.8575, -1.7149,  0.8575,  1.2862,  0.8575,\n",
      "         -1.2862, -1.2862, -2.1437, -0.4287, -0.4287,  1.7149,  3.0011, -0.4287,\n",
      "         -0.8575, -2.1437, -0.4287,  0.8575, -0.4287, -2.1437, -0.8575, -2.1437,\n",
      "          1.2862, -0.8575, -1.7149, -1.7149, -1.7149,  0.0000,  0.4287,  0.0000,\n",
      "          1.7149,  2.5724,  1.7149, -0.8575, -0.8575,  0.4287,  1.7149, -1.2862,\n",
      "         -0.4287,  0.4287, -0.4287, -0.4287, -1.2862,  0.4287, -1.7149, -0.8575,\n",
      "          0.8575, -0.4287,  1.2862, -0.4287, -1.2862, -1.7149, -2.1437, -1.2862,\n",
      "         -1.2862,  0.0000, -1.2862, -0.8575, -1.2862,  0.0000, -0.4287, -0.8575,\n",
      "         -1.2862,  1.7149,  0.4287,  0.0000,  0.0000, -1.2862, -2.1437, -0.8575,\n",
      "         -0.4287, -1.2862,  0.8575, -0.4287, -0.4287,  0.4287,  0.0000, -3.0011,\n",
      "         -1.7149, -0.8575,  1.7149,  2.5724, -0.4287,  0.8575,  0.8575, -0.8575,\n",
      "          1.2862,  0.4287, -0.8575, -2.5724, -1.7149,  3.0011, -2.1437,  0.8575,\n",
      "          1.2862, -0.4287, -0.8575, -1.2862,  5.1448,  0.8575,  0.0000,  0.4287,\n",
      "         -1.2862, -1.7149,  3.0011,  0.0000, -1.2862,  0.4287, -0.4287,  2.1437,\n",
      "          0.0000, -1.7149,  0.4287, -1.2862,  0.8575, -2.1437,  0.4287, -0.8575,\n",
      "         -1.7149,  3.0011,  1.7149, -0.4287,  1.2862, -0.4287, -1.2862, -1.2862,\n",
      "         -0.4287,  0.0000, -2.1437, -1.2862,  0.0000,  0.4287, -0.8575,  1.2862,\n",
      "         -1.2862, -1.2862, -0.4287, -0.8575, -1.2862, -2.1437, -0.4287,  3.0011,\n",
      "          1.2862,  1.7149, -0.4287, -2.1437,  1.2862,  0.0000, -0.4287,  0.4287,\n",
      "         -0.4287, -1.2862,  1.7149,  1.7149, -1.2862, -0.4287,  0.0000, -1.2862,\n",
      "          0.0000, -0.8575,  0.4287,  3.4299,  1.7149, -0.4287,  0.4287, -0.8575,\n",
      "         -2.1437, -1.7149,  0.0000, -2.1437, -0.8575,  0.8575, -1.7149, -0.4287,\n",
      "          1.2862,  0.4287, -1.2862, -0.4287,  0.4287, -0.4287,  0.4287,  0.8575,\n",
      "          0.8575,  0.8575,  0.0000, -0.8575,  2.5724, -0.4287,  2.5724,  0.0000,\n",
      "          0.0000, -0.4287,  0.4287,  0.8575, -2.5724, -0.4287, -1.7149,  1.2862,\n",
      "         -1.2862,  1.2862, -0.4287,  1.2862, -1.7149, -2.1437,  0.4287, -1.2862,\n",
      "          1.2862, -0.8575, -0.4287, -0.8575,  0.0000,  2.1437, -0.4287, -0.8575,\n",
      "          0.0000, -0.4287, -0.8575, -1.7149, -3.0011,  1.2862, -0.8575,  0.8575,\n",
      "         -0.8575,  0.0000,  0.8575,  1.2862, -0.4287, -0.8575, -0.8575, -1.2862,\n",
      "          2.5724,  0.0000, -1.2862, -1.7149,  0.4287,  0.4287, -1.2862,  1.2862,\n",
      "          1.2862, -0.4287, -0.8575, -0.4287,  1.7149,  0.8575, -0.8575, -0.4287,\n",
      "          0.4287, -3.0011, -0.4287, -0.4287, -0.4287,  0.0000, -0.8575,  0.4287,\n",
      "          3.8586, -0.4287,  0.4287, -0.8575,  1.2862, -3.0011, -0.8575, -0.4287,\n",
      "         -0.8575,  1.7149,  0.4287, -0.8575, -2.1437,  1.2862, -2.5724,  1.7149,\n",
      "          0.8575, -0.4287, -1.2862,  0.8575, -1.2862,  1.7149, -2.5724,  3.0011,\n",
      "         -0.4287,  0.0000,  1.7149, -1.2862, -0.8575,  2.5724,  0.0000, -0.4287,\n",
      "          0.8575, -0.4287, -0.4287, -0.4287,  0.4287,  1.7149,  0.4287, -2.1437,\n",
      "          0.4287, -0.4287,  0.0000,  0.0000,  0.8575, -0.8575,  0.0000, -1.2862,\n",
      "          0.4287, -1.2862, -0.8575, -1.2862,  1.2862, -0.8575,  0.0000,  0.0000,\n",
      "         -0.4287,  0.4287, -2.5724,  0.4287,  0.0000,  0.4287, -1.7149,  2.5724,\n",
      "          0.4287, -0.8575,  0.4287, -0.4287, -1.2862, -0.4287,  2.1437,  0.8575,\n",
      "         -0.4287, -1.2862, -1.2862,  0.4287, -1.7149,  0.4287, -0.8575,  0.0000,\n",
      "          0.8575,  0.4287,  5.1448, -0.4287, -0.8575, -0.4287, -1.2862,  0.8575,\n",
      "         -2.1437,  0.0000, -0.8575, -0.4287,  0.4287, -0.4287, -1.2862,  1.2862,\n",
      "         -0.8575,  1.7149, -0.8575,  1.2862,  0.4287,  0.8575, -1.7149,  4.7161,\n",
      "          0.4287, -2.1437,  2.1437,  1.2862, -0.8575, -0.8575,  1.2862, -0.4287,\n",
      "          3.4299, -1.2862,  0.0000,  1.7149,  2.5724, -0.8575, -2.5724, -0.4287,\n",
      "         -0.8575, -1.7149,  0.0000,  0.0000, -0.4287,  0.4287, -0.4287, -0.4287,\n",
      "          0.8575,  0.0000,  0.8575,  0.4287,  1.2862,  0.4287,  1.7149, -1.2862,\n",
      "         -0.8575,  0.0000, -0.4287,  1.2862,  1.7149, -0.8575,  2.1437,  0.4287,\n",
      "         -3.0011, -0.4287,  0.0000,  0.8575, -1.2862,  1.7149,  0.8575, -2.5724,\n",
      "         -2.1437, -2.5724,  0.0000, -0.4287,  3.4299, -1.2862,  0.4287, -1.7149,\n",
      "          1.2862, -1.2862, -2.1437, -0.4287,  1.7149, -0.8575, -1.7149,  1.7149,\n",
      "          0.4287, -0.8575,  0.4287,  1.7149,  0.8575,  3.0011,  0.0000,  0.4287,\n",
      "         -0.8575,  0.4287, -0.8575, -0.8575,  2.1437,  0.4287,  0.4287, -0.8575,\n",
      "          1.2862,  0.4287,  0.0000,  0.4287,  1.2862,  0.4287, -0.8575,  1.2862,\n",
      "          3.8586,  1.2862,  0.0000,  0.4287,  0.4287,  2.5724,  1.2862,  3.0011,\n",
      "          0.8575,  0.0000, -1.7149, -0.4287,  1.7149,  0.8575,  0.0000, -0.4287,\n",
      "          1.2862,  3.4299,  2.5724,  0.4287, -1.2862,  2.1437,  0.4287, -0.8575,\n",
      "          0.0000,  0.8575,  0.0000,  0.0000, -0.8575,  0.0000, -0.4287,  0.0000,\n",
      "          0.0000,  0.8575, -0.8575, -0.4287,  1.2862,  1.2862,  0.4287,  0.8575,\n",
      "         -1.2862, -0.4287, -0.8575,  2.1437,  1.7149,  0.4287,  2.5724,  2.5724,\n",
      "          2.1437,  0.4287,  0.4287,  0.8575, -1.7149,  1.2862,  0.8575, -0.8575,\n",
      "          0.8575,  0.4287,  0.4287,  0.4287,  0.4287, -1.2862,  1.2862,  1.7149,\n",
      "          0.8575,  0.8575,  0.4287,  3.8586,  0.0000,  4.2873,  1.2862,  1.2862,\n",
      "         -0.4287,  2.1437,  1.2862,  0.4287,  4.2873, -0.4287,  0.8575,  1.7149,\n",
      "          0.8575,  0.4287,  0.0000,  2.1437,  0.8575,  1.7149,  1.2862,  2.1437,\n",
      "          1.7149,  0.4287,  0.4287,  1.2862,  1.2862,  1.2862,  2.1437,  0.4287]])\n"
     ]
    }
   ],
   "source": [
    "print(type(modules[-1]))\n",
    "print(before_l[-1].dtype,before_l[-1])\n",
    "print(after_l[-1].dtype,after_l[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.features.0.weight\n",
      "model.features.0.bias\n",
      "model.features.0.scale\n",
      "model.features.0.zero_point\n",
      "model.features.2.weight\n",
      "model.features.2.bias\n",
      "model.features.2.scale\n",
      "model.features.2.zero_point\n",
      "model.features.5.weight\n",
      "model.features.5.bias\n",
      "model.features.5.scale\n",
      "model.features.5.zero_point\n",
      "model.features.7.weight\n",
      "model.features.7.bias\n",
      "model.features.7.scale\n",
      "model.features.7.zero_point\n",
      "model.features.10.weight\n",
      "model.features.10.bias\n",
      "model.features.10.scale\n",
      "model.features.10.zero_point\n",
      "model.features.12.weight\n",
      "model.features.12.bias\n",
      "model.features.12.scale\n",
      "model.features.12.zero_point\n",
      "model.features.14.weight\n",
      "model.features.14.bias\n",
      "model.features.14.scale\n",
      "model.features.14.zero_point\n",
      "model.features.17.weight\n",
      "model.features.17.bias\n",
      "model.features.17.scale\n",
      "model.features.17.zero_point\n",
      "model.features.19.weight\n",
      "model.features.19.bias\n",
      "model.features.19.scale\n",
      "model.features.19.zero_point\n",
      "model.features.21.weight\n",
      "model.features.21.bias\n",
      "model.features.21.scale\n",
      "model.features.21.zero_point\n",
      "model.features.24.weight\n",
      "model.features.24.bias\n",
      "model.features.24.scale\n",
      "model.features.24.zero_point\n",
      "model.features.26.weight\n",
      "model.features.26.bias\n",
      "model.features.26.scale\n",
      "model.features.26.zero_point\n",
      "model.features.28.weight\n",
      "model.features.28.bias\n",
      "model.features.28.scale\n",
      "model.features.28.zero_point\n",
      "model.classifier.0.scale\n",
      "model.classifier.0.zero_point\n",
      "model.classifier.0._packed_params.dtype\n",
      "model.classifier.0._packed_params._packed_params\n",
      "model.classifier.3.scale\n",
      "model.classifier.3.zero_point\n",
      "model.classifier.3._packed_params.dtype\n",
      "model.classifier.3._packed_params._packed_params\n",
      "model.classifier.6.scale\n",
      "model.classifier.6.zero_point\n",
      "model.classifier.6._packed_params.dtype\n",
      "model.classifier.6._packed_params._packed_params\n",
      "quant.scale\n",
      "quant.zero_point\n",
      "tensor([[[[ -23,  -75, -101],\n",
      "          [   5,  -64, -127],\n",
      "          [  24,  -50, -100]],\n",
      "\n",
      "         [[  36,  -21,  -39],\n",
      "          [  53,    6,  -14],\n",
      "          [  53,   30,   12]],\n",
      "\n",
      "         [[  54,    4,  -36],\n",
      "          [  64,   38,   -1],\n",
      "          [   6,   17,    5]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  21,   17,   -7],\n",
      "          [  19,    3,  -26],\n",
      "          [  15,   28,  -10]],\n",
      "\n",
      "         [[  17,   32,   44],\n",
      "          [  21,   28,   26],\n",
      "          [  -7,   15,   38]],\n",
      "\n",
      "         [[  17,  -16,  -76],\n",
      "          [ -44,  -54,  -59],\n",
      "          [ -29,  -19,   -3]]],\n",
      "\n",
      "\n",
      "        [[[  -9,  -55,  -95],\n",
      "          [ -26,  -58, -100],\n",
      "          [ -30,  -76, -114]],\n",
      "\n",
      "         [[  -5,   34,   11],\n",
      "          [  13,    7,  -11],\n",
      "          [   1,  -23,  -25]],\n",
      "\n",
      "         [[  10,  -10,  -31],\n",
      "          [ -13,   10,  -58],\n",
      "          [  13,    2,    6]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  28,   25,   18],\n",
      "          [  11,    2,   -8],\n",
      "          [  17,   12,  -18]],\n",
      "\n",
      "         [[ -16,    2,  -15],\n",
      "          [   0,   -5,   12],\n",
      "          [  -3,  -16,  -15]],\n",
      "\n",
      "         [[ -20,  -19,   19],\n",
      "          [ -41,  -39,   50],\n",
      "          [ -45,  -27,   29]]],\n",
      "\n",
      "\n",
      "        [[[   5,   10,   23],\n",
      "          [ -11,   -7,   12],\n",
      "          [   5,    4,   12]],\n",
      "\n",
      "         [[ -33,  -37,  -23],\n",
      "          [ -20,  -47,   -5],\n",
      "          [  15,   10,   46]],\n",
      "\n",
      "         [[ -18,   -7,   -4],\n",
      "          [  12,   19,    7],\n",
      "          [  37,   53,   30]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[   1,    6,   -7],\n",
      "          [  -3,   -5,   -1],\n",
      "          [   3,    1,  -14]],\n",
      "\n",
      "         [[  -3,   -2,    0],\n",
      "          [   0,   -3,    4],\n",
      "          [  13,    7,   14]],\n",
      "\n",
      "         [[  25,   -3,   16],\n",
      "          [  -9,  -20,   17],\n",
      "          [  10,    3,   15]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -25,  -16,    6],\n",
      "          [ -26,  -28,   10],\n",
      "          [ -25,  -14,  -14]],\n",
      "\n",
      "         [[ -11,    9,  -36],\n",
      "          [  17,   16,  -36],\n",
      "          [  -8,    1,  -35]],\n",
      "\n",
      "         [[ -16,  -12,  -29],\n",
      "          [   6,  -29,  -31],\n",
      "          [ -22,    0,   20]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  -1,    1,   -4],\n",
      "          [   1,    5,   10],\n",
      "          [   7,    4,   19]],\n",
      "\n",
      "         [[   5,   18,   22],\n",
      "          [  -1,    1,    8],\n",
      "          [  -3,    5,   16]],\n",
      "\n",
      "         [[ -17,  -38,  -42],\n",
      "          [ -20,   -5,   34],\n",
      "          [  -9,   33,   72]]],\n",
      "\n",
      "\n",
      "        [[[  22,   21,    9],\n",
      "          [  22,   16,    9],\n",
      "          [  -3,   12,   -1]],\n",
      "\n",
      "         [[  17,   -9,    6],\n",
      "          [ -32,  -25,  -15],\n",
      "          [ -17,   10,   -7]],\n",
      "\n",
      "         [[ -54,  -54,  -72],\n",
      "          [  40,   -4,  -40],\n",
      "          [  93,  100,   18]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  14,   18,    3],\n",
      "          [  -2,   11,    9],\n",
      "          [  -3,  -16,    0]],\n",
      "\n",
      "         [[   4,   -3,    3],\n",
      "          [   0,    7,   11],\n",
      "          [   1,   -7,    2]],\n",
      "\n",
      "         [[  18,    6,   17],\n",
      "          [  -5,   33,   19],\n",
      "          [ -11,    9,   -7]]],\n",
      "\n",
      "\n",
      "        [[[ -25,    4,   29],\n",
      "          [   4,  -12,   20],\n",
      "          [ -14,  -11,   24]],\n",
      "\n",
      "         [[  20,  -71,   37],\n",
      "          [ -69,    5,   92],\n",
      "          [  -2,   11,  -33]],\n",
      "\n",
      "         [[ -55,  -48,   65],\n",
      "          [ -58,   56,   31],\n",
      "          [  35,   32,  -12]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[   5,   12,   -4],\n",
      "          [  19,   -3,  -13],\n",
      "          [   5,  -11,   10]],\n",
      "\n",
      "         [[  11,   11,    2],\n",
      "          [   1,   -7,   -8],\n",
      "          [  -3,  -14,   -7]],\n",
      "\n",
      "         [[   9,   17,   25],\n",
      "          [   4,   12,  -22],\n",
      "          [  10,  -35,  -15]]]], dtype=torch.int8)\n",
      "odict_keys(['model.features.0.weight', 'model.features.0.bias', 'model.features.2.weight', 'model.features.2.bias', 'model.features.5.weight', 'model.features.5.bias', 'model.features.7.weight', 'model.features.7.bias', 'model.features.10.weight', 'model.features.10.bias', 'model.features.12.weight', 'model.features.12.bias', 'model.features.14.weight', 'model.features.14.bias', 'model.features.17.weight', 'model.features.17.bias', 'model.features.19.weight', 'model.features.19.bias', 'model.features.21.weight', 'model.features.21.bias', 'model.features.24.weight', 'model.features.24.bias', 'model.features.26.weight', 'model.features.26.bias', 'model.features.28.weight', 'model.features.28.bias', 'model.classifier.0.weight', 'model.classifier.0.bias', 'model.classifier.3.weight', 'model.classifier.3.bias', 'model.classifier.6.weight', 'model.classifier.6.bias'])\n",
      "tensor([[[[-3.0606e-02, -9.8520e-02, -1.3260e-01],\n",
      "          [ 6.8208e-03, -8.3483e-02, -1.6697e-01],\n",
      "          [ 3.1015e-02, -6.5803e-02, -1.3171e-01]],\n",
      "\n",
      "         [[ 4.7407e-02, -2.7588e-02, -5.1127e-02],\n",
      "          [ 7.0129e-02,  8.2528e-03, -1.8340e-02],\n",
      "          [ 6.9918e-02,  3.8993e-02,  1.6228e-02]],\n",
      "\n",
      "         [[ 7.0700e-02,  5.2703e-03, -4.7362e-02],\n",
      "          [ 8.4006e-02,  4.9190e-02, -1.6474e-03],\n",
      "          [ 8.5166e-03,  2.2350e-02,  5.9118e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.7666e-02,  2.1778e-02, -9.4606e-03],\n",
      "          [ 2.5511e-02,  4.1186e-03, -3.4521e-02],\n",
      "          [ 2.0150e-02,  3.7068e-02, -1.3509e-02]],\n",
      "\n",
      "         [[ 2.1684e-02,  4.1812e-02,  5.8284e-02],\n",
      "          [ 2.7431e-02,  3.6847e-02,  3.4335e-02],\n",
      "          [-9.4839e-03,  1.9745e-02,  5.0264e-02]],\n",
      "\n",
      "         [[ 2.1769e-02, -2.1388e-02, -9.9363e-02],\n",
      "          [-5.7156e-02, -7.1328e-02, -7.7600e-02],\n",
      "          [-3.7508e-02, -2.5453e-02, -4.5096e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.3319e-02, -7.7979e-02, -1.3496e-01],\n",
      "          [-3.7411e-02, -8.1807e-02, -1.4195e-01],\n",
      "          [-4.1913e-02, -1.0756e-01, -1.6164e-01]],\n",
      "\n",
      "         [[-6.8725e-03,  4.8598e-02,  1.5008e-02],\n",
      "          [ 1.8636e-02,  9.8393e-03, -1.5973e-02],\n",
      "          [ 9.5164e-04, -3.2665e-02, -3.5824e-02]],\n",
      "\n",
      "         [[ 1.4780e-02, -1.4260e-02, -4.4468e-02],\n",
      "          [-1.8438e-02,  1.4841e-02, -8.2337e-02],\n",
      "          [ 1.8329e-02,  2.1435e-03,  9.0911e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.0342e-02,  3.6146e-02,  2.5515e-02],\n",
      "          [ 1.5779e-02,  3.1012e-03, -1.0942e-02],\n",
      "          [ 2.3790e-02,  1.6440e-02, -2.5835e-02]],\n",
      "\n",
      "         [[-2.2844e-02,  2.5371e-03, -2.1714e-02],\n",
      "          [ 3.9534e-04, -6.4903e-03,  1.6979e-02],\n",
      "          [-4.7200e-03, -2.2301e-02, -2.1298e-02]],\n",
      "\n",
      "         [[-2.8434e-02, -2.6771e-02,  2.7432e-02],\n",
      "          [-5.8088e-02, -5.5869e-02,  7.0289e-02],\n",
      "          [-6.4470e-02, -3.8172e-02,  4.1569e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2508e-02,  2.4738e-02,  5.3893e-02],\n",
      "          [-2.5838e-02, -1.6176e-02,  2.8344e-02],\n",
      "          [ 1.2948e-02,  9.0717e-03,  2.8181e-02]],\n",
      "\n",
      "         [[-7.9309e-02, -8.8828e-02, -5.5737e-02],\n",
      "          [-4.8359e-02, -1.1139e-01, -1.0901e-02],\n",
      "          [ 3.4640e-02,  2.3298e-02,  1.1002e-01]],\n",
      "\n",
      "         [[-4.3616e-02, -1.6598e-02, -1.0547e-02],\n",
      "          [ 2.8982e-02,  4.5279e-02,  1.6869e-02],\n",
      "          [ 8.8168e-02,  1.2599e-01,  7.2292e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3967e-03,  1.5333e-02, -1.7196e-02],\n",
      "          [-8.3107e-03, -1.2905e-02, -1.4394e-03],\n",
      "          [ 6.3471e-03,  1.7825e-03, -3.3770e-02]],\n",
      "\n",
      "         [[-7.9354e-03, -5.8610e-03, -7.6292e-05],\n",
      "          [ 6.8661e-04, -6.0019e-03,  1.0119e-02],\n",
      "          [ 3.0581e-02,  1.6821e-02,  3.2570e-02]],\n",
      "\n",
      "         [[ 5.9351e-02, -7.4398e-03,  3.8274e-02],\n",
      "          [-2.0792e-02, -4.8833e-02,  4.0274e-02],\n",
      "          [ 2.3138e-02,  7.4794e-03,  3.5027e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.5464e-02, -2.9275e-02,  1.0282e-02],\n",
      "          [-4.6952e-02, -4.9473e-02,  1.7632e-02],\n",
      "          [-4.3815e-02, -2.4871e-02, -2.4908e-02]],\n",
      "\n",
      "         [[-1.9359e-02,  1.5948e-02, -6.3554e-02],\n",
      "          [ 3.0096e-02,  2.8515e-02, -6.4439e-02],\n",
      "          [-1.4547e-02,  2.2238e-03, -6.3371e-02]],\n",
      "\n",
      "         [[-2.7915e-02, -2.1738e-02, -5.1691e-02],\n",
      "          [ 1.0889e-02, -5.1994e-02, -5.4649e-02],\n",
      "          [-3.9741e-02,  5.2832e-04,  3.5375e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9130e-03,  1.0995e-03, -7.4082e-03],\n",
      "          [ 1.8428e-03,  8.2602e-03,  1.8680e-02],\n",
      "          [ 1.1709e-02,  6.9263e-03,  3.3630e-02]],\n",
      "\n",
      "         [[ 8.9195e-03,  3.2807e-02,  3.9282e-02],\n",
      "          [-2.5044e-03,  1.7645e-03,  1.4056e-02],\n",
      "          [-5.4708e-03,  9.5572e-03,  2.8639e-02]],\n",
      "\n",
      "         [[-3.0241e-02, -6.7225e-02, -7.4299e-02],\n",
      "          [-3.6222e-02, -9.7721e-03,  6.1669e-02],\n",
      "          [-1.6392e-02,  5.8156e-02,  1.2894e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.0321e-02,  3.7677e-02,  1.7181e-02],\n",
      "          [ 4.0298e-02,  2.9479e-02,  1.6698e-02],\n",
      "          [-4.8739e-03,  2.1128e-02, -2.6295e-03]],\n",
      "\n",
      "         [[ 3.1079e-02, -1.6475e-02,  1.0769e-02],\n",
      "          [-5.8626e-02, -4.6721e-02, -2.7764e-02],\n",
      "          [-3.1245e-02,  1.7635e-02, -1.2211e-02]],\n",
      "\n",
      "         [[-9.8675e-02, -9.8915e-02, -1.3225e-01],\n",
      "          [ 7.3214e-02, -7.5238e-03, -7.4033e-02],\n",
      "          [ 1.7070e-01,  1.8346e-01,  3.3906e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.5318e-02,  3.2680e-02,  5.2874e-03],\n",
      "          [-4.1894e-03,  2.0011e-02,  1.6021e-02],\n",
      "          [-4.8276e-03, -2.9130e-02,  7.4975e-04]],\n",
      "\n",
      "         [[ 7.1959e-03, -5.1528e-03,  5.1333e-03],\n",
      "          [-8.6466e-04,  1.2417e-02,  2.0805e-02],\n",
      "          [ 2.4862e-03, -1.3194e-02,  3.6563e-03]],\n",
      "\n",
      "         [[ 3.2415e-02,  1.0823e-02,  3.1720e-02],\n",
      "          [-8.4911e-03,  5.9831e-02,  3.4606e-02],\n",
      "          [-2.0674e-02,  1.6525e-02, -1.3183e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.6955e-02,  1.0821e-02,  7.9260e-02],\n",
      "          [ 1.0361e-02, -3.3380e-02,  5.2775e-02],\n",
      "          [-3.8372e-02, -3.0964e-02,  6.5422e-02]],\n",
      "\n",
      "         [[ 5.4925e-02, -1.9160e-01,  9.9709e-02],\n",
      "          [-1.8754e-01,  1.3248e-02,  2.4748e-01],\n",
      "          [-5.6662e-03,  2.8737e-02, -8.9787e-02]],\n",
      "\n",
      "         [[-1.4905e-01, -1.2974e-01,  1.7649e-01],\n",
      "          [-1.5774e-01,  1.5130e-01,  8.4126e-02],\n",
      "          [ 9.4988e-02,  8.5186e-02, -3.3635e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2278e-02,  3.3348e-02, -1.0524e-02],\n",
      "          [ 5.0310e-02, -9.1280e-03, -3.3890e-02],\n",
      "          [ 1.3459e-02, -2.9601e-02,  2.6862e-02]],\n",
      "\n",
      "         [[ 3.1006e-02,  2.8481e-02,  5.2361e-03],\n",
      "          [ 3.8914e-03, -1.8899e-02, -2.1819e-02],\n",
      "          [-8.4505e-03, -3.6632e-02, -1.9476e-02]],\n",
      "\n",
      "         [[ 2.3835e-02,  4.5658e-02,  6.7297e-02],\n",
      "          [ 1.1103e-02,  3.2035e-02, -5.8449e-02],\n",
      "          [ 2.6805e-02, -9.3975e-02, -4.0504e-02]]]])\n"
     ]
    }
   ],
   "source": [
    "for child in model_static_quantized.children():\n",
    "    if isinstance(child, nn.Sequential) or isinstance(child, torchvision.models.vgg.VGG):\n",
    "        for n,c in child.named_children():\n",
    "            for name, param in c.named_parameters():\n",
    "                print(name)\n",
    "\n",
    "state = model.state_dict()\n",
    "for names in model_static_quantized.state_dict():\n",
    "    print(names)\n",
    "print(model_static_quantized.model.features[2].weight().int_repr())\n",
    "print(state.keys())\n",
    "print(state['model.features.2.weight'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defualt qconfig ; QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "fbgemm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1209: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Quantized(\n",
       "  (model): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): QuantizedLinear(in_features=25088, out_features=4096, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): QuantizedDropout(p=0.5, inplace=False)\n",
       "      (3): QuantizedLinear(in_features=4096, out_features=4096, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): QuantizedDropout(p=0.5, inplace=False)\n",
       "      (6): QuantizedLinear(in_features=4096, out_features=1000, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "    )\n",
       "  )\n",
       "  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.vgg.vgg16(pretrained=True)\n",
    "model = Quantized(model)\n",
    "\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "model.fuse_model()\n",
    "backend = \"fbgemm\"\n",
    "qconfig = torch.ao.quantization.get_default_qconfig(backend)\n",
    "model.qconfig = qconfig\n",
    "print(f\"defualt qconfig ; {model.qconfig}\")\n",
    "torch.backends.quantized.engine = backend\n",
    "print(torch.backends.quantized.engine)\n",
    "model_static_quantized = torch.ao.quantization.prepare(model, inplace = False)\n",
    "\n",
    "# make quantized model\n",
    "torch.ao.quantization.convert(model_static_quantized, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [13:38<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test result : 5.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nocali_test_acc = test(model_static_quantized, val_loader)\n",
    "print(f\"test result : {nocali_test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integer Sample Calibraiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defualt qconfig ; QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "fbgemm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1563 [00:22<53:59,  2.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Quantized(\n",
       "  (model): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.17084623873233795, zero_point=58, padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.35520821809768677, zero_point=73, padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.5221890211105347, zero_point=82, padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.7225201725959778, zero_point=71, padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.9027827978134155, zero_point=74, padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.046532154083252, zero_point=71, padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.045250415802002, zero_point=72, padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.2762900590896606, zero_point=75, padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0824809074401855, zero_point=80, padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.887078046798706, zero_point=83, padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.9002187848091125, zero_point=65, padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.7038030624389648, zero_point=81, padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.6258416175842285, zero_point=75, padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): QuantizedLinear(in_features=25088, out_features=4096, scale=0.42471253871917725, zero_point=77, qscheme=torch.per_channel_affine)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): QuantizedDropout(p=0.5, inplace=False)\n",
       "      (3): QuantizedLinear(in_features=4096, out_features=4096, scale=0.24681514501571655, zero_point=79, qscheme=torch.per_channel_affine)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): QuantizedDropout(p=0.5, inplace=False)\n",
       "      (6): QuantizedLinear(in_features=4096, out_features=1000, scale=0.42873188853263855, zero_point=32, qscheme=torch.per_channel_affine)\n",
       "    )\n",
       "  )\n",
       "  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.vgg.vgg16(pretrained=True)\n",
    "model = Quantized(model)\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Resize(256),\n",
    "    transforms.PILToTensor(),\n",
    "    ])\n",
    "test_dataloader = torch.utils.data.DataLoader(val_data, batch_size=32,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "model.fuse_model()\n",
    "backend = \"fbgemm\"\n",
    "qconfig = torch.ao.quantization.get_default_qconfig(backend)\n",
    "model.qconfig = qconfig\n",
    "print(f\"defualt qconfig ; {model.qconfig}\")\n",
    "torch.backends.quantized.engine = backend\n",
    "print(torch.backends.quantized.engine)\n",
    "model_static_quantized = torch.ao.quantization.prepare(model, inplace = False)\n",
    "# calibraiotn\n",
    "test(model_static_quantized,test_dataloader,10)\n",
    "\n",
    "# make quantized model\n",
    "torch.ao.quantization.convert(model_static_quantized, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [13:32<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test result : 71.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "intcali_test_acc = test(model_static_quantized, test_dataloader)\n",
    "print(f\"test result : {intcali_test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '_int_mm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspace/int_infer/quant.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/quant.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39m255\u001b[39m,(\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/quant.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39m255\u001b[39m,(\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f696e66616c6c69626c655f616c6c656e227d/workspace/int_infer/quant.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m c_int32 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_int_mm(a,b)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute '_int_mm'"
     ]
    }
   ],
   "source": [
    "a = torch.randint(0,255,(3,3))\n",
    "b = torch.randint(0,255,(3,3))\n",
    "\n",
    "c_int32 = torch._int_mm(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
