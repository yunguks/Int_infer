{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import _log_api_usage_once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def manual_seed(seed):\n",
    "    np.random.seed(seed) #1\n",
    "    random.seed(seed) #2\n",
    "    torch.manual_seed(seed) #3\n",
    "    torch.cuda.manual_seed(seed) #4.1\n",
    "    torch.cuda.manual_seed_all(seed) #4.2\n",
    "    torch.backends.cudnn.benchmark = False #5 \n",
    "    torch.backends.cudnn.deterministic = True #6\n",
    "\n",
    "manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantized(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        self.model = model\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    def fuse_model(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# filepath = './checkpoint/vgg16.pth'\n",
    "# checkpoint = torch.load(filepath)\n",
    "# model = checkpoint['model']\n",
    "# print(model)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = torchvision.models.vgg.vgg16(pretrained=True)\n",
    "model = Quantized(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = []\n",
    "before_l = []\n",
    "after_l = []\n",
    "hooks = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    modules.append(module)\n",
    "    before_l.append(input[0])\n",
    "    after_l.append(output)\n",
    "\n",
    "def add_forward_hook(net, hooks):\n",
    "    for name, layer in net._modules.items():\n",
    "        if isinstance(layer, nn.Sequential) or isinstance(layer, torchvision.models.vgg.VGG):\n",
    "            add_forward_hook(layer, hooks)\n",
    "        else:\n",
    "            hook = layer.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "            \n",
    "    return hooks\n",
    "\n",
    "def remove_forward_hook(hooks):\n",
    "    for i in hooks:\n",
    "        i.remove()\n",
    "# out = model((torch.randn(1,3,32,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defualt qconfig ; QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "x86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41 41 41\n",
      "41 41 41\n",
      "Quantized(\n",
      "  (model): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.1237657368183136, zero_point=59, padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.38565802574157715, zero_point=65, padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.6498428583145142, zero_point=80, padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.8637077808380127, zero_point=62, padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.9496995806694031, zero_point=72, padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.9350182414054871, zero_point=73, padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.9701123833656311, zero_point=69, padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.1143577098846436, zero_point=73, padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.6404507756233215, zero_point=73, padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.4346373975276947, zero_point=83, padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.31292328238487244, zero_point=64, padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.23627974092960358, zero_point=65, padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.15628105401992798, zero_point=90, padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): QuantizedLinear(in_features=25088, out_features=4096, scale=0.0719834640622139, zero_point=75, qscheme=torch.per_channel_affine)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): QuantizedDropout(p=0.5, inplace=False)\n",
      "      (3): QuantizedLinear(in_features=4096, out_features=4096, scale=0.056296683847904205, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): QuantizedDropout(p=0.5, inplace=False)\n",
      "      (6): QuantizedLinear(in_features=4096, out_features=1000, scale=0.08006454259157181, zero_point=51, qscheme=torch.per_channel_affine)\n",
      "    )\n",
      "  )\n",
      "  (quant): Quantize(scale=tensor([0.0611]), zero_point=tensor([62]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()\n",
    "model.fuse_model()\n",
    "backend = \"x86\"\n",
    "qconfig = torch.ao.quantization.get_default_qconfig(backend)\n",
    "model.qconfig = qconfig\n",
    "print(f\"defualt qconfig ; {model.qconfig}\")\n",
    "torch.backends.quantized.engine = backend\n",
    "print(torch.backends.quantized.engine)\n",
    "model_static_quantized = torch.ao.quantization.prepare(model, inplace = False)\n",
    "# calibration\n",
    "for _ in range(10):\n",
    "    sample = (torch.randn(1,3,224,224))\n",
    "    model_static_quantized(sample)\n",
    "# sample = (torch.randint(0,255,(1,3,32,32),dtype=torch.uint8))/255\n",
    "torch.ao.quantization.convert(model_static_quantized, inplace = True) \n",
    "\n",
    "## test, quantized modules, input, output\n",
    "# make hook\n",
    "hooks = add_forward_hook(model_static_quantized, hooks)\n",
    "model_static_quantized(sample)\n",
    "print(len(hooks), len(modules), len(before_l), len(after_l))\n",
    "# remove hook, hook works at once\n",
    "remove_forward_hook(hooks)\n",
    "# for _ in range(5):\n",
    "#     sample = (torch.randn(1,3,224,224))\n",
    "#     model_static_quantized(sample)\n",
    "# print(len(modules), len(before_l), len(after_l))\n",
    "print(model_static_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.CenterCrop((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.PILToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std= (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "val_data = torchvision.datasets.ImageNet(root=\"./dataset/ImageNet\", split=\"val\", transform=test_transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=32,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [23:01<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test result : 51.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def memory_usage(message = \"debug\"):\n",
    "    p = psutil.Process()\n",
    "    rss = p.memory_info().rss / 2**20\n",
    "    print(f\"{message} memory usage : {rss:10.5f} MB\")\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    test_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(tqdm(test_loader,leave=True)):\n",
    "            imgs, target = data[0], data[1]\n",
    "            # imgs, target = data[0].to(device), data[1].to(device)\n",
    "            output = model(imgs)\n",
    "            _, preds = torch.max(output.data, 1)\n",
    "            test_acc += (preds==target).detach().sum().item()\n",
    "            # if i % 10 == 0:\n",
    "            #     memory_usage()\n",
    "\n",
    "    test_acc = 100. * test_acc/len(test_loader.dataset)\n",
    "    \n",
    "\n",
    "    return test_acc\n",
    "\n",
    "test_acc = test(model_static_quantized, val_loader)\n",
    "print(f\"test result : {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.ao.nn.quantized.modules.Quantize'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>\n",
      "<class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.dropout.Dropout'>\n",
      "<class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.ao.nn.quantized.modules.dropout.Dropout'>\n",
      "<class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "<class 'torch.ao.nn.quantized.modules.DeQuantize'>\n",
      "41 41 41\n",
      "<class 'torch.ao.nn.quantized.modules.Quantize'>\n",
      "before : <class 'torch.Tensor'>, torch.float32\n",
      "after : <class 'torch.Tensor'>, torch.quint8\n",
      "tensor([[[[-1.1616, -0.7948, -0.4891,  ...,  0.6114, -0.3057, -0.2445],\n",
      "          [-0.8559, -0.4891, -0.1223,  ...,  0.2445, -1.7730,  0.4280],\n",
      "          [-1.1616, -0.4280,  0.1223,  ..., -1.5896, -0.3668,  0.1834],\n",
      "          ...,\n",
      "          [ 1.6507,  1.3450, -0.1223,  ...,  0.7336, -0.1834,  0.5502],\n",
      "          [-0.3668, -0.3057,  0.5502,  ..., -1.7730,  0.1223, -0.1834],\n",
      "          [-0.9171,  0.2445,  0.1223,  ...,  0.3668,  1.4673,  1.1616]],\n",
      "\n",
      "         [[-0.5502, -0.1223, -0.1834,  ..., -1.5896, -2.3232, -0.3668],\n",
      "          [-0.3057, -0.4280, -1.0393,  ...,  1.1005, -1.1616, -0.4891],\n",
      "          [ 1.2227,  0.0611,  0.6114,  ...,  0.6725,  0.3668, -0.4280],\n",
      "          ...,\n",
      "          [ 0.1834,  0.7948,  1.1616,  ...,  0.8559,  1.0393, -0.2445],\n",
      "          [-0.6114, -1.3450,  0.9782,  ...,  1.2227,  0.1223,  0.5502],\n",
      "          [ 0.0000, -0.1223,  0.6114,  ...,  0.7948, -0.2445,  0.8559]],\n",
      "\n",
      "         [[-1.7730, -0.4891,  0.4280,  ...,  1.1616,  0.9782,  0.0000],\n",
      "          [-0.8559, -0.9171, -0.7948,  ..., -0.0611,  0.9782, -0.6114],\n",
      "          [ 1.2227,  1.2839, -0.1834,  ..., -0.1834, -2.6289,  0.7948],\n",
      "          ...,\n",
      "          [ 0.2445,  2.0787,  0.2445,  ...,  1.1005, -0.0611, -0.8559],\n",
      "          [ 0.9782,  0.5502, -0.5502,  ...,  1.8341, -0.6114, -0.9171],\n",
      "          [-1.2227, -0.3668,  0.9171,  ...,  1.5896,  0.1223,  0.9782]]]],\n",
      "       size=(1, 3, 224, 224), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.06113744527101517,\n",
      "       zero_point=62)\n",
      "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1139, 2.5991],\n",
      "          [0.0000, 2.3515, 0.2475,  ..., 0.0000, 0.7426, 0.8664],\n",
      "          [0.0000, 3.4654, 0.0000,  ..., 0.0000, 0.8664, 2.7228],\n",
      "          ...,\n",
      "          [0.9901, 1.3614, 1.9803,  ..., 0.0000, 4.0843, 0.7426],\n",
      "          [0.2475, 0.0000, 1.4852,  ..., 0.0000, 4.3318, 0.0000],\n",
      "          [0.1238, 0.6188, 0.6188,  ..., 2.2278, 3.8367, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.8664, 0.0000,  ..., 0.0000, 0.2475, 2.4753],\n",
      "          [2.5991, 2.2278, 1.4852,  ..., 0.0000, 0.0000, 1.9803],\n",
      "          [1.1139, 0.0000, 0.3713,  ..., 0.7426, 2.9704, 1.9803],\n",
      "          ...,\n",
      "          [1.2377, 0.0000, 0.0000,  ..., 1.9803, 0.0000, 0.0000],\n",
      "          [0.0000, 1.8565, 0.9901,  ..., 0.9901, 0.0000, 1.6090],\n",
      "          [0.0000, 2.1040, 0.1238,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[2.4753, 1.3614, 0.4951,  ..., 0.0000, 3.5892, 2.7228],\n",
      "          [0.0000, 0.1238, 1.3614,  ..., 0.0000, 0.7426, 0.9901],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 2.9704, 0.7426, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2475, 0.3713],\n",
      "          [1.1139, 5.6932, 1.9803,  ..., 0.1238, 2.9704, 0.0000],\n",
      "          [1.3614, 0.3713, 0.0000,  ..., 0.0000, 0.8664, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.6188, 0.0000, 0.0000],\n",
      "          [0.0000, 0.1238, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.3713,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.3713, 0.7426, 0.2475,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1238, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1238],\n",
      "          [0.0000, 0.0000, 0.2475,  ..., 0.0000, 0.0000, 0.7426]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.6188, 0.6188, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.1238, 0.2475, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.2475, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.6090, 0.4951, 1.1139,  ..., 0.6188, 0.4951, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 2.9704, 1.7327, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.4951, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.2475,  ..., 0.0000, 1.2377, 1.8565],\n",
      "          [1.3614, 0.1238, 0.7426,  ..., 0.0000, 0.3713, 0.0000],\n",
      "          [1.2377, 0.0000, 0.0000,  ..., 1.4852, 0.4951, 0.0000]]]],\n",
      "       size=(1, 64, 224, 224), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.1237657368183136,\n",
      "       zero_point=59)\n"
     ]
    }
   ],
   "source": [
    "for i in modules:\n",
    "    print(type(i))\n",
    "print(len(modules), len(before_l), len(after_l))\n",
    "for i in range(len(modules)):\n",
    "    if not isinstance(type(modules[i]), torch.quantization.QuantStub) and not isinstance(type(modules[i]), torch.quantization.DeQuantStub):\n",
    "        print(type(modules[i]))\n",
    "        print(f\"before : {type(before_l[i])}, {before_l[i].dtype}\")\n",
    "        print(f\"after : {type(after_l[i])}, {after_l[i].dtype}\")\n",
    "        break\n",
    "\n",
    "print(before_l[1])\n",
    "print(after_l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "torch.quint8 tensor([[[[ 59,  59,  59,  ...,  59,  68,  80],\n",
      "          [ 59,  78,  61,  ...,  59,  65,  66],\n",
      "          [ 59,  87,  59,  ...,  59,  66,  81],\n",
      "          ...,\n",
      "          [ 67,  70,  75,  ...,  59,  92,  65],\n",
      "          [ 61,  59,  71,  ...,  59,  94,  59],\n",
      "          [ 60,  64,  64,  ...,  77,  90,  59]],\n",
      "\n",
      "         [[ 59,  66,  59,  ...,  59,  61,  79],\n",
      "          [ 80,  77,  71,  ...,  59,  59,  75],\n",
      "          [ 68,  59,  62,  ...,  65,  83,  75],\n",
      "          ...,\n",
      "          [ 69,  59,  59,  ...,  75,  59,  59],\n",
      "          [ 59,  74,  67,  ...,  67,  59,  72],\n",
      "          [ 59,  76,  60,  ...,  59,  59,  59]],\n",
      "\n",
      "         [[ 79,  70,  63,  ...,  59,  88,  81],\n",
      "          [ 59,  60,  70,  ...,  59,  65,  67],\n",
      "          [ 59,  59,  59,  ...,  83,  65,  59],\n",
      "          ...,\n",
      "          [ 59,  59,  59,  ...,  59,  61,  62],\n",
      "          [ 68, 105,  75,  ...,  60,  83,  59],\n",
      "          [ 70,  62,  59,  ...,  59,  66,  59]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 59,  59,  59,  ...,  64,  59,  59],\n",
      "          [ 59,  60,  59,  ...,  59,  59,  59],\n",
      "          [ 59,  59,  62,  ...,  59,  59,  59],\n",
      "          ...,\n",
      "          [ 62,  65,  61,  ...,  59,  59,  59],\n",
      "          [ 60,  59,  59,  ...,  59,  59,  60],\n",
      "          [ 59,  59,  61,  ...,  59,  59,  65]],\n",
      "\n",
      "         [[ 59,  59,  59,  ...,  64,  64,  59],\n",
      "          [ 59,  59,  59,  ...,  60,  61,  59],\n",
      "          [ 59,  59,  59,  ...,  59,  59,  59],\n",
      "          ...,\n",
      "          [ 59,  59,  59,  ...,  59,  59,  59],\n",
      "          [ 61,  59,  59,  ...,  59,  59,  59],\n",
      "          [ 59,  59,  59,  ...,  59,  59,  59]],\n",
      "\n",
      "         [[ 72,  63,  68,  ...,  64,  63,  59],\n",
      "          [ 59,  59,  59,  ...,  83,  73,  59],\n",
      "          [ 59,  59,  59,  ...,  63,  59,  59],\n",
      "          ...,\n",
      "          [ 59,  59,  61,  ...,  59,  69,  74],\n",
      "          [ 70,  60,  65,  ...,  59,  62,  59],\n",
      "          [ 69,  59,  59,  ...,  71,  63,  59]]]], dtype=torch.uint8)\n",
      "torch.quint8 tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1139, 2.5991],\n",
      "          [0.0000, 2.3515, 0.2475,  ..., 0.0000, 0.7426, 0.8664],\n",
      "          [0.0000, 3.4654, 0.0000,  ..., 0.0000, 0.8664, 2.7228],\n",
      "          ...,\n",
      "          [0.9901, 1.3614, 1.9803,  ..., 0.0000, 4.0843, 0.7426],\n",
      "          [0.2475, 0.0000, 1.4852,  ..., 0.0000, 4.3318, 0.0000],\n",
      "          [0.1238, 0.6188, 0.6188,  ..., 2.2278, 3.8367, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.8664, 0.0000,  ..., 0.0000, 0.2475, 2.4753],\n",
      "          [2.5991, 2.2278, 1.4852,  ..., 0.0000, 0.0000, 1.9803],\n",
      "          [1.1139, 0.0000, 0.3713,  ..., 0.7426, 2.9704, 1.9803],\n",
      "          ...,\n",
      "          [1.2377, 0.0000, 0.0000,  ..., 1.9803, 0.0000, 0.0000],\n",
      "          [0.0000, 1.8565, 0.9901,  ..., 0.9901, 0.0000, 1.6090],\n",
      "          [0.0000, 2.1040, 0.1238,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[2.4753, 1.3614, 0.4951,  ..., 0.0000, 3.5892, 2.7228],\n",
      "          [0.0000, 0.1238, 1.3614,  ..., 0.0000, 0.7426, 0.9901],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 2.9704, 0.7426, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2475, 0.3713],\n",
      "          [1.1139, 5.6932, 1.9803,  ..., 0.1238, 2.9704, 0.0000],\n",
      "          [1.3614, 0.3713, 0.0000,  ..., 0.0000, 0.8664, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.6188, 0.0000, 0.0000],\n",
      "          [0.0000, 0.1238, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.3713,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.3713, 0.7426, 0.2475,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1238, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1238],\n",
      "          [0.0000, 0.0000, 0.2475,  ..., 0.0000, 0.0000, 0.7426]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.6188, 0.6188, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.1238, 0.2475, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.2475, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.6090, 0.4951, 1.1139,  ..., 0.6188, 0.4951, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 2.9704, 1.7327, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.4951, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.2475,  ..., 0.0000, 1.2377, 1.8565],\n",
      "          [1.3614, 0.1238, 0.7426,  ..., 0.0000, 0.3713, 0.0000],\n",
      "          [1.2377, 0.0000, 0.0000,  ..., 1.4852, 0.4951, 0.0000]]]],\n",
      "       size=(1, 64, 224, 224), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.1237657368183136,\n",
      "       zero_point=59)\n"
     ]
    }
   ],
   "source": [
    "print(type(modules[2]))\n",
    "print(before_l[2].dtype,before_l[2].int_repr())\n",
    "print(after_l[2].dtype,after_l[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.ao.nn.quantized.modules.DeQuantize'>\n",
      "torch.quint8 tensor([[-0.1601,  3.0425, -1.6013, -1.1209,  0.5605,  2.2418,  2.3219,  0.9608,\n",
      "          0.2402,  0.0801, -0.0801,  1.8415,  0.8807,  0.4804,  0.8006,  1.2010,\n",
      "         -2.2418, -1.5212,  1.5212, -0.9608, -1.2010, -1.5212,  0.2402,  0.1601,\n",
      "         -0.9608, -1.3611,  0.2402, -0.3203, -1.1209,  0.0801, -1.4412, -1.5212,\n",
      "         -2.4019, -0.4003, -0.1601, -0.7206, -0.1601, -1.5212, -1.3611, -1.5212,\n",
      "          0.0000, -2.1617, -1.0408, -1.2810, -1.9215, -0.1601,  0.4003, -1.4412,\n",
      "         -0.8807, -0.8006,  0.4804, -2.2418, -1.0408, -1.8415, -1.9215, -1.4412,\n",
      "         -3.0425, -2.9624,  0.2402, -1.6013, -0.9608, -2.8823, -2.8823, -1.4412,\n",
      "         -1.0408,  1.0408, -2.3219, -1.5212, -3.5228, -0.7206,  0.8006,  0.4003,\n",
      "          0.4003,  0.9608,  1.7614,  0.8006,  0.4003,  1.8415,  2.2418,  2.0817,\n",
      "         -0.3203, -0.6405, -0.3203, -0.6405,  0.6405,  0.0801,  1.1209, -1.6814,\n",
      "          1.7614,  0.0801, -0.0801,  0.0000,  2.1617, -0.1601,  0.3203, -2.0817,\n",
      "          1.8415, -1.2010, -2.3219,  0.8006, -0.2402,  0.3203, -2.2418,  0.0000,\n",
      "          0.0000, -2.0817, -0.5605,  4.5637,  2.6421,  2.6421,  2.6421,  3.4428,\n",
      "          0.3203,  0.1601,  0.3203,  2.0016,  1.3611,  0.5605,  2.1617, -0.0801,\n",
      "         -0.8006,  0.0801, -0.8807,  0.2402,  0.6405, -1.3611,  1.7614,  0.9608,\n",
      "          0.7206,  1.0408,  1.2810,  0.0000,  2.0817,  1.1209,  2.0016,  0.0000,\n",
      "         -0.4804, -0.5605,  1.0408, -1.7614, -1.8415, -1.0408, -1.2010, -0.2402,\n",
      "         -1.0408,  0.4003, -2.2418, -1.7614, -0.9608, -0.5605, -0.2402,  0.0801,\n",
      "         -2.0016, -0.9608, -1.6013, -0.9608, -0.8006, -0.3203, -0.8006, -0.0801,\n",
      "         -1.2010, -0.4003,  0.1601,  0.6405, -1.0408, -0.4003, -0.7206, -0.8807,\n",
      "         -0.4003, -0.5605, -0.7206, -1.1209, -1.1209,  0.0000, -2.3219, -0.8807,\n",
      "         -1.4412, -0.7206, -0.5605, -1.2010, -1.6814, -0.5605, -0.2402, -0.8006,\n",
      "         -0.2402, -0.7206, -0.7206, -0.5605, -1.2010, -0.4804, -2.0817,  0.8006,\n",
      "         -0.9608,  0.4003, -1.0408, -0.1601, -1.2810, -0.7206, -1.8415, -0.2402,\n",
      "         -1.0408, -0.4003, -0.7206,  1.4412,  0.0801, -1.6013, -0.8807,  0.0000,\n",
      "         -0.7206, -1.2810, -2.7222,  0.8006, -0.3203,  1.2010,  0.1601, -0.0801,\n",
      "         -0.6405, -0.4003, -0.1601, -0.3203, -0.4003,  0.4003, -0.4003, -0.8006,\n",
      "          0.0801, -1.1209, -1.3611, -0.8807, -0.8807,  0.5605,  0.7206,  0.9608,\n",
      "         -0.6405, -1.0408,  0.4804, -0.4804,  0.2402, -0.4804, -0.4003, -0.6405,\n",
      "          0.7206, -0.2402, -0.3203, -1.1209, -1.2810, -1.1209, -2.1617, -0.8006,\n",
      "         -0.8807, -0.9608, -0.1601, -1.0408, -1.0408,  0.2402, -0.1601, -0.7206,\n",
      "         -0.8807, -0.3203,  0.0000,  0.4804,  0.5605, -0.8006, -1.9215,  0.0000,\n",
      "         -0.9608, -0.0801, -2.0016, -0.8006, -3.2026,  0.1601,  0.5605,  2.0016,\n",
      "          1.2010,  1.2010,  2.1617,  0.0801, -0.2402,  2.0817,  0.1601,  1.0408,\n",
      "          1.0408, -1.1209,  0.6405,  0.0801, -0.1601, -1.2810,  0.5605, -1.4412,\n",
      "         -0.5605, -1.8415, -1.4412,  0.1601,  0.5605,  0.4804,  0.9608,  1.0408,\n",
      "          1.6814, -0.0801,  0.2402, -0.3203, -1.9215,  1.4412,  0.2402,  0.0000,\n",
      "          0.2402, -1.4412, -0.3203,  0.7206,  0.3203,  1.4412,  4.5637, -1.0408,\n",
      "         -0.5605,  1.7614,  2.4820, -0.5605, -1.4412, -1.0408,  0.7206, -0.3203,\n",
      "         -1.6814, -2.2418, -1.4412, -0.1601, -1.7614,  0.0801, -2.1617,  1.5212,\n",
      "          2.4019,  2.4019,  0.0801,  1.6814,  0.0000, -0.4003,  1.5212, -0.3203,\n",
      "         -1.0408,  0.8006,  0.0000, -0.5605,  0.3203,  0.2402,  0.1601, -1.1209,\n",
      "          0.5605,  0.8006, -0.0801,  1.6013, -0.8807, -1.5212, -0.8006,  0.7206,\n",
      "          1.6814,  2.0016,  0.0000, -0.4804, -0.4003, -1.0408, -0.9608, -1.6013,\n",
      "         -0.8006,  0.2402,  0.8006, -0.8807, -2.8823,  1.4412, -1.2010, -0.8006,\n",
      "         -0.4804, -1.3611,  0.3203,  0.4804,  0.2402, -0.4003,  1.2010, -0.4804,\n",
      "          0.4804,  0.7206,  0.0801,  0.4804,  0.7206,  0.9608,  0.6405, -0.3203,\n",
      "         -1.1209,  0.5605, -0.5605, -0.3203, -1.0408, -0.2402,  0.9608,  1.0408,\n",
      "          1.6013,  0.6405, -0.9608,  0.6405,  0.8807, -0.5605, -0.2402,  1.6814,\n",
      "         -1.1209, -0.7206, -0.1601, -2.9624, -1.6814,  0.0801,  1.1209, -1.4412,\n",
      "         -3.4428,  2.2418,  1.4412,  2.0016,  1.4412, -1.2810,  0.0801, -0.4804,\n",
      "          0.2402,  2.5621, -0.5605, -0.7206,  0.0000,  1.3611, -1.5212, -0.9608,\n",
      "          0.4804,  3.0425, -0.0801,  1.7614,  1.7614,  0.7206,  0.3203, -1.7614,\n",
      "          0.1601,  1.2810,  1.6013,  2.5621, -1.8415,  1.4412, -0.1601, -0.9608,\n",
      "          0.1601,  2.1617, -0.5605,  1.2010,  0.5605,  0.4003,  2.7222, -0.2402,\n",
      "          0.0801,  0.0000, -2.8823, -0.4804,  1.0408, -2.0817, -1.2010,  0.4003,\n",
      "         -0.5605, -0.2402,  2.2418,  0.4003, -1.0408,  0.8807,  2.4019,  1.6814,\n",
      "         -1.2010, -1.6814, -3.0425, -0.4003,  0.3203,  1.2010,  2.1617, -0.5605,\n",
      "          0.0000, -2.7222,  0.3203,  1.2810, -0.5605, -2.8023, -0.4804, -1.9215,\n",
      "          1.2010, -1.0408, -2.0016, -0.2402, -1.3611, -0.4804,  0.7206, -0.0801,\n",
      "          2.4820,  4.2434,  2.0016, -0.9608, -0.3203, -0.0801,  1.4412, -2.0016,\n",
      "         -0.6405,  2.1617, -0.0801, -0.4804, -0.5605,  1.9215, -1.6814, -0.6405,\n",
      "          0.1601, -0.5605,  1.2010, -0.8006, -1.2010, -1.8415, -2.4820, -1.1209,\n",
      "         -1.5212, -0.1601, -1.3611, -0.8006, -1.0408,  0.3203, -0.4003, -0.4804,\n",
      "         -1.6013,  1.2810,  0.5605,  0.3203, -0.1601, -0.8006, -2.2418, -0.6405,\n",
      "         -0.5605, -1.7614,  1.2810, -0.8807, -0.7206,  1.4412, -0.1601, -4.0032,\n",
      "         -1.2810, -0.6405,  2.3219,  3.6830, -0.8807,  0.8807,  0.4804, -1.6013,\n",
      "          0.4804, -0.1601, -0.8807, -2.2418, -2.3219,  3.6029, -2.9624,  0.3203,\n",
      "          2.3219, -0.6405, -1.2810, -1.2010,  5.2042,  1.6814,  0.0000,  1.6013,\n",
      "         -0.8807, -1.5212,  4.9640,  0.1601, -1.1209,  1.5212, -0.4804,  1.0408,\n",
      "          0.5605, -1.2010,  0.8807, -0.2402,  1.0408, -2.4820,  0.3203,  0.0000,\n",
      "         -1.6013,  2.1617,  2.4820,  0.2402,  2.8023,  0.0801, -1.7614, -0.8807,\n",
      "         -1.5212,  0.4003, -2.1617, -1.0408,  0.4003,  0.3203, -2.0817,  2.0016,\n",
      "         -1.6013, -1.2010, -0.6405,  0.0801, -1.2010, -2.4820, -0.6405,  3.0425,\n",
      "          1.1209,  1.6814, -0.6405, -1.7614,  1.2010,  0.0000, -0.3203,  0.1601,\n",
      "         -0.4003, -0.9608,  1.6013,  2.4019, -1.1209, -1.0408,  0.5605, -1.6814,\n",
      "         -0.3203, -0.7206,  0.0801,  4.0833,  2.1617,  0.6405,  0.0801, -1.0408,\n",
      "         -1.8415, -1.6013, -0.0801, -2.1617, -1.2010,  0.5605, -2.4820, -1.1209,\n",
      "          1.4412, -0.4804, -1.6013, -1.5212,  0.9608,  1.0408,  0.4003,  1.1209,\n",
      "          0.8006, -0.0801,  0.0000, -0.5605,  2.0016, -0.2402,  3.0425, -0.4804,\n",
      "         -0.3203,  0.1601,  0.6405,  0.7206, -2.7222,  0.0000, -1.2810,  2.2418,\n",
      "         -0.7206,  1.1209, -0.3203,  0.8807, -0.9608, -1.8415,  0.2402, -0.4003,\n",
      "          1.9215, -0.1601, -1.1209, -0.4003,  0.3203,  2.5621, -0.1601, -0.3203,\n",
      "          0.9608, -0.7206, -0.9608, -1.2810, -3.8431,  1.2010, -1.2810,  0.0000,\n",
      "         -2.0817,  0.3203,  2.2418,  1.4412, -1.6013, -1.6013, -1.4412, -1.2810,\n",
      "          2.9624,  0.4003, -1.6013, -2.8823,  0.8807,  0.8006, -0.7206,  2.4019,\n",
      "          2.2418, -1.2010, -0.2402, -0.3203,  1.8415,  1.1209, -0.5605,  1.6814,\n",
      "          1.3611, -2.1617, -0.4003,  0.2402, -0.4003,  0.4804, -0.8807,  1.0408,\n",
      "          2.7222, -1.2810, -0.4003, -1.2810,  2.8023, -2.9624, -0.6405, -1.2010,\n",
      "         -1.4412,  2.8023,  0.0801, -0.7206, -2.0817,  1.0408, -3.5228,  1.7614,\n",
      "          1.7614, -1.2810, -0.5605,  0.8807, -1.5212,  2.4019, -2.7222,  3.9232,\n",
      "         -0.4003,  0.1601,  2.1617, -2.4019, -1.5212,  4.3235, -0.1601,  0.2402,\n",
      "          0.3203, -0.6405, -1.2810,  0.4003,  0.6405,  2.3219,  0.6405, -2.3219,\n",
      "          0.8006,  0.4003, -0.3203,  0.1601,  1.4412, -0.6405, -0.2402, -1.6013,\n",
      "          0.8807, -1.8415, -0.7206, -1.6814,  1.5212, -0.4804, -0.4003, -0.4003,\n",
      "         -0.8006,  0.4804, -3.1225,  0.4003,  0.4804,  0.1601, -2.2418,  3.4428,\n",
      "          0.5605, -0.7206,  0.3203,  0.3203, -1.1209,  0.0000,  2.8823,  0.2402,\n",
      "         -0.6405, -1.8415, -1.3611,  0.1601, -2.0016,  0.3203, -1.2010,  1.6013,\n",
      "          1.8415, -0.3203,  5.9248,  0.2402, -0.7206,  0.0801, -1.6013,  1.2010,\n",
      "         -2.4820, -0.7206, -0.4804, -0.4003,  0.3203, -0.0801, -1.0408,  0.8807,\n",
      "         -1.0408,  0.8006, -1.5212,  1.5212,  0.0801,  0.5605, -2.1617,  5.1241,\n",
      "          0.1601, -2.3219,  2.8023,  2.4820, -0.4804, -0.5605,  0.5605, -0.8807,\n",
      "          4.8039, -0.4003, -0.6405,  1.2810,  1.9215, -0.4804, -2.0016, -0.4003,\n",
      "         -0.2402, -1.8415,  0.4003, -0.0801, -0.2402,  0.6405, -0.8807,  0.2402,\n",
      "          1.6013,  0.8006,  0.4003,  1.5212,  1.2810,  0.4003,  1.8415, -1.6814,\n",
      "         -1.3611, -0.4003, -0.9608,  2.0016,  2.4820,  0.1601,  3.1225,  0.0801,\n",
      "         -3.1225,  0.6405,  0.9608,  1.6013, -1.3611,  1.7614,  1.2810, -1.7614,\n",
      "         -2.3219, -3.1225,  0.8006,  0.2402,  4.0032, -1.0408,  0.9608, -1.6013,\n",
      "          1.8415, -1.2010, -2.5621, -0.4804,  2.2418, -0.4804, -2.5621,  3.3627,\n",
      "          1.0408, -0.3203,  0.8807,  2.0016,  1.2810,  4.2434, -0.4804,  0.8006,\n",
      "          0.5605,  0.3203, -1.5212, -1.8415,  2.4019,  0.4804,  1.1209, -1.2810,\n",
      "          1.6814,  0.1601,  0.3203,  0.0801,  2.2418, -0.4804, -1.0408,  1.5212,\n",
      "          5.4444,  2.6421, -0.8006,  1.2010,  0.8006,  1.5212,  0.5605,  4.0833,\n",
      "          1.6814, -0.0801, -1.7614,  0.5605,  1.9215,  1.3611, -0.4804,  0.7206,\n",
      "          2.0817,  5.2042,  2.8023, -0.5605, -1.8415,  0.9608, -1.2810, -2.3219,\n",
      "         -1.2810,  0.7206, -0.9608, -0.8807, -1.7614, -1.1209, -1.4412, -1.1209,\n",
      "         -0.2402,  0.7206, -1.3611, -0.4804,  0.8807,  0.4003, -0.9608,  0.4003,\n",
      "         -1.7614, -0.7206, -0.8006,  2.3219,  1.6013,  0.0801,  2.5621,  2.4820,\n",
      "          1.1209, -0.0801,  0.3203,  0.8006, -2.5621,  0.8807,  2.4019, -2.0817,\n",
      "         -0.3203, -0.8006, -0.1601, -0.0801, -1.2010, -2.2418,  1.7614,  0.5605,\n",
      "          0.3203, -0.4003,  0.6405,  3.9232,  0.4003,  4.7238,  2.1617,  2.8023,\n",
      "         -0.2402,  2.4019,  1.7614,  1.2010,  5.2843, -0.0801,  2.3219,  2.1617,\n",
      "          2.2418,  1.0408, -0.1601,  2.0016, -0.0801,  1.6013,  1.8415,  2.4820,\n",
      "          1.5212, -0.9608, -0.7206,  0.8006,  0.0000,  1.1209,  2.0817,  1.1209]],\n",
      "       size=(1, 1000), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.08006454259157181,\n",
      "       zero_point=51)\n",
      "torch.float32 tensor([[-0.1601,  3.0425, -1.6013, -1.1209,  0.5605,  2.2418,  2.3219,  0.9608,\n",
      "          0.2402,  0.0801, -0.0801,  1.8415,  0.8807,  0.4804,  0.8006,  1.2010,\n",
      "         -2.2418, -1.5212,  1.5212, -0.9608, -1.2010, -1.5212,  0.2402,  0.1601,\n",
      "         -0.9608, -1.3611,  0.2402, -0.3203, -1.1209,  0.0801, -1.4412, -1.5212,\n",
      "         -2.4019, -0.4003, -0.1601, -0.7206, -0.1601, -1.5212, -1.3611, -1.5212,\n",
      "          0.0000, -2.1617, -1.0408, -1.2810, -1.9215, -0.1601,  0.4003, -1.4412,\n",
      "         -0.8807, -0.8006,  0.4804, -2.2418, -1.0408, -1.8415, -1.9215, -1.4412,\n",
      "         -3.0425, -2.9624,  0.2402, -1.6013, -0.9608, -2.8823, -2.8823, -1.4412,\n",
      "         -1.0408,  1.0408, -2.3219, -1.5212, -3.5228, -0.7206,  0.8006,  0.4003,\n",
      "          0.4003,  0.9608,  1.7614,  0.8006,  0.4003,  1.8415,  2.2418,  2.0817,\n",
      "         -0.3203, -0.6405, -0.3203, -0.6405,  0.6405,  0.0801,  1.1209, -1.6814,\n",
      "          1.7614,  0.0801, -0.0801,  0.0000,  2.1617, -0.1601,  0.3203, -2.0817,\n",
      "          1.8415, -1.2010, -2.3219,  0.8006, -0.2402,  0.3203, -2.2418,  0.0000,\n",
      "          0.0000, -2.0817, -0.5605,  4.5637,  2.6421,  2.6421,  2.6421,  3.4428,\n",
      "          0.3203,  0.1601,  0.3203,  2.0016,  1.3611,  0.5605,  2.1617, -0.0801,\n",
      "         -0.8006,  0.0801, -0.8807,  0.2402,  0.6405, -1.3611,  1.7614,  0.9608,\n",
      "          0.7206,  1.0408,  1.2810,  0.0000,  2.0817,  1.1209,  2.0016,  0.0000,\n",
      "         -0.4804, -0.5605,  1.0408, -1.7614, -1.8415, -1.0408, -1.2010, -0.2402,\n",
      "         -1.0408,  0.4003, -2.2418, -1.7614, -0.9608, -0.5605, -0.2402,  0.0801,\n",
      "         -2.0016, -0.9608, -1.6013, -0.9608, -0.8006, -0.3203, -0.8006, -0.0801,\n",
      "         -1.2010, -0.4003,  0.1601,  0.6405, -1.0408, -0.4003, -0.7206, -0.8807,\n",
      "         -0.4003, -0.5605, -0.7206, -1.1209, -1.1209,  0.0000, -2.3219, -0.8807,\n",
      "         -1.4412, -0.7206, -0.5605, -1.2010, -1.6814, -0.5605, -0.2402, -0.8006,\n",
      "         -0.2402, -0.7206, -0.7206, -0.5605, -1.2010, -0.4804, -2.0817,  0.8006,\n",
      "         -0.9608,  0.4003, -1.0408, -0.1601, -1.2810, -0.7206, -1.8415, -0.2402,\n",
      "         -1.0408, -0.4003, -0.7206,  1.4412,  0.0801, -1.6013, -0.8807,  0.0000,\n",
      "         -0.7206, -1.2810, -2.7222,  0.8006, -0.3203,  1.2010,  0.1601, -0.0801,\n",
      "         -0.6405, -0.4003, -0.1601, -0.3203, -0.4003,  0.4003, -0.4003, -0.8006,\n",
      "          0.0801, -1.1209, -1.3611, -0.8807, -0.8807,  0.5605,  0.7206,  0.9608,\n",
      "         -0.6405, -1.0408,  0.4804, -0.4804,  0.2402, -0.4804, -0.4003, -0.6405,\n",
      "          0.7206, -0.2402, -0.3203, -1.1209, -1.2810, -1.1209, -2.1617, -0.8006,\n",
      "         -0.8807, -0.9608, -0.1601, -1.0408, -1.0408,  0.2402, -0.1601, -0.7206,\n",
      "         -0.8807, -0.3203,  0.0000,  0.4804,  0.5605, -0.8006, -1.9215,  0.0000,\n",
      "         -0.9608, -0.0801, -2.0016, -0.8006, -3.2026,  0.1601,  0.5605,  2.0016,\n",
      "          1.2010,  1.2010,  2.1617,  0.0801, -0.2402,  2.0817,  0.1601,  1.0408,\n",
      "          1.0408, -1.1209,  0.6405,  0.0801, -0.1601, -1.2810,  0.5605, -1.4412,\n",
      "         -0.5605, -1.8415, -1.4412,  0.1601,  0.5605,  0.4804,  0.9608,  1.0408,\n",
      "          1.6814, -0.0801,  0.2402, -0.3203, -1.9215,  1.4412,  0.2402,  0.0000,\n",
      "          0.2402, -1.4412, -0.3203,  0.7206,  0.3203,  1.4412,  4.5637, -1.0408,\n",
      "         -0.5605,  1.7614,  2.4820, -0.5605, -1.4412, -1.0408,  0.7206, -0.3203,\n",
      "         -1.6814, -2.2418, -1.4412, -0.1601, -1.7614,  0.0801, -2.1617,  1.5212,\n",
      "          2.4019,  2.4019,  0.0801,  1.6814,  0.0000, -0.4003,  1.5212, -0.3203,\n",
      "         -1.0408,  0.8006,  0.0000, -0.5605,  0.3203,  0.2402,  0.1601, -1.1209,\n",
      "          0.5605,  0.8006, -0.0801,  1.6013, -0.8807, -1.5212, -0.8006,  0.7206,\n",
      "          1.6814,  2.0016,  0.0000, -0.4804, -0.4003, -1.0408, -0.9608, -1.6013,\n",
      "         -0.8006,  0.2402,  0.8006, -0.8807, -2.8823,  1.4412, -1.2010, -0.8006,\n",
      "         -0.4804, -1.3611,  0.3203,  0.4804,  0.2402, -0.4003,  1.2010, -0.4804,\n",
      "          0.4804,  0.7206,  0.0801,  0.4804,  0.7206,  0.9608,  0.6405, -0.3203,\n",
      "         -1.1209,  0.5605, -0.5605, -0.3203, -1.0408, -0.2402,  0.9608,  1.0408,\n",
      "          1.6013,  0.6405, -0.9608,  0.6405,  0.8807, -0.5605, -0.2402,  1.6814,\n",
      "         -1.1209, -0.7206, -0.1601, -2.9624, -1.6814,  0.0801,  1.1209, -1.4412,\n",
      "         -3.4428,  2.2418,  1.4412,  2.0016,  1.4412, -1.2810,  0.0801, -0.4804,\n",
      "          0.2402,  2.5621, -0.5605, -0.7206,  0.0000,  1.3611, -1.5212, -0.9608,\n",
      "          0.4804,  3.0425, -0.0801,  1.7614,  1.7614,  0.7206,  0.3203, -1.7614,\n",
      "          0.1601,  1.2810,  1.6013,  2.5621, -1.8415,  1.4412, -0.1601, -0.9608,\n",
      "          0.1601,  2.1617, -0.5605,  1.2010,  0.5605,  0.4003,  2.7222, -0.2402,\n",
      "          0.0801,  0.0000, -2.8823, -0.4804,  1.0408, -2.0817, -1.2010,  0.4003,\n",
      "         -0.5605, -0.2402,  2.2418,  0.4003, -1.0408,  0.8807,  2.4019,  1.6814,\n",
      "         -1.2010, -1.6814, -3.0425, -0.4003,  0.3203,  1.2010,  2.1617, -0.5605,\n",
      "          0.0000, -2.7222,  0.3203,  1.2810, -0.5605, -2.8023, -0.4804, -1.9215,\n",
      "          1.2010, -1.0408, -2.0016, -0.2402, -1.3611, -0.4804,  0.7206, -0.0801,\n",
      "          2.4820,  4.2434,  2.0016, -0.9608, -0.3203, -0.0801,  1.4412, -2.0016,\n",
      "         -0.6405,  2.1617, -0.0801, -0.4804, -0.5605,  1.9215, -1.6814, -0.6405,\n",
      "          0.1601, -0.5605,  1.2010, -0.8006, -1.2010, -1.8415, -2.4820, -1.1209,\n",
      "         -1.5212, -0.1601, -1.3611, -0.8006, -1.0408,  0.3203, -0.4003, -0.4804,\n",
      "         -1.6013,  1.2810,  0.5605,  0.3203, -0.1601, -0.8006, -2.2418, -0.6405,\n",
      "         -0.5605, -1.7614,  1.2810, -0.8807, -0.7206,  1.4412, -0.1601, -4.0032,\n",
      "         -1.2810, -0.6405,  2.3219,  3.6830, -0.8807,  0.8807,  0.4804, -1.6013,\n",
      "          0.4804, -0.1601, -0.8807, -2.2418, -2.3219,  3.6029, -2.9624,  0.3203,\n",
      "          2.3219, -0.6405, -1.2810, -1.2010,  5.2042,  1.6814,  0.0000,  1.6013,\n",
      "         -0.8807, -1.5212,  4.9640,  0.1601, -1.1209,  1.5212, -0.4804,  1.0408,\n",
      "          0.5605, -1.2010,  0.8807, -0.2402,  1.0408, -2.4820,  0.3203,  0.0000,\n",
      "         -1.6013,  2.1617,  2.4820,  0.2402,  2.8023,  0.0801, -1.7614, -0.8807,\n",
      "         -1.5212,  0.4003, -2.1617, -1.0408,  0.4003,  0.3203, -2.0817,  2.0016,\n",
      "         -1.6013, -1.2010, -0.6405,  0.0801, -1.2010, -2.4820, -0.6405,  3.0425,\n",
      "          1.1209,  1.6814, -0.6405, -1.7614,  1.2010,  0.0000, -0.3203,  0.1601,\n",
      "         -0.4003, -0.9608,  1.6013,  2.4019, -1.1209, -1.0408,  0.5605, -1.6814,\n",
      "         -0.3203, -0.7206,  0.0801,  4.0833,  2.1617,  0.6405,  0.0801, -1.0408,\n",
      "         -1.8415, -1.6013, -0.0801, -2.1617, -1.2010,  0.5605, -2.4820, -1.1209,\n",
      "          1.4412, -0.4804, -1.6013, -1.5212,  0.9608,  1.0408,  0.4003,  1.1209,\n",
      "          0.8006, -0.0801,  0.0000, -0.5605,  2.0016, -0.2402,  3.0425, -0.4804,\n",
      "         -0.3203,  0.1601,  0.6405,  0.7206, -2.7222,  0.0000, -1.2810,  2.2418,\n",
      "         -0.7206,  1.1209, -0.3203,  0.8807, -0.9608, -1.8415,  0.2402, -0.4003,\n",
      "          1.9215, -0.1601, -1.1209, -0.4003,  0.3203,  2.5621, -0.1601, -0.3203,\n",
      "          0.9608, -0.7206, -0.9608, -1.2810, -3.8431,  1.2010, -1.2810,  0.0000,\n",
      "         -2.0817,  0.3203,  2.2418,  1.4412, -1.6013, -1.6013, -1.4412, -1.2810,\n",
      "          2.9624,  0.4003, -1.6013, -2.8823,  0.8807,  0.8006, -0.7206,  2.4019,\n",
      "          2.2418, -1.2010, -0.2402, -0.3203,  1.8415,  1.1209, -0.5605,  1.6814,\n",
      "          1.3611, -2.1617, -0.4003,  0.2402, -0.4003,  0.4804, -0.8807,  1.0408,\n",
      "          2.7222, -1.2810, -0.4003, -1.2810,  2.8023, -2.9624, -0.6405, -1.2010,\n",
      "         -1.4412,  2.8023,  0.0801, -0.7206, -2.0817,  1.0408, -3.5228,  1.7614,\n",
      "          1.7614, -1.2810, -0.5605,  0.8807, -1.5212,  2.4019, -2.7222,  3.9232,\n",
      "         -0.4003,  0.1601,  2.1617, -2.4019, -1.5212,  4.3235, -0.1601,  0.2402,\n",
      "          0.3203, -0.6405, -1.2810,  0.4003,  0.6405,  2.3219,  0.6405, -2.3219,\n",
      "          0.8006,  0.4003, -0.3203,  0.1601,  1.4412, -0.6405, -0.2402, -1.6013,\n",
      "          0.8807, -1.8415, -0.7206, -1.6814,  1.5212, -0.4804, -0.4003, -0.4003,\n",
      "         -0.8006,  0.4804, -3.1225,  0.4003,  0.4804,  0.1601, -2.2418,  3.4428,\n",
      "          0.5605, -0.7206,  0.3203,  0.3203, -1.1209,  0.0000,  2.8823,  0.2402,\n",
      "         -0.6405, -1.8415, -1.3611,  0.1601, -2.0016,  0.3203, -1.2010,  1.6013,\n",
      "          1.8415, -0.3203,  5.9248,  0.2402, -0.7206,  0.0801, -1.6013,  1.2010,\n",
      "         -2.4820, -0.7206, -0.4804, -0.4003,  0.3203, -0.0801, -1.0408,  0.8807,\n",
      "         -1.0408,  0.8006, -1.5212,  1.5212,  0.0801,  0.5605, -2.1617,  5.1241,\n",
      "          0.1601, -2.3219,  2.8023,  2.4820, -0.4804, -0.5605,  0.5605, -0.8807,\n",
      "          4.8039, -0.4003, -0.6405,  1.2810,  1.9215, -0.4804, -2.0016, -0.4003,\n",
      "         -0.2402, -1.8415,  0.4003, -0.0801, -0.2402,  0.6405, -0.8807,  0.2402,\n",
      "          1.6013,  0.8006,  0.4003,  1.5212,  1.2810,  0.4003,  1.8415, -1.6814,\n",
      "         -1.3611, -0.4003, -0.9608,  2.0016,  2.4820,  0.1601,  3.1225,  0.0801,\n",
      "         -3.1225,  0.6405,  0.9608,  1.6013, -1.3611,  1.7614,  1.2810, -1.7614,\n",
      "         -2.3219, -3.1225,  0.8006,  0.2402,  4.0032, -1.0408,  0.9608, -1.6013,\n",
      "          1.8415, -1.2010, -2.5621, -0.4804,  2.2418, -0.4804, -2.5621,  3.3627,\n",
      "          1.0408, -0.3203,  0.8807,  2.0016,  1.2810,  4.2434, -0.4804,  0.8006,\n",
      "          0.5605,  0.3203, -1.5212, -1.8415,  2.4019,  0.4804,  1.1209, -1.2810,\n",
      "          1.6814,  0.1601,  0.3203,  0.0801,  2.2418, -0.4804, -1.0408,  1.5212,\n",
      "          5.4444,  2.6421, -0.8006,  1.2010,  0.8006,  1.5212,  0.5605,  4.0833,\n",
      "          1.6814, -0.0801, -1.7614,  0.5605,  1.9215,  1.3611, -0.4804,  0.7206,\n",
      "          2.0817,  5.2042,  2.8023, -0.5605, -1.8415,  0.9608, -1.2810, -2.3219,\n",
      "         -1.2810,  0.7206, -0.9608, -0.8807, -1.7614, -1.1209, -1.4412, -1.1209,\n",
      "         -0.2402,  0.7206, -1.3611, -0.4804,  0.8807,  0.4003, -0.9608,  0.4003,\n",
      "         -1.7614, -0.7206, -0.8006,  2.3219,  1.6013,  0.0801,  2.5621,  2.4820,\n",
      "          1.1209, -0.0801,  0.3203,  0.8006, -2.5621,  0.8807,  2.4019, -2.0817,\n",
      "         -0.3203, -0.8006, -0.1601, -0.0801, -1.2010, -2.2418,  1.7614,  0.5605,\n",
      "          0.3203, -0.4003,  0.6405,  3.9232,  0.4003,  4.7238,  2.1617,  2.8023,\n",
      "         -0.2402,  2.4019,  1.7614,  1.2010,  5.2843, -0.0801,  2.3219,  2.1617,\n",
      "          2.2418,  1.0408, -0.1601,  2.0016, -0.0801,  1.6013,  1.8415,  2.4820,\n",
      "          1.5212, -0.9608, -0.7206,  0.8006,  0.0000,  1.1209,  2.0817,  1.1209]])\n"
     ]
    }
   ],
   "source": [
    "print(type(modules[-1]))\n",
    "print(before_l[-1].dtype,before_l[-1])\n",
    "print(after_l[-1].dtype,after_l[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "===\n",
      "===\n",
      "===\n",
      "model.features.0.weight\n",
      "model.features.0.bias\n",
      "model.features.0.scale\n",
      "model.features.0.zero_point\n",
      "model.features.2.weight\n",
      "model.features.2.bias\n",
      "model.features.2.scale\n",
      "model.features.2.zero_point\n",
      "model.features.5.weight\n",
      "model.features.5.bias\n",
      "model.features.5.scale\n",
      "model.features.5.zero_point\n",
      "model.features.7.weight\n",
      "model.features.7.bias\n",
      "model.features.7.scale\n",
      "model.features.7.zero_point\n",
      "model.features.10.weight\n",
      "model.features.10.bias\n",
      "model.features.10.scale\n",
      "model.features.10.zero_point\n",
      "model.features.12.weight\n",
      "model.features.12.bias\n",
      "model.features.12.scale\n",
      "model.features.12.zero_point\n",
      "model.features.14.weight\n",
      "model.features.14.bias\n",
      "model.features.14.scale\n",
      "model.features.14.zero_point\n",
      "model.features.17.weight\n",
      "model.features.17.bias\n",
      "model.features.17.scale\n",
      "model.features.17.zero_point\n",
      "model.features.19.weight\n",
      "model.features.19.bias\n",
      "model.features.19.scale\n",
      "model.features.19.zero_point\n",
      "model.features.21.weight\n",
      "model.features.21.bias\n",
      "model.features.21.scale\n",
      "model.features.21.zero_point\n",
      "model.features.24.weight\n",
      "model.features.24.bias\n",
      "model.features.24.scale\n",
      "model.features.24.zero_point\n",
      "model.features.26.weight\n",
      "model.features.26.bias\n",
      "model.features.26.scale\n",
      "model.features.26.zero_point\n",
      "model.features.28.weight\n",
      "model.features.28.bias\n",
      "model.features.28.scale\n",
      "model.features.28.zero_point\n",
      "model.classifier.0.scale\n",
      "model.classifier.0.zero_point\n",
      "model.classifier.0._packed_params.dtype\n",
      "model.classifier.0._packed_params._packed_params\n",
      "model.classifier.3.scale\n",
      "model.classifier.3.zero_point\n",
      "model.classifier.3._packed_params.dtype\n",
      "model.classifier.3._packed_params._packed_params\n",
      "model.classifier.6.scale\n",
      "model.classifier.6.zero_point\n",
      "model.classifier.6._packed_params.dtype\n",
      "model.classifier.6._packed_params._packed_params\n",
      "quant.scale\n",
      "quant.zero_point\n",
      "tensor([[[[ -23,  -75, -101],\n",
      "          [   5,  -64, -127],\n",
      "          [  24,  -50, -100]],\n",
      "\n",
      "         [[  36,  -21,  -39],\n",
      "          [  53,    6,  -14],\n",
      "          [  53,   30,   12]],\n",
      "\n",
      "         [[  54,    4,  -36],\n",
      "          [  64,   38,   -1],\n",
      "          [   6,   17,    5]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  21,   17,   -7],\n",
      "          [  19,    3,  -26],\n",
      "          [  15,   28,  -10]],\n",
      "\n",
      "         [[  17,   32,   44],\n",
      "          [  21,   28,   26],\n",
      "          [  -7,   15,   38]],\n",
      "\n",
      "         [[  17,  -16,  -76],\n",
      "          [ -44,  -54,  -59],\n",
      "          [ -29,  -19,   -3]]],\n",
      "\n",
      "\n",
      "        [[[  -9,  -55,  -95],\n",
      "          [ -26,  -58, -100],\n",
      "          [ -30,  -76, -114]],\n",
      "\n",
      "         [[  -5,   34,   11],\n",
      "          [  13,    7,  -11],\n",
      "          [   1,  -23,  -25]],\n",
      "\n",
      "         [[  10,  -10,  -31],\n",
      "          [ -13,   10,  -58],\n",
      "          [  13,    2,    6]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  28,   25,   18],\n",
      "          [  11,    2,   -8],\n",
      "          [  17,   12,  -18]],\n",
      "\n",
      "         [[ -16,    2,  -15],\n",
      "          [   0,   -5,   12],\n",
      "          [  -3,  -16,  -15]],\n",
      "\n",
      "         [[ -20,  -19,   19],\n",
      "          [ -41,  -39,   50],\n",
      "          [ -45,  -27,   29]]],\n",
      "\n",
      "\n",
      "        [[[   5,   10,   23],\n",
      "          [ -11,   -7,   12],\n",
      "          [   5,    4,   12]],\n",
      "\n",
      "         [[ -33,  -37,  -23],\n",
      "          [ -20,  -47,   -5],\n",
      "          [  15,   10,   46]],\n",
      "\n",
      "         [[ -18,   -7,   -4],\n",
      "          [  12,   19,    7],\n",
      "          [  37,   53,   30]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[   1,    6,   -7],\n",
      "          [  -3,   -5,   -1],\n",
      "          [   3,    1,  -14]],\n",
      "\n",
      "         [[  -3,   -2,    0],\n",
      "          [   0,   -3,    4],\n",
      "          [  13,    7,   14]],\n",
      "\n",
      "         [[  25,   -3,   16],\n",
      "          [  -9,  -20,   17],\n",
      "          [  10,    3,   15]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -25,  -16,    6],\n",
      "          [ -26,  -28,   10],\n",
      "          [ -25,  -14,  -14]],\n",
      "\n",
      "         [[ -11,    9,  -36],\n",
      "          [  17,   16,  -36],\n",
      "          [  -8,    1,  -35]],\n",
      "\n",
      "         [[ -16,  -12,  -29],\n",
      "          [   6,  -29,  -31],\n",
      "          [ -22,    0,   20]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  -1,    1,   -4],\n",
      "          [   1,    5,   10],\n",
      "          [   7,    4,   19]],\n",
      "\n",
      "         [[   5,   18,   22],\n",
      "          [  -1,    1,    8],\n",
      "          [  -3,    5,   16]],\n",
      "\n",
      "         [[ -17,  -38,  -42],\n",
      "          [ -20,   -5,   34],\n",
      "          [  -9,   33,   72]]],\n",
      "\n",
      "\n",
      "        [[[  22,   21,    9],\n",
      "          [  22,   16,    9],\n",
      "          [  -3,   12,   -1]],\n",
      "\n",
      "         [[  17,   -9,    6],\n",
      "          [ -32,  -25,  -15],\n",
      "          [ -17,   10,   -7]],\n",
      "\n",
      "         [[ -54,  -54,  -72],\n",
      "          [  40,   -4,  -40],\n",
      "          [  93,  100,   18]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  14,   18,    3],\n",
      "          [  -2,   11,    9],\n",
      "          [  -3,  -16,    0]],\n",
      "\n",
      "         [[   4,   -3,    3],\n",
      "          [   0,    7,   11],\n",
      "          [   1,   -7,    2]],\n",
      "\n",
      "         [[  18,    6,   17],\n",
      "          [  -5,   33,   19],\n",
      "          [ -11,    9,   -7]]],\n",
      "\n",
      "\n",
      "        [[[ -25,    4,   29],\n",
      "          [   4,  -12,   20],\n",
      "          [ -14,  -11,   24]],\n",
      "\n",
      "         [[  20,  -71,   37],\n",
      "          [ -69,    5,   92],\n",
      "          [  -2,   11,  -33]],\n",
      "\n",
      "         [[ -55,  -48,   65],\n",
      "          [ -58,   56,   31],\n",
      "          [  35,   32,  -12]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[   5,   12,   -4],\n",
      "          [  19,   -3,  -13],\n",
      "          [   5,  -11,   10]],\n",
      "\n",
      "         [[  11,   11,    2],\n",
      "          [   1,   -7,   -8],\n",
      "          [  -3,  -14,   -7]],\n",
      "\n",
      "         [[   9,   17,   25],\n",
      "          [   4,   12,  -22],\n",
      "          [  10,  -35,  -15]]]], dtype=torch.int8)\n",
      "odict_keys(['model.features.0.weight', 'model.features.0.bias', 'model.features.2.weight', 'model.features.2.bias', 'model.features.5.weight', 'model.features.5.bias', 'model.features.7.weight', 'model.features.7.bias', 'model.features.10.weight', 'model.features.10.bias', 'model.features.12.weight', 'model.features.12.bias', 'model.features.14.weight', 'model.features.14.bias', 'model.features.17.weight', 'model.features.17.bias', 'model.features.19.weight', 'model.features.19.bias', 'model.features.21.weight', 'model.features.21.bias', 'model.features.24.weight', 'model.features.24.bias', 'model.features.26.weight', 'model.features.26.bias', 'model.features.28.weight', 'model.features.28.bias', 'model.classifier.0.weight', 'model.classifier.0.bias', 'model.classifier.3.weight', 'model.classifier.3.bias', 'model.classifier.6.weight', 'model.classifier.6.bias'])\n",
      "tensor([[[[-3.0606e-02, -9.8520e-02, -1.3260e-01],\n",
      "          [ 6.8208e-03, -8.3483e-02, -1.6697e-01],\n",
      "          [ 3.1015e-02, -6.5803e-02, -1.3171e-01]],\n",
      "\n",
      "         [[ 4.7407e-02, -2.7588e-02, -5.1127e-02],\n",
      "          [ 7.0129e-02,  8.2528e-03, -1.8340e-02],\n",
      "          [ 6.9918e-02,  3.8993e-02,  1.6228e-02]],\n",
      "\n",
      "         [[ 7.0700e-02,  5.2703e-03, -4.7362e-02],\n",
      "          [ 8.4006e-02,  4.9190e-02, -1.6474e-03],\n",
      "          [ 8.5166e-03,  2.2350e-02,  5.9118e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.7666e-02,  2.1778e-02, -9.4606e-03],\n",
      "          [ 2.5511e-02,  4.1186e-03, -3.4521e-02],\n",
      "          [ 2.0150e-02,  3.7068e-02, -1.3509e-02]],\n",
      "\n",
      "         [[ 2.1684e-02,  4.1812e-02,  5.8284e-02],\n",
      "          [ 2.7431e-02,  3.6847e-02,  3.4335e-02],\n",
      "          [-9.4839e-03,  1.9745e-02,  5.0264e-02]],\n",
      "\n",
      "         [[ 2.1769e-02, -2.1388e-02, -9.9363e-02],\n",
      "          [-5.7156e-02, -7.1328e-02, -7.7600e-02],\n",
      "          [-3.7508e-02, -2.5453e-02, -4.5096e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.3319e-02, -7.7979e-02, -1.3496e-01],\n",
      "          [-3.7411e-02, -8.1807e-02, -1.4195e-01],\n",
      "          [-4.1913e-02, -1.0756e-01, -1.6164e-01]],\n",
      "\n",
      "         [[-6.8725e-03,  4.8598e-02,  1.5008e-02],\n",
      "          [ 1.8636e-02,  9.8393e-03, -1.5973e-02],\n",
      "          [ 9.5164e-04, -3.2665e-02, -3.5824e-02]],\n",
      "\n",
      "         [[ 1.4780e-02, -1.4260e-02, -4.4468e-02],\n",
      "          [-1.8438e-02,  1.4841e-02, -8.2337e-02],\n",
      "          [ 1.8329e-02,  2.1435e-03,  9.0911e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.0342e-02,  3.6146e-02,  2.5515e-02],\n",
      "          [ 1.5779e-02,  3.1012e-03, -1.0942e-02],\n",
      "          [ 2.3790e-02,  1.6440e-02, -2.5835e-02]],\n",
      "\n",
      "         [[-2.2844e-02,  2.5371e-03, -2.1714e-02],\n",
      "          [ 3.9534e-04, -6.4903e-03,  1.6979e-02],\n",
      "          [-4.7200e-03, -2.2301e-02, -2.1298e-02]],\n",
      "\n",
      "         [[-2.8434e-02, -2.6771e-02,  2.7432e-02],\n",
      "          [-5.8088e-02, -5.5869e-02,  7.0289e-02],\n",
      "          [-6.4470e-02, -3.8172e-02,  4.1569e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2508e-02,  2.4738e-02,  5.3893e-02],\n",
      "          [-2.5838e-02, -1.6176e-02,  2.8344e-02],\n",
      "          [ 1.2948e-02,  9.0717e-03,  2.8181e-02]],\n",
      "\n",
      "         [[-7.9309e-02, -8.8828e-02, -5.5737e-02],\n",
      "          [-4.8359e-02, -1.1139e-01, -1.0901e-02],\n",
      "          [ 3.4640e-02,  2.3298e-02,  1.1002e-01]],\n",
      "\n",
      "         [[-4.3616e-02, -1.6598e-02, -1.0547e-02],\n",
      "          [ 2.8982e-02,  4.5279e-02,  1.6869e-02],\n",
      "          [ 8.8168e-02,  1.2599e-01,  7.2292e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3967e-03,  1.5333e-02, -1.7196e-02],\n",
      "          [-8.3107e-03, -1.2905e-02, -1.4394e-03],\n",
      "          [ 6.3471e-03,  1.7825e-03, -3.3770e-02]],\n",
      "\n",
      "         [[-7.9354e-03, -5.8610e-03, -7.6292e-05],\n",
      "          [ 6.8661e-04, -6.0019e-03,  1.0119e-02],\n",
      "          [ 3.0581e-02,  1.6821e-02,  3.2570e-02]],\n",
      "\n",
      "         [[ 5.9351e-02, -7.4398e-03,  3.8274e-02],\n",
      "          [-2.0792e-02, -4.8833e-02,  4.0274e-02],\n",
      "          [ 2.3138e-02,  7.4794e-03,  3.5027e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.5464e-02, -2.9275e-02,  1.0282e-02],\n",
      "          [-4.6952e-02, -4.9473e-02,  1.7632e-02],\n",
      "          [-4.3815e-02, -2.4871e-02, -2.4908e-02]],\n",
      "\n",
      "         [[-1.9359e-02,  1.5948e-02, -6.3554e-02],\n",
      "          [ 3.0096e-02,  2.8515e-02, -6.4439e-02],\n",
      "          [-1.4547e-02,  2.2238e-03, -6.3371e-02]],\n",
      "\n",
      "         [[-2.7915e-02, -2.1738e-02, -5.1691e-02],\n",
      "          [ 1.0889e-02, -5.1994e-02, -5.4649e-02],\n",
      "          [-3.9741e-02,  5.2832e-04,  3.5375e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9130e-03,  1.0995e-03, -7.4082e-03],\n",
      "          [ 1.8428e-03,  8.2602e-03,  1.8680e-02],\n",
      "          [ 1.1709e-02,  6.9263e-03,  3.3630e-02]],\n",
      "\n",
      "         [[ 8.9195e-03,  3.2807e-02,  3.9282e-02],\n",
      "          [-2.5044e-03,  1.7645e-03,  1.4056e-02],\n",
      "          [-5.4708e-03,  9.5572e-03,  2.8639e-02]],\n",
      "\n",
      "         [[-3.0241e-02, -6.7225e-02, -7.4299e-02],\n",
      "          [-3.6222e-02, -9.7721e-03,  6.1669e-02],\n",
      "          [-1.6392e-02,  5.8156e-02,  1.2894e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.0321e-02,  3.7677e-02,  1.7181e-02],\n",
      "          [ 4.0298e-02,  2.9479e-02,  1.6698e-02],\n",
      "          [-4.8739e-03,  2.1128e-02, -2.6295e-03]],\n",
      "\n",
      "         [[ 3.1079e-02, -1.6475e-02,  1.0769e-02],\n",
      "          [-5.8626e-02, -4.6721e-02, -2.7764e-02],\n",
      "          [-3.1245e-02,  1.7635e-02, -1.2211e-02]],\n",
      "\n",
      "         [[-9.8675e-02, -9.8915e-02, -1.3225e-01],\n",
      "          [ 7.3214e-02, -7.5238e-03, -7.4033e-02],\n",
      "          [ 1.7070e-01,  1.8346e-01,  3.3906e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.5318e-02,  3.2680e-02,  5.2874e-03],\n",
      "          [-4.1894e-03,  2.0011e-02,  1.6021e-02],\n",
      "          [-4.8276e-03, -2.9130e-02,  7.4975e-04]],\n",
      "\n",
      "         [[ 7.1959e-03, -5.1528e-03,  5.1333e-03],\n",
      "          [-8.6466e-04,  1.2417e-02,  2.0805e-02],\n",
      "          [ 2.4862e-03, -1.3194e-02,  3.6563e-03]],\n",
      "\n",
      "         [[ 3.2415e-02,  1.0823e-02,  3.1720e-02],\n",
      "          [-8.4911e-03,  5.9831e-02,  3.4606e-02],\n",
      "          [-2.0674e-02,  1.6525e-02, -1.3183e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.6955e-02,  1.0821e-02,  7.9260e-02],\n",
      "          [ 1.0361e-02, -3.3380e-02,  5.2775e-02],\n",
      "          [-3.8372e-02, -3.0964e-02,  6.5422e-02]],\n",
      "\n",
      "         [[ 5.4925e-02, -1.9160e-01,  9.9709e-02],\n",
      "          [-1.8754e-01,  1.3248e-02,  2.4748e-01],\n",
      "          [-5.6662e-03,  2.8737e-02, -8.9787e-02]],\n",
      "\n",
      "         [[-1.4905e-01, -1.2974e-01,  1.7649e-01],\n",
      "          [-1.5774e-01,  1.5130e-01,  8.4126e-02],\n",
      "          [ 9.4988e-02,  8.5186e-02, -3.3635e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2278e-02,  3.3348e-02, -1.0524e-02],\n",
      "          [ 5.0310e-02, -9.1280e-03, -3.3890e-02],\n",
      "          [ 1.3459e-02, -2.9601e-02,  2.6862e-02]],\n",
      "\n",
      "         [[ 3.1006e-02,  2.8481e-02,  5.2361e-03],\n",
      "          [ 3.8914e-03, -1.8899e-02, -2.1819e-02],\n",
      "          [-8.4505e-03, -3.6632e-02, -1.9476e-02]],\n",
      "\n",
      "         [[ 2.3835e-02,  4.5658e-02,  6.7297e-02],\n",
      "          [ 1.1103e-02,  3.2035e-02, -5.8449e-02],\n",
      "          [ 2.6805e-02, -9.3975e-02, -4.0504e-02]]]])\n"
     ]
    }
   ],
   "source": [
    "for child in model_static_quantized.children():\n",
    "    if isinstance(child, nn.Sequential) or isinstance(child, torchvision.models.vgg.VGG):\n",
    "        print(\"---\")\n",
    "        for n,c in child.named_children():\n",
    "            print(\"===\")\n",
    "            for name, param in c.named_parameters():\n",
    "                print(name)\n",
    "\n",
    "state = model.state_dict()\n",
    "for names in model_static_quantized.state_dict():\n",
    "    print(names)\n",
    "print(model_static_quantized.model.features[2].weight().int_repr())\n",
    "print(state.keys())\n",
    "print(state['model.features.2.weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
